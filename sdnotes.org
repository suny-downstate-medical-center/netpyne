* 14jun18 Created repo
* 14sep19 First improvements by Giljael and me
** README by Gil
1.How to add new cell types in the model: <plx_cellpopdata.py> Insert cell info in lists like PMd case.  popnames = ['PMd',
'ER2', 'IF2', 'IL2', 'ER5', 'EB5', 'IF5', 'IL5', 'ER6', 'IF6', 'IL6'] popclasses = [-1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3] #
Izhikevich population type popEorI = [ 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1] # Whether it's excitatory or inhibitory popratios =
[numPMd,150, 25, 25, 167, 72, 40, 40, 192, 32, 32] Prior cells will have lower gids. E.g., PMd.gid < ER2.gid <...<IL6.gid

Change cellproperties() according to the cell types: cell info is returned to plx_model and the info is used for simulations.
For PMd type cells, the following is added, because the number of PMd cells are not changed as scale changes.  If cells
increase in the cell types as scale increases, the following modification is not needed.  indexPopName =
checkIndexPopName('PMd', popnames) # checkIndexPopName returns PMd index in popnames if not indexPopName == -1:
popnumbers[indexPopName] = numPMd # Number of PMds is fixed. # return back the number of PMd cells to numPMd.  "ncells" is a
global value, and total number of cells in the model.

According to the cell types added, return statement in names2inds() and its call in plx_cellpopdata.py and plx_model.py
should be updated.

def setconnprobs() needs connection probabilities for new cell types. Currently inserting PMds doesn't modify this function.

def setconnweights() needs new connection weight. For the PMds, connweights[PMd,ER2,AMPA]=10 was added.

<In plx_model.py> Update "## set cell types." @Line 189 For PMd, "elif cellclasses[c]==-1: celltypes.append(pmdnsloc)" is
added. pmdnsloc is defined in nsloc.py

Update "## set positions." @Line 202 Positions are changed. That is, if new cell types are added in the model, existing cells
in the model will have different positions because of randomness change.  xlocs = modelsize*rand(ncells) # Create random x
locations ylocs = modelsize*rand(ncells) # Create random y locations zlocs = verticalextent*rand(ncells) # Create random z
locations Add zlayer position update. For PMd, "elif cellnames[c][-1]=='d': zlocs[c]+=zlayerpositions[5]" is added. 'd' is in
'PMd'.

Update "## Actually create the cells." @Line 221 In this version(r2206 in SVN), all cells including new cells inserted are
distributed evenly in a round-robin way by "for c in xrange(int(pc.id()), ncells, nhosts):."  "cellsperhost" indicates how
many cells including new cell types inserted each worker created. Each worker might have different value of cellsperhost.
For PMd, the following code snippet is executed for NULL->NetCon->PMd connection. NSLOC-based cell types will follow the code
snippet, but inncl is only to feed PMds with external PMd spikes: if cellnames[gid] == 'PMd': cell = celltypes[gid](cellid =
gid) # create an NSLOC inncl.append(h.NetCon(None, cell)) # This netcon will receive external PMd spikes innclDic[gid] =
ninnclDic # This dictionary will be used for NetCon search.  ninnclDic += 1 else:

Update ##calculate distance and probabilities. @Line 256 Connection probabilities among cells are calculated prior to making
connections, However, PMds won't be post synaptic cells in the connections. So the following code snippet is only for PMds:
if cellnames[gid] == 'PMd': # There is no connection for cells -> PMds continue In order to make connections between the new
cells added and others based on probabilities, def setconnprobs() in plx_cellpopdata.py should be modified accordingly.
Connection between a ER2 and PMd is controlled explicitly by PMd[gid%numPMd]->ER2[gid]. So, if you want to control the
connections for other cells, follow the code for PMds:

pmdStart = cpd.popGidStart[PMd] # get pmd's start gid by using cpd.popGidStart[cellname] pmdEnd = cpd.popGidEnd[PMd] # get
PMd's end gid for c in xrange(pmdStart, pmdEnd + 1): allrands[c] = 1 # set all PMd values in allrands to 1.  if
cellnames[gid] == 'ER2': pMdId = (gid % numPMd) # select PMd being connected to this ER2 cell.  allconnprobs[pMdId] = 1 # to
make this PMd connected to the ER2 cell allrands[pMdId] = 0 # to make this PMd connected to the ER2 cell distances[pMdId] =
300 # to make the NetCon delay for this connection 5ms

Update ## Add background inputs @Line 447 ER2 and PMd cells won't be fired by background spikes. The following avoid them not
to be fired by background spikes: gid = gidvec[c] if isOriginal == 0: if cellnames[gid] == 'ER2' or cellnames[gid] ==
'PMd': # 'ER2' won't be fired by background stimulations.  continue

2.How to connect m1ms with Plexon?  # Connect m1ms with Plexon
- Copy m1ms/sim/Client to Windows machine having MATLAB and Plexon software.
- Open Client/plx_mat_interface.m on the Windows machine, and set up "remoteAddr" to the IP address m1ms runs on. In
  addition, set up "addapth" with the path for the library required for the Plexon software.
- Set up parameters in m1ms/sim/config.py accordingly.  isOriginal|isCommunication|isQueueTest a. 1 | x | x - To run the
  original m1ms (Cliff's parallelized model). X means don't care b. 0 | 1 | 1 - To run m1ms w/o connection to Plexon, but
  with PMd spike files c. 0 | 1 | 0 - To run m1ms, getting spikes from Plexon through the communication program Note: for b
  and c, check if PMd spike file (spikePMd-6sec.txv) is in data/.

3. How to run m1ms?  For 2.a, 2.b: $plx_runsim <# of workers>

For 2.c, 1. $plx_runsim <# of workers> 2. Run client in the Windows machine.  3. Run the Plexon softsever.

4. How to plot raster, lfp and power spectra?  Spikes are stored in m1ms/sim/m1ms-spk.txt and m1ms-spk.txt.mat Just run
python fileplots.py m1ms-spk.txt. It stores plots to files.  $python fileplots.py m1ms-spk.txt



** List of changes by Gil
- Added PMd population receiving external input
- Cells (inlcuidng PMd) distributed over workers using round-robin (each worker doesnt have same number of cells)
- Cells not referenced by realtive id, so easier to add and reference cells
- Master worker gets data from PMd cells and broadcasts it to other workers
- With PMd data, 30 workers over 10 nodes, and 10 scale (7846 cells), this model (6sec sim) runs in real-time (6sec).
- Added P population (proprioceptive from virtual arm) and udp interface to arm
** List of changes by me
- Tidied up code and merged with cliff's tutorial code
- Included generic stimulation code based on classes, eg. class for 'natural touch', class for 'optogenetic'

* 14sep22 RL in M1 model
** stdp.mod implementation
- adapted from george's cleanmodel by cliff
- includes STDP and RL as 2 diff mechanisms -- not dopamine-based STDP! - need to modify
- To implement RL in model need to run reward_punish from each element of stdpmechs (instantiations of stdp.mod = weight
adjuster) eg. every 100 ms: for s in stdpmechs: s.reward_punish
** RL interval?
- error used RL rewards should include difference etween current and previous time step (eg. 5ms) or previous RL update (50
  or 100 ms) ??
- in arm2dms it was errro with previous time step (10 ms) -- but I think it should be prev RL update or maybe Eligibility
  trace interval or motor command window !?
- No EM lag because aready included in the musculoskeletal arm
* 15jan11 Learning targets (reward signal)  :paper:
** reward-modulated STDP between biological neurons and model neurons
- different reward signal to different L2 subpopulations depending on target
- similar to Koca15
- PMd population = 128 neurons = biological neurons
- P population = proprioceptive = PPC / Thalamus (from virtual arm)
- To speed up training: 1) play back vector of PMd inputs; 2) use simple kinematic arm (once working replace with
  musculoskeletal and retrain)
- Start with just 2 targets (left, right); if working move to 4 targets
*** Training: different options from more realistic to more practical
- Feed PMd data (for different targets) and for each one enforce exploratory movements over all targets
- STDP + RL when hand getting closer to correct target
- Plasticity only between PMd->L2; L5/CSP -> Spinal Cord; P->L2 ??
- What connections will be reinforced: those linking PMd data corresponding to target X, with the arm movements to target X
- Need to divide training and testing dataset?
- In L2/3, the accuracy of neuronal ensemble prediction of lever trajectory remained unchanged globally, with a subset of
  individual neurons retaining high prediction accuracy throughout the training period. However, in L5a, the ensemble
  prediction accuracy steadily improved, and one-third of neurons, including subcortical projection neurons, evolved to
  contribute substantially to ensemble prediction in the late stage of learning. The L2/3 network may represent coordination
  of signals from other areas throughout learning, whereas L5a may participate in the evolving network representing
  well-learned movements.(Masamizu et al, 2014)

*** Connectivity: different options from more realistic/autonomous, to more hard-wired/easy to learn
**** PMd -> L2 (all-to-all) overlapping with P -> L2 (all-to-all)
**** PMd -> L2 (all-to-50%), P -> L2 (all-to-other 50%)

* 15jan28 Focus on this model now (2 months to final demo) first steps: mpi, circuitry, conceptual framework
** Debug msarm.py so can run model
- modified msarm to use self. for most variables in run method
- 'randomOutput' arm runs fast, but dummyArm quite slow because has to search/collect spikes to generate motor command
** Test mpi in mac
*** Salvador-Duras-MacBook-Pro% mpiexec -n 4 nrniv -python -mpi model.py
ssh: Could not resolve hostname Salvador-Duras-MacBook-Pro: nodename nor servname provided, or not known
^C[mpiexec@Salvador-Duras-MacBook-Pro] Sending Ctrl-C to processes as requested [mpiexec@Salvador-Duras-MacBook-Pro] Press
Ctrl-C again to force abort [mpiexec@Salvador-Duras-MacBook-Pro] HYDU_sock_write (./utils/sock/sock.c:291): write error (Bad
file descriptor) [mpiexec@Salvador-Duras-MacBook-Pro] HYD_pmcd_pmiserv_send_signal (./pm/pmiserv/pmiserv_cb.c:170): unable to
write data to proxy [mpiexec@Salvador-Duras-MacBook-Pro] ui_cmd_cb (./pm/pmiserv/pmiserv_pmci.c:79): unable to send signal
downstream [mpiexec@Salvador-Duras-MacBook-Pro] HYDT_dmxu_poll_wait_for_event (./tools/demux/demux_poll.c:77): callback
returned error status [mpiexec@Salvador-Duras-MacBook-Pro] HYD_pmci_wait_for_completion (./pm/pmiserv/pmiserv_pmci.c:197):
error waiting for event [mpiexec@Salvador-Duras-MacBook-Pro] main (./ui/mpich/mpiexec.c:331): process manager error waiting
for completion
*** Salvador-Duras-MacBook-Pro% mpiexec
[mpiexec@Salvador-Duras-MacBook-Pro] set_default_values (./ui/mpich/utils.c:1542): no executable provided
[mpiexec@Salvador-Duras-MacBook-Pro] HYD_uii_mpx_get_parameters (./ui/mpich/utils.c:1751): setting default values failed
[mpiexec@Salvador-Duras-MacBook-Pro] main (./ui/mpich/mpiexec.c:153): error parsing parameters Salvador-Duras-MacBook-Pro%
which mpiexec /usr/local/bin/mpich3/bin/mpiexec

*** Salvador-Duras-MacBook-Pro% brew install mpich - works
==> Installing dependencies for mpich2: cloog, gfortran Error: You must `brew link isl' before cloog can be installed
Salvador-Duras-MacBook-Pro% brew link isl Linking /usr/local/Cellar/isl/0.12.1... 9 symlinks created
Salvador-Duras-MacBook-Pro% brew install mpich ==> Installing dependencies for mpich2: cloog, gfortran ==> Installing mpich2
dependency: cloog ==> Downloading https://downloads.sf.net/project/machomebrew/Bottles/cloog-0.18.1.mavericks.bottle.1.tar.gz
######################################################################## 100.0% ==> Pouring
cloog-0.18.1.mavericks.bottle.1.tar.gz ðŸº /usr/local/Cellar/cloog/0.18.1: 33 files, 556K ==> Installing mpich2 dependency:
gfortran ==> Downloading https://downloads.sf.net/project/machomebrew/Bottles/gfortran-4.8.2.mavericks.bottle.1.tar.gz
######################################################################## 100.0% ==> Pouring
gfortran-4.8.2.mavericks.bottle.1.tar.gz ==> Caveats Formulae that require a Fortran compiler should use: depends_on :fortran
==> Summary ðŸº /usr/local/Cellar/gfortran/4.8.2: 960 files, 113M ==> Installing mpich2 ==> Using Homebrew-provided fortran
compiler.  This may be changed by setting the FC environment variable.  ==> Downloading
http://www.mpich.org/static/downloads/3.1/mpich-3.1.tar.gz
######################################################################## 100.0% ==> ./configure --disable-silent-rules
--prefix=/usr/local/Cellar/mpich2/3.1 --mandir=/usr/local/Cellar/mpich2/3.1/share/man ==> make ==> make install ðŸº
/usr/local/Cellar/mpich2/3.1: 457 files, 15M, built in 3.3 minutes Salvador-Duras-MacBook-Pro%
*** Warning: detected user attempt to enable MPI, but MPI support was disabled at build time.

** Install NEURON in mac
http://www.neuron.yale.edu/neuron/download/compilestd_osx

brew install mpich build.sh config: ./configure --with-iv=$IVB --prefix=$ND --with-nrnpython=dynamic CC='gcc-4.6'
CCX='g++-4.6' --with-paranrn https://discussions.apple.com/thread/3406578 make make install python setup.py install
--home=/usr/arch/nrn/share/python

*** clean steps from scratch
- install required libraries via brew (list of libs?)
- brew install open-mpi
- cd $NSRC; ./build.sh
-./configure --with-iv=$IVB/iv --prefix=$ND --with-nrnpython=dynamic CC='gcc-4.6' CCX='g++-4.6' --with-paranrn=dynamic
- sudo bash
- export ARCHFLAGS='-arch i386 -arch x86_64'
- cd $ND
- make
- make install
- cd $NB/src/nrnpython
- python setup.py install --home=/usr/arch/nrn/share/python

**** didnt work (see error here)
Salvador-Duras-MacBook-Pro% m1ms Salvador-Duras-MacBook-Pro% /usr/local/Cellar/open-mpi/1.7.4/bin/mpirun -n 4 nrniv -python
-mpi model.py dyld: Library not loaded: /usr/local/lib/libpmpich.12.dylib Referenced from:
/usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not found dyld: Library not loaded: /usr/local/lib/libpmpich.12.dylib
Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not found dyld: Library not loaded:
/usr/local/lib/libpmpich.12.dylib Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not found dyld:
Library not loaded: /usr/local/lib/libpmpich.12.dylib Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason:
image not found -------------------------------------------------------------------------- mpirun noticed that process rank 2
with PID 24333 on node Salvador-Duras-MacBook-Pro exited on signal 5 (Trace/BPT trap: 5).
-------------------------------------------------------------------------- Salvador-Duras-MacBook-Pro% nrniv dyld: Library
not loaded: /usr/local/lib/libpmpich.12.dylib Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not
found Trace/BPT trap

**** try specifying open-mpi folder - worked!
 ./configure --with-iv=$IVB/iv --prefix=$ND --with-nrnpython=dynamic CC='gcc-4.6' CCX='g++-4.6'
 --with-paranrn=/usr/local/Cellar/open-mpi/1.7.4/

** Define inputs to M1 circuitry :paper:
*** what layer do proprioceptive inputs (via spinal cord+thalamus) target?
- thalamic inputs to upper layers (Weiler et al,2008; Kiritani et al, 2010)
- Thalamocortical inputs from anterior, motor-related thalamic regions (VA/VL) with cerebellar afferents -> L2/3, L5A, L5B (IT+PT)
  (Hooks et al, 2013)
- Posterior sensory-related thalamic areas (POm) -> L2/3 and L5A (Hooks et al, 2013)
- Inputs from sensory-related cortical and thalamic areas preferentially target the upper-layer pyramidal neurons in
  vM1.(Hooks et al, 2013)
- VL axons in the cortex excited both IT and PT neurons (Yamawaki & Shepherd, 2015)
- Area 2 receives its main input from area 1 as well as from the VPS, which is the main relay nucleus for proprioceptive
  information in the monkey. (Francis 2009); Until recently, the rat homolog of the VPS had not been identified, which is
  surprising given the wide spread use of the rat as an animal model. (Francis 2009) --> In macaque proprioceptive info via
  VPS
- Mapped out a region in the rostral VPL of the rat that responds preferentially to joint manipulation and muscle palpation
  (Francis et al. 2008) --> In rat proprioceptive info via VPL

*** what layer do PMd inputs target?
- cortical inputs to upper layers (Weiler et al,2008; Kiritani et al, 2010)
- Orbital cortex (OC) -> L6
- Secondary motor cortex (M2) -> L5B
- Inputs from OC and M2, areas associated with volitional and cognitive aspects of movements, bypass local circuitry and have
  direct monosynaptic access to neurons projecting to brainstem and thalamus.
- In macaque input from PMd seems to go primarily to upper layers (layer 1?) and spread across the rest (based on fig 2 Shipp, 2005) 
- In macaque PMd->M1 target ~55% deep layers and ~45% superficial layers (1-3) (Dum & Strick, 2005)
- In rhesus monkey more than 90% of all labeled neurons within the premotor and motor cortices were found in layer 3; the rest in
  layers 5 and 6 (Barbas & Panday, 1987)

- "In L2/3, the accuracy of neuronal ensemble prediction of lever trajectory remained unchanged globally, with a subset of
  individual neurons retaining high prediction accuracy throughout the training period. However, in L5a, the ensemble
  prediction accuracy steadily improved, and one-third of neurons, including subcortical projection neurons, evolved to
  contribute substantially to ensemble prediction in the late stage of learning. The L2/3 network may represent coordination
  of signals from other areas throughout learning, whereas L5a may participate in the evolving network representing
  well-learned movements" (Masamizu et al, 2014)

** Cell and neuron densities in the primary motor cortex of primates (Young,2013)
- 50,000 neurons / mm2
** Added new spinal cord populations
*** code
-popnames = ['PMd', 'ASC', 'DSC', 'ER2', 'IF2', 'IL2', 'ER5', 'EB5', 'IF5', 'IL5', 'ER6', 'IF6', 'IL6'] popclasses = [-1, -1,
--1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3] # Izhikevich population type popEorI = [ 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1] # Whether
-it's excitatory or inhibitory popratios = [numPMd, 48, 48, 150, 25, 25, 167, 72, 40, 40, 192, 32, 32] # Cell population
-numbers
*** description
- PMd = input from PMd with target/reward information; NSLOCs; reproduces PMd recorded data
- ASC = Ascending Spinal Cord; proprioceptive info; encode x-y speed (previously P population encoding muscle lengths); 
- DSC = Descending Spinal Cord; muscle excitations; receives input from EB5 (all-to-all mapping)
  
** Proprioceptive population encodes cartesian direction and velocity :papers:
*** From Roll04
It is widely recognized nowadays that sensory information produced by muscle spindles constitutes a crucial part of
proprioception (Cordo 1990; Gandevia 1996; Gandevia and Burke 1992; Roll 2003). As far as the sensory level is concerned, one
might mention recent studies examining the coding of two-dimensional pointing and drawing movements, the results of which
have shown that muscle spindle population activity is strongly correlated with both the direction and the velocity of the
ongoing movement under both passive and active conditions (Bergenheim et al. 2000; Roll et al. 2000; Jones et al. 2001). In
addition, these studies showed that each muscle spindle is sensitive to a specific range of movement directions (the
so-called preferred sensory sector, PSS), and shows maximum sensitivity to a specific direction (denoted the preferred
sensory direction, PSD). The PSS and the PSD of the various muscle spindles within a given muscle are quite similar, which
makes it possible to calculate an average PSD and PSS for that muscle. Each muscle has its own PSD and PSS which differs from
those of other muscles.  When examining the PSS of the muscle groups acting on the ankle joint, it was observed that they
overlapped in such a way that, together, they covered the whole range of possible movement directions in that particular
joint (Bergenheim et al. 2000; Roll et al. 2000). The authors of the latter studies concluded that the proprioceptive
information arising from all muscles surrounding a joint was needed for accurate sensory and perceptual coding to be
performed throughout the whole movement.  In other studies, using a similar population vector model to that used by Schwartz
(1992, 1993) at the cortical level, it was established that the â€œsum vectorâ€ of all the oriented and weighted activity from
the whole population of muscle spindles in all the muscles acting on a given joint, accurately describes the instantaneous
direction and velocity of the ongoing movement in two dimensional space (Bergenheim et al. 2000; Roll et al. 2000; Jones et
al. 2001; Ribot-Ciscar et al. 2002).

*** From Bosc01 cited in Fran09
XV. SUMMARY

A.â€‚ Role of Limb Biomechanics in Global Limb Representations

We have presentedhere a possible framework for interpreting proprioceptive signals at the spinal level. It is based on the
premise that global limb information rather than localized receptor-like proprioceptive information is encoded by the nervous
system. Within a basic global framework, information is encoded by a distributed system in which each neural element may
still bias the global information according to some local detail. For example, the DSCT data suggest that details about
stiffness at a single joint might be contained in a population signal that encodes a representation of the limb end point
that may then depend on the joint covariance resulting from specific levels of joint stiffness.

This global sensory representation is not organized entirely by the neural circuitry however. It begins in the periphery with
the biomechanical structure of the limb. Biomechanical constraints ensure that theactivity from individual sensory receptors
will be correlated in certain ways that depend on whole limb parameters. Therefore, even a minimum of central sensory
convergence could lead to global representations with this peripheral apparatus (37,38, 281).

B.â€‚ Significance of Kinematic-Based Representations

We also suggest that this framework could be based on limb kinematics. If so, it is noteworthy that while many participating
sensory receptors are associated with muscles and some even specifically tuned to muscle force, nevertheless their ensemble
is capable of encoding limb kinematics. In other words, inputs from receptors located in individual muscles or associated
deep structures as well as in the skin are assembled at very early stages of central processing to provide a representation
of limb kinematics. Because this occurs at these earliest stages, it suggests that the peripheral apparatus may also in some
way play a role in itsdetermination. The result, however, is that centrally directed sensory information may be encoded in a
framework common to that of central motor activity that relates to limb kinematics. It may therefore be analogous to the
situation in the superior colliculus where sensory information from various modalities is mapped congruently within a
retinotopic map (227, 306) that may be modified or transformed by gaze (123, 124,157, 158); that is, the sensory information
is combined and integrated through a common coding framework. Although the retinal projection may provide the basis for a
common framework for eye, head, and body movement control, limb biomechanics and associated proprioceptors appear to provide
the basis for a common framework for limb movement control.
*** From Berg00
The results show that each muscle spindle afferent, and likewise each muscle, has a specific preferred sensory direction, as
well as a preferred sensory sector within which it is capable of sending sensory information to the central nervous
system. Interestingly, the results also demonstrate that the preferred directions are the same as the directions of
vibration-induced illusions. In addition, the results show that the neuronal population vector model describes the
multipopulation proprioceptive coding of spatially oriented 2D limb movements, even at the peripheral sensory level, based on
the sum vectors calculated from all the muscles involved in the movement.
 
** PMd input with multiple targets requires new conceptual framework! :papers:
- PMd provides preparation activity -- where to go; target info with respect to hand
- Learning adpats weight to map different PMd activity to M1 activity that directs arm to different targets
- ASC population encodes proprioceptive info from arm (direction and velocity) and visual feedback from eyes (arm position;
  or arm - target position) -- doesn't make sense because ASC (=spinal cord) doesn't contain vision; would have to call it
PPC -- if only encode direction (population code for angle) and velocity (amplitude as firing rate), system shouldn't be able
to tell difference if placed in different starting point; however this may not be required because input from PMd is guiding
movement (ie. telling system, go to the right/left etc., not specifying target location!), so can argue hand/target position
more relevant for PMd (?!)  -- when pmd activity dies off (reached target), so should M1 activity?


- "We demonstrate that an MI ensemble can reconstruct hand or joint trajectory more accurately than an equally sized PMd
  ensemble.In contrast, PMd can more precisely predict the future occurrence of one of several discrete targets to be
  reached.These results also support the hierarchical view that MI ensembles are involved in lower-level movement execution,
  whereas PMd populations represent the early intention to move to visually presented targets." Hatsopoulos, 2004

- Above also possible argument for developing model of M1 -- PMd activity itself doesn't provide accurate position/vel representation

* 15feb06 dummyArm
- dummyArm was independent python executable which used udp messages to communicate with model
- udp obsolete (now pipes)
- replaced dummyArm with same code but running within model (no udp or pipes)
- requires all same RL apparatus; after that working test muscskel
* 15feb09 MPI issues: gidDic, motor commands, and RL
** gidVec vs gidDic
- CHECK gidVec vs gidDic
- gidVec is vector local to each node, where index=local id, and value = gid
- gidDic is dictionary local to each node, where key=global id, value = local id 
- redundant! but using gidVec.index() to get the local id is very slow (~300x slower) -- so use gidDic to get local id
*** speed comparison from gil
gidvec.index() takes so long time.  Test for vec.index and dictionary import time import random vec = [] for c in
range(10000): vec.append(c) dic = { x:x for x in range(10000)} seq = range(10000) random.shuffle(seq) measure = time.time()
for c in seq: a = vec.index(c) measure = time.time() - measure print 'vec.index:', measure measure = time.time() for c in
seq: b = dic[c] measure = time.time() - measure print 'dic:', measure

vec.index: 1.80838108063 dic: 0.00332403182983

*** speed comparison in m1 model
10k cells, 1sec sim, 16 cores, with gidVec.index() = 153 sec 10k cells, 1sec sim, 16 cores, with gidDic = 25.9 sec = x5.9
speedup
** differences when using mpi with different number of nodes due to motor commands implementation
- not present when arm is off
- not present when proprio is off -> due to proprio -- not true, also present with propio off 
- joint angles different in 1 vs 16 cores
- not due to broadcasting error - same before and after
- difference in motor commands! - maybe due to timing?
- FOUND: error originates in these 2 lines used to speed up sim by removing past spikes from list:

#self.hostspiketimes = self.hostspiketimes[self.hostspiketimes > (h.t - 2*max(self.shtimewin,self.eltimewin))] # remove
#unncessary old spikes self.hostspikecells = self.hostspikecells[self.hostspiketimes > (h.t - 
2*max(self.shtimewin,self.eltimewin))] # remove unncessary old spikes

JUST HAD TO SWITCH AROUND TO AVOID SPIKETIMES GETTING DELETED BEFORE USING IT TO SEARCH SPIKECELLS!!

- still diffs when add RL

** differences when using mpi with different number of nodes due to RL (STDP BUG AND FIX)
- different num of spikes when using 1 vs >1 cores
- was only changing weights in worker0
- seems error related to STDP  -- also happens when RL off
- doesn't happen when STDP off
- also happens in cliff tutorial code
- can reproduce easily with this code: http://neuron.yale.edu/neuron/static/courses/cns2014/large-scale.zip , scale=4,
 duration=3, 1 vs 8 mpi cores 
- also tested in neurosim (zn) /u/salvadord/Documents/ISB/Models/large-scale/
- also happens when using scale=3, dur=3, 4 vs 12 cores (Spikes: 14031 vs 14003)
- compared output: difference in weightchanges (38/83311), spikes (28/14031) and lfp; rest the same:
        distances: [164138x1 double]
             EorI: [3000x1 int64]
          lfptime: [600x1 double]
            ylocs: [3000x1 double]
         cellpops: [3000x1 int64]
         stimdata: []
           delays: [164138x1 double]
      cellclasses: [3000x1 int64]
        cellnames: [3000x3 char]
      connweights: [15x15x4 double]
        connprobs: [15x15 double]
      connections: [164138x2 double]
            xlocs: [3000x1 double]
          weights: [164138x4 double]
          simcode: {7x1 cell}
    weightchanges: {83311x1 cell}
         stdpdata: [83311x3 double]
        spikedata: [14031x2 double]
            zlocs: [3000x1 double]
             lfps: [600x6 double]
*** chat with cliff
cliff I find differences in the number of spikes when using 1 core vs >1 core did u have this problem?  I tested the sim you
used for tutorial and also reproduced it there eg. 1 core = 19771 spikes; 8 cores = 19758 spikes Cliff Kerr hmm that's
strange -- i used to have that problem but it got fixed at some point, could've gotten broken again though...  it had to do
with the random seeds being initialized differently Salvador Dura so how did u fix? maybe I ahve old version hmm ok, I'll
check that I was thinking it was stdp related cause doesn't seem to happen when stdp off -- but need to check more thoroughly
was thinking maybe related to stdp happening between cells in different cores, but just speculation Cliff Kerr each cell
should have its own random number generator linked to gid but it's possible i haven't checked for stdp Salvador Dura so the
random generator error u had, was related to using different number of cores?  Cliff Kerr yeah Salvador Dura cause if I use
the same number of cores, the result is always the same ok it seems when stdp off, error doesnt happen Cliff Kerr
interesting...  Salvador Dura I'll check the rand gen and the stdp code, see if I can find anything Cliff Kerr anyway my
feeling is that it's probably not a big deal, i.e. each one is equally valid, but yeah agree they should match Salvador Dura
@equally valid - yeah probably, but just need for reproducibility of results -- small error could carry forward in time I
guess
sal:can reproduce easily with this code: http://neuron.yale.edu/neuron/static/courses/cns2014/large-scale.zip , scale=4, duration=3, 1 vs 8 mpi cores
cliff: i guess you could check that the stdp connections and weights are the same in the 1 and 8 core cases?

*** more systematic tests
**** scale=2, dur=2, 1 vs 2 cores
- differences in weightchanges:
conn    pre    post     weightchanges (1 core)                          weightchanges (2 cores) gid (1core) gid (2cores)
645	602	35	[0,2.63385014695182;1005,2.61453901661609]	[0,2.63385014695182]	[0,0]	[1,0]
646	672	35	[0,0.486465429624879;1005,0.467154299289152]	[0,0.486465429624879]	[0,0]	[1,0]
3326	532	178	[0,2.65632182511313;1005,2.64518027212770]	[0,2.65632182511313]	[0,0]	[1,0]
3540	639	188	[0,1.82277902626864;1005,1.81531061995854]	[0,1.82277902626864]	[0,0]	[1,0]

- weight decreases due to antiHebb learning
- spike times:
cell    spk time
602	540

35	514
35	540

672	540
672	1278.50000000000
672	1689.50000000000

- occurs when spk time same in pre and post
- fixed (do differences now) by changing stdp.mod :
if  ((tlastpost > -1) && (interval != 0))  -->  if  ((tlastpost > -1) && (interval > 0.0))
if  ((tlastpre > -1) && (interval != 0.0))  -->  if  ((tlastpre > -1) && (interval > 0.0)) 

**** tested using scale=4, dur=4, 1 vs 16 cores --> >10k weightchange diffs and 82 more spikes

**** test scale scale=2, dur=2, 1 vs 16 cores --> 22 wc diffs, 1 spike more:
conn    pre    post     weightchanges (1 core)                          weightchanges (2 cores) gid (1core) gid (2cores)
349	134	11	[0,2.86023987946172]	[0,2.86023987946172;1005,2.90268947422707]	[0,0]	[1,0]
654	358	22	[0,0.149294195666084;1005,0.305054352280365]	[0,0.149294195666084]	[0,0]	[2,0]
3246	536	106	[0,2.73408129675365]	[0,2.73408129675365;1005,2.73408129675365]	[0,0]	[4,0]
3278	1269	107	[0,1.74122404294975]	[0,1.74122404294975;1005,1.74122404359625]	[0,0]	[10,0]
6487	362	218	[0,1.34847151295259]	[0,1.34847151295259;1005,1.37991894621531]	[0,0]	[2,1]
7360	499	249	[0,1.79714956561962]	[0,1.79714956561962;1005,1.85738840800206]	[0,0]	[3,1]
8881	1203	300	[0,1.68536212703375]	[0,1.68536212703375;1005,1.74266308640579]	[0,0]	[9,2]
17022	9	578	[0,1.61835650498962]	[0,1.61835650498962;1005,1.62882444617930]	[0,0]	[0,4]
17023	22	578	[0,2.35002265490450]	[0,2.35002265490450;1005,2.36728137220437]	[0,0]	[0,4]
17024	29	578	[0,1.70413279263212]	[0,1.70413279263212;1005,1.70487236537541]	[0,0]	[0,4]
17025	121	578	[0,1.33952623608263]	[0,1.33952623608263;1005,1.34080810277188]	[0,0]	[0,4]
17028	202	578	[0,1.07834345749672]	[0,1.07834345749672;1005,1.07920271843487]	[0,0]	[1,4]
17029	217	578	[0,0.629349148726955]	[0,0.629349148726955;1005,0.648422981170065]	[0,0]	[1,4]
17032	325	578	[0,1.41251536458843]	[0,1.41251536458843;1005,1.41367524553380]	[0,0]	[2,4]
17033	363	578	[0,0.665475706621114]	[0,0.665475706621114;1005,0.666293060908807]	[0,0]	[2,4]
17034	397	578	[0,2.45872497666071]	[0,2.45872497666071;1005,2.47285521927279]	[0,0]	[3,4]
17036	434	578	[0,1.07763999597961]	[0,1.07763999597961;1005,1.07834349933459]	[0,0]	[3,4]
17037	493	578	[0,0.145238375107857]	[0,0.145238375107857;1005,0.146015866556552]	[0,0]	[3,4]
17038	525	578	[0,1.42405633633909]	[0,1.42405633633909;1005,1.42491559727724]	[0,0]	[4,4]
17043	849	578	[0,20.7127766032884]	[0,20.7127766032884;1005,20.7249386158134]	[0,0]	[6,4]
23626	896	827	[0,2.36270498436227]	[0,2.36270498436227;1005,2.36270498436295]	[0,0]	[7,6]
30949	1285	1221	[0,0.0225237576864728]	[0,0.0225237576864728;1005,0.0225237576892504]	[0,0]	[10,9]

- spike times:
---------------- (same t)
134	709
134	724.500000000000 *
134	1215
134	1507.50000000000
134	1522
134	1936.50000000000

11	704
11	724.500000000000 *
11	1220
11	1502
11	1522.50000000000
11	1932

-------------- (different t, but diff spikes)
358	761.500000000000 *
358	1264.50000000000
358	1559

(2 cores)
22	728
22	764 *
22	1267
22	1524
22	1548

(1 core)
22	728
22	756.500000000000
22	1267
22	1524
22	1548

---------------- (same t)
536	553.500000000000 *
536	570.500000000000
536	902 *
536	1198.50000000000

106	546.500000000000 *
106	902 *
106	1202

---------------- (same t)
362	728 *
362	1242
362	1505
362	1526
362	1936

218	728 *
218	1139
218	1216
218	1524.50000000000

*** tried modifying stdp.mod to increase interval
if  ((tlastpre > -1) && (interval > 0.01)) - 22 diffs
if  ((tlastpre > -1) && (interval > 0.1)) - 22 diffs 
if  ((tlastpre > -1) && (interval > 1)) - 26 diffs
if  ((tlastpre > -1) && (interval > 2.0)) - 26 diffs
if  ((tlastpre > -1) && (interval > 2.0)) - 13 diffs

*** found problem!
17.47 found problem: when pre is in different node and happens same time as post, the pre net_receive event sometimes arrives after post, so pre time is still previous spike time of that cell and thus stdp happens this is quite common since all spk times interval of 0.5 ms !

*** fixed problem!
20.44 found fix for stdp bug: instead of updating w directly use net_send() to update 1ms later and check for simultaneous
spike; tested with 10-sec 10k cell sim for 1 vs 10 cores and both identical; code here:
/u/salvadord/Documents/ISB/Models/large-scale/stdp.mod  

*** Code with fix for bug
COMMENT

STDP + RL weight adjuster mechanism

Original STDP code adapted from:
http://senselab.med.yale.edu/modeldb/showmodel.asp?model=64261&file=\bfstdp\stdwa_songabbott.mod

Adapted to implement a "nearest-neighbor spike-interaction" model (see 
Scholarpedia article on STDP) that just looks at the last-seen pre- and 
post-synaptic spikes, and implementing a reinforcement learning algorithm based
on (Chadderdon et al., 2012):
http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0047251

Modified by salvadord to avoid bug when simultaneous pre and post spikes occur in different nodes (with mpi)

Example Python usage:

from neuron import h

## Create cells
dummy = h.Section() # Create a dummy section to put the point processes in
ncells = 2
cells = []
for c in range(ncells): cells.append(h.IntFire4(0,sec=dummy)) # Create the cells

## Create synapses
threshold = 10 # Set voltage threshold
delay = 1 # Set connection delay
singlesyn = h.NetCon(cells[0],cells[1], threshold, delay, 0.5) # Create a connection between the cells
stdpmech = h.STDP(0,sec=dummy) # Create the STDP mechanism
presyn = h.NetCon(cells[0],stdpmech, threshold, delay, 1) # Feed presynaptic spikes to the STDP mechanism -- must have weight >0
pstsyn = h.NetCon(cells[1],stdpmech, threshold, delay, -1) # Feed postsynaptic spikes to the STDP mechanism -- must have weight <0
h.setpointer(singlesyn._ref_weight[0],'synweight',stdpmech) # Point the STDP mechanism to the connection weight

Version: 2013oct24 by cliffk

ENDCOMMENT

NEURON {
    POINT_PROCESS STDP : Definition of mechanism
    POINTER synweight : Pointer to the weight (in a NetCon object) to be adjusted.
    RANGE tauhebb, tauanti : LTP/LTD decay time constants (in ms) for the Hebbian (pre-before-post-synaptic spikes), and anti-Hebbian (post-before-pre-synaptic) cases. 
    RANGE potrate, deprate : Maximal adjustment (can be positive or negative) for Hebbian and anti-Hebbian cases (i.e., as inter-spike interval approaches zero).  This should be set positive for LTP and negative for LTD.
    RANGE RLwindhebb, RLwindanti : Maximum interval between pre- and post-synaptic events for an starting an eligibility trace.  There are separate ones for the Hebbian and anti-Hebbian events.
    RANGE useRLexp : Use exponentially decaying eligibility traces?  If 0, then the eligibility traces are binary, turning on at the beginning and completely off after time has passed corresponding to RLlen.
    RANGE RLlenhebb, RLlenanti : Length of the eligibility Hebbian and anti-Hebbian eligibility traces, or the decay time constants if the traces are decaying exponentials.
    RANGE RLpotrate, RLdeprate : Maximum gains to be applied to the reward or punishing signal by Hebbian and anti-Hebbian eligibility traces.  
    RANGE wmax : The maximum weight for the synapse.
    RANGE softthresh : Flag turning on "soft thresholding" for the maximal adjustment parameters.
    RANGE STDPon : Flag for turning STDP adjustment on / off.
    RANGE RLon : Flag for turning RL adjustment on / off.
    RANGE verbose : Flag for turning off prints of weight update events for debugging.
    RANGE tlastpre, tlastpost : Remembered times for last pre- and post-synaptic spikes.
    RANGE tlasthebbelig, tlastantielig : Remembered times for Hebbian anti-Hebbian eligibility traces.
    RANGE interval : Interval between current time t and previous spike.
    RANGE deltaw : The calculated weight change.
    RANGE newweight : New calculated weight.
}

ASSIGNED {
    synweight        
    tlastpre   (ms)    
    tlastpost  (ms)   
    tlasthebbelig   (ms)    
    tlastantielig  (ms)        
    interval    (ms)    
    deltaw
    newweight          
}

INITIAL {
    tlastpre = -1            : no spike yet
    tlastpost = -1           : no spike yet
    tlasthebbelig = -1      : no eligibility yet
    tlastantielig = -1  : no eligibility yet   
    interval = 0
    deltaw = 0
    newweight = 0
}

PARAMETER {
    tauhebb  = 10  (ms)   
    tauanti  = 10  (ms)    
    potrate = 1.0
    deprate = -1.0
    RLwindhebb = 10 (ms)
    RLwindanti = 10 (ms)
    useRLexp = 0   : default to using binary eligibility traces
    RLlenhebb = 100 (ms)
    RLlenanti = 100 (ms)
    RLpotrate = 1.0
    RLdeprate = -1.0
    wmax  = 15.0
    softthresh = 0
    STDPon = 1
    RLon = 1
    verbose = 0
}

NET_RECEIVE (w) {
     deltaw = 0.0 : Default the weight change to 0.

    : Hebbian weight update happens 1ms later to check for simultaneous spikes (otherwise bug when using mpi)
    if ((flag == -1) && (tlastpre != t-1)) {   
        w = 0
        deltaw = potrate * exp(-interval / tauhebb)   : Use the Hebbian decay to set the Hebbian weight adjustment. 
        if (softthresh == 1) { deltaw = softthreshold(deltaw) } : If we have soft-thresholding on, apply it.
        if (verbose > 0) { printf("Hebbian STDP event: t = %f ms; tlastpre = %f ms; interval = %f; deltaw = %f\n",t,tlastpre,interval,deltaw) } : Show weight update information if debugging on.
    }

    : Ant-hebbian weight update happens 1ms later to check for simultaneous spikes (otherwise bug when using mpi)
    else if ((flag == 1) && (tlastpost != t-1)) { :update weight 1ms later to check for simultaneous spikes (otherwise bug when using mpi)
        w = 0
        deltaw = deprate * exp(interval / tauanti) : Use the anti-Hebbian decay to set the anti-Hebbian weight adjustment.
        if (softthresh == 1) { deltaw = softthreshold(deltaw) } : If we have soft-thresholding on, apply it.
        if (verbose > 0) { printf("anti-Hebbian STDP event: t = %f ms; deltaw = %f\n",t,deltaw) } : Show weight update information if debugging on. 
    }
     
    : If we receive a non-negative weight value, we are receiving a pre-synaptic spike (and thus need to check for an anti-Hebbian event, since the post-synaptic weight must be earlier).
    if (w > 0) {           
        interval = tlastpost - t  : Get the interval; interval is negative
        if  ((tlastpost > -1) && (interval > 0.0)) { : If we had a post-synaptic spike and a non-zero interval...
            if (STDPon == 1) { : If STDP learning is turned on...
                net_send(1,1) : instead of updating weight directly, use net_send to check if simultaneous spike occurred (otherwise bug when using mpi)    
            }
            if ((RLon == 1) && (-interval <= RLwindanti)) { tlastantielig = t } : If RL and anti-Hebbian eligibility traces are turned on, and the interval falls within the maximum window for eligibility, remember the eligibilty trace start at the current time.
        }
        tlastpre = t : Remember the current spike time for next NET_RECEIVE.  
    
    : Else, if we receive a negative weight value, we are receiving a post-synaptic spike (and thus need to check for an anti-Hebbian event, since the post-synaptic weight must be earlier).    
    } else if (w < 0) {            
        interval = t - tlastpre : Get the interval; interval is positive
        if  ((tlastpre > -1) && (interval > 1.0)) { : If we had a pre-synaptic spike and a non-zero interval...
            if (STDPon == 1) { : If STDP learning is turned on...
                net_send(1,-1) : instead of updating weight directly, use net_send to check if simultaneous spike occurred (otherwise bug when using mpi)
            }
            if ((RLon == 1) && (interval <= RLwindhebb)) { tlasthebbelig = t } : If RL and Hebbian eligibility traces are turned on, and the interval falls within the maximum window for eligibility, remember the eligibilty trace start at the current time.
        }
        tlastpost = t : Remember the current spike time for next NET_RECEIVE.
    }
    adjustweight(deltaw) : Adjust the weight.
}

PROCEDURE reward_punish(reinf) {
    if (RLon == 1) { : If RL is turned on...
        deltaw = 0.0 : Start the weight change as being 0.
        deltaw = deltaw + reinf * hebbRL() : If we have the Hebbian eligibility traces on, add their effect in.   
        deltaw = deltaw + reinf * antiRL() : If we have the anti-Hebbian eligibility traces on, add their effect in.
        if (softthresh == 1) { deltaw = softthreshold(deltaw) }  : If we have soft-thresholding on, apply it.  
        adjustweight(deltaw) : Adjust the weight.
        if (verbose > 0) { printf("RL event: t = %f ms; deltaw = %f\n",t,deltaw) } : Show weight update information if debugging on.     
    }
}

FUNCTION hebbRL() {
    if ((RLon == 0) || (tlasthebbelig < 0.0)) { hebbRL = 0.0  } : If RL is turned off or eligibility has not occurred yet, return 0.0.
    else if (useRLexp == 0) { : If we are using a binary (i.e. square-wave) eligibility traces...
        if (t - tlasthebbelig <= RLlenhebb) { hebbRL = RLpotrate } : If we are within the length of the eligibility trace...
        else { hebbRL = 0.0 } : Otherwise (outside the length), return 0.0.
    } 
    else { hebbRL = RLpotrate * exp((tlasthebbelig - t) / RLlenhebb) } : Otherwise (if we re using an exponential decay traces)...use the Hebbian decay to calculate the gain.
      
}

FUNCTION antiRL() {
    if ((RLon == 0) || (tlastantielig < 0.0)) { antiRL = 0.0 } : If RL is turned off or eligibility has not occurred yet, return 0.0.
    else if (useRLexp == 0) { : If we are using a binary (i.e. square-wave) eligibility traces...
        if (t - tlastantielig <= RLlenanti) { antiRL = RLdeprate } : If we are within the length of the eligibility trace...
        else {antiRL = 0.0 } : Otherwise (outside the length), return 0.0.
    }
    else { antiRL = RLdeprate * exp((tlastantielig - t) / RLlenanti) } : Otherwise (if we re using an exponential decay traces), use the anti-Hebbian decay to calculate the gain.  
}

FUNCTION softthreshold(rawwc) {
    if (rawwc >= 0) { softthreshold = rawwc * (1.0 - synweight / wmax) } : If the weight change is non-negative, scale by 1 - weight / wmax.
    else { softthreshold = rawwc * synweight / wmax } : Otherwise (the weight change is negative), scale by weight / wmax.    
}

PROCEDURE adjustweight(wc) {
   synweight = synweight + wc : apply the synaptic modification, and then clip the weight if necessary to make sure its between 0 and wmax.
   if (synweight > wmax) { synweight = wmax }
   if (synweight < 0) { synweight = 0 }
}

*** Time differences
**** 1 core with fix
  Done; run time = 129.9 s; real-time ratio: 0.08.

Gathering spikes...
  Done; gather time = 27.0 s.
Minimum delay (time-step for queue exchange) is  10.0

Analyzing...
  Spikes: 189371 (1.89 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output1.0...
  Done; time = 28.2 s

Done; total time = 267.4 s.
**** 10 cores with fix 
  Done; run time = 33.7 s; real-time ratio: 0.30.

Gathering spikes...
  Done; gather time = 17.1 s.
Minimum delay (time-step for queue exchange) is  1.0

Analyzing...
  Spikes: 189371 (1.89 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output10.0...
  Done; time = 54.2 s

Done; total time = 118.2 s.
**** 1 core without fix (bug)
  Done; run time = 105.4 s; real-time ratio: 0.09.

Gathering spikes...
  Done; gather time = 71.0 s.
Minimum delay (time-step for queue exchange) is  10.0

Analyzing...
  Spikes: 131236 (1.31 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output1.0...
  Done; time = 55.1 s

Done; total time = 271.1 s.
**** 10 cores without fix (bug)
  Done; run time = 34.0 s; real-time ratio: 0.29.

Gathering spikes...
  Done; gather time = 23.2 s.
Minimum delay (time-step for queue exchange) is  1.0

Analyzing...
  Spikes: 123575 (1.24 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output10.0...
  Done; time = 56.8 s

Done; total time = 129.3 s.
* 15feb10 Sim working with STDP, RL + musculoskeletal arm in hpc (ma)
** end of output from running in 'ma'
Writing to MSM pipe: packetID=199.000000
[0.5, -0.5, 0.5, -0.5]
read from msm pipe: 184
[0.128276, 0.0939427, 0.118085, 0.0700224, 0.201749, 0.185767, 0.244255, 0.0703298, 0.154234, 0.141332, 0.135775, 0.107342, 0.145956, 0.10446, 0.101539, 0.126831, 0.154701, 0.0776188]
read from msm pipe: 19
[-0.175841, 1.41304]
Received packet 199.000000 from MSM: (0.128,0.094,0.118,0.070,0.202,0.186,0.244,0.070,0.154,0.141,0.136)
Received packet 199.000000 from MSM: (-0.176,1.413)
  t = 2.0 s (100%; time remaining: 0.0 s)

Writing to MSM pipe: packetID=200.000000
[0.5, -0.5, 0.5, -0.5]
read from msm pipe: 184
[0.128332, 0.0939427, 0.118054, 0.0700076, 0.201694, 0.185664, 0.244182, 0.0702832, 0.154242, 0.141379, 0.135819, 0.10737, 0.145849, 0.104454, 0.101537, 0.126407, 0.154472, 0.0776281]
read from msm pipe: 19
[-0.176098, 1.41247]
Received packet 200.000000 from MSM: (0.128,0.094,0.118,0.070,0.202,0.186,0.244,0.070,0.154,0.141,0.136)
Received packet 200.000000 from MSM: (-0.176,1.412)
  Done; run time = 78.8 s; real-time ratio: 0.03.

Gathering spikes...
  Done; gather time = 5.1 s.

Analyzing...
  Run time: 78.8 s (2-s sim; 2 scale; 1838 cells; 1 workers)
  Spikes: 25857 (7.03 Hz)
  Connections: 45993 (45693 STDP; 25.02 per cell)
  Mean connection distance: 794.83 um
  Mean connection delay: 9.95 ms
Saving output as data/m1ms...
  Done; time = 5.4 s
Plotting raster...
  Done; time = 1.8 s
Plotting connectivity matrix...
Plotting weight changes...

Done; total time = 398.9 s.
** output with mpi
 Done; run time = 9.1 s; real-time ratio: 0.22.

Gathering spikes...
  Done; gather time = 2.1 s.

Analyzing...
  Run time: 9.1 s (2-s sim; 2 scale; 1838 cells; 4 workers)
  Spikes: 25870 (7.04 Hz)
  Connections: 45993 (45693 STDP; 25.02 per cell)
  Mean connection distance: 794.83 um
  Mean connection delay: 9.95 ms
Saving output as data/m1ms...
  Done; time = 5.5 s
  Plotting raster despite using too many cores (4)

* 15feb14 Sharing variables between modules using shared.py
- shared.py will contain all objects that need to be shared across modules (network.py, analysis.py, arm.py), including
  static parameters (duration, popnames,...), and variables modified during runtime (cells, spikerecorders, ...)
- https://docs.python.org/2/faq/programming.html#how-do-i-share-global-variables-across-modules
- https://docs.python.org/2/faq/programming.html#what-are-the-best-practices-for-using-import-in-a-module
- http://docs.python-guide.org/en/latest/writing/structure/#modules
- http://stackoverflow.com/questions/19158339/python-why-are-global-variables-evil

* 15feb17 Spinal cord motor neuron using Izhikevich and reciprocal inhibition to antagonistic
** chat
Salvador Dura
trying to find what would be the best izhikevich parameters for spinal cord motor neuronâ€¦ any ideas?
[samnemo] suppose he doesn't describe those types of neurons in his paper...anything specific you want to capture in its firing pattern?
Salvador Dura
@types in paper â€” not specifically
@capture - right now using pyramidal so thought might be better to make a bit more accurate
also right now the spinal population following the L5B oscillationsâ€¦ so thought maybe different cell type would generate different output pattern to muscles
[samnemo] @following - is it helpful for the performance to follow the oscillations?
Salvador Dura
@helpful - I don't really know yet, but doesn't happen in real-life right? (thats the whole hypothesis of m1 grant â€¦ oscillations -> rate coding)
[samnemo] physiological (or parkinson's) tremor...@hypothesis - temporal -> rate code, true, but maybe it's wrong :) , does temporal -> rate code exclude all oscillations at the periphery?
[samnemo] anyway, haven't worked with those neurons...maybe ben/bill can comment
Salvador Dura
@exclude osc - probably not (eg phys tremor) â€¦ but likely to be diff freqs â€” of course my model is a super simplification â€¦ missing eg. all interneurons and a thousand other things, so that also explains why sync to layer 5b
thx
[samnemo] k was just curious about whether osc. helped since that's one of the debates re. oscillations - whether they have a function
Salvador Dura
might doâ€¦ I'm adapting model to run evol alg on hpc and tune params for reaching
neurosim-isb@im.partych.at
[samnemo] sounds cool
** izhi links
http://www.izhikevich.org/publications/spikes.pdf
http://izhikevich.org/publications/nesb.pdf
http://www.izhikevich.org/publications/hybrid_spiking_models.pdf
http://www.izhikevich.org/publications/whichmod.htm
http://www.izhikevich.org/publications/spikes.htm

** reciprocal inhib to antagon muscle
- "The afferent of the muscle spindle bifurcates in the spinal cord. One branch innervates the alpha motor neuron that causes
  the homonymous muscle to contract, producing the reflex. The other branch innervates the inhibitory interneuron, which in
  turn innervates the alpha motor neuron that synapses onto the opposing muscle. Because the interneuron is inhibitory, it
  prevents the opposing alpha motor neuron from firing, thereby reducing the contraction of the opposing muscle. "
- Implemented programatically during readout of DSC pop:
if s.antagInh: # antagonist inhibition
                if self.motorCmd[SH_EXT] > self.motorCmd[SH_FLEX]: # sh ext > sh flex
                    self.motorCmd[SH_FLEX] =  self.motorCmd[SH_FLEX]**2 / self.motorCmd[SH_EXT] / s.antagInh
                elif self.motorCmd[SH_EXT] < self.motorCmd[SH_FLEX]: # sh flex > sh ext
                    self.motorCmd[SH_EXT] = self.motorCmd[SH_EXT]**2 / self.motorCmd[SH_FLEX] / s.antagInh
                if self.motorCmd[EL_EXT] > self.motorCmd[EL_FLEX]: # el ext > el flex
                    self.motorCmd[EL_FLEX] = self.motorCmd[EL_FLEX]**2 / self.motorCmd[EL_EXT] / s.antagInh
                elif self.motorCmd[EL_EXT] < self.motorCmd[EL_FLEX]: # el ext > el flex
                    self.motorCmd[EL_EXT] = self.motorCmd[EL_EXT]**2 / self.motorCmd[EL_FLEX] / s.antagInh
- Alternative would be to implement inhibitory interneuron
** Burke 2013 Spinal motoneurons review
[[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150219_204957.png][fig]] - steady firing 20-60 Hz

** ModelDB motoneurons
- https://senselab.med.yale.edu/ModelDb/ModelList.cshtml?id=276&celldescr=no
- maybe can try to tune izhi params to IF curve if have time!
- online simulation of spinal network and muscles - http://remoto.leb.usp.br/remoto/index.html
- firing rates 10-30 Hz

* 15feb18 Manually tuning network to learn 1 target
- velsh = shflex - shext (by default flexes = increases = shflex higher)
- velel = elflex - elext (by default extends = decreases = elext higher)
- middle subpopulations higher activity -- some bias??
- explor movs now change on a 10ms basis -- too fast 
- added explor movs and antagonistic
- difficult to tune by hand -- will try evol

* 15feb19 Added 3d distance for delays
Added 3d distance:
zpath=(abs(s.zlocs-s.zlocs[gid]))**2
distances3d = sqrt(xpath + ypath + zpath)
s.conndata[3].append(s.mindelay + distances3d[preids]/float(s.velocity))
mean connection delay went from 10.83ms to 11.30ms
in 1k cell 5-sec sim, spikes went from 40979 (8.48 Hz) to 33627 (6.95 Hz)
raster shows pretty similar oscillations

* 15feb20 Setup evol alg
- test input parameters from main -- cannot use * !
- copy over evol island and adapt 
- add params automatically in a loop - make flexible

* 15feb21 sim - 2 targets (single), 1k neurons, dummyArm 	    :results:
** changeset
changeset:   105:a993e07e853c
user:        Salvador Dura <salvadordura@gmail.com>
date:        Mon Feb 23 22:43:14 2015 -0500
summary:     fixed bug - lasttimes were not reseted during setup

** evol.py params
imdatadir = 'data/15feb21_evol' # folder to save sim results
saveMuscles = 0
num_islands = 10 # number of islands
numproc = 4 # number of cores per job
max_migrants = 1 #
migration_interval = 5
pop_size = 10 # population size per island
num_elites = 1 
max_generations = 1000
max_evaluations = max_generations *  num_islands * pop_size
mutation_rate = 0.4
crossover = 0.5

# parameter names and ranges
pNames = []
pRanges = []
pNames.append('trainTime'); pRanges.append([30*1e3,180*1e3]) #int
pNames.append('plastConnsType'); pRanges.append([0,1,2,3]) # int
pNames.append('stdpFactor'); pRanges.append([0,1])
pNames.append('RLfactor'); pRanges.append([0,4])
#pNames.append('stdpwin'); pRanges.append([10,30])
pNames.append('eligwin'); pRanges.append([50,150])
#pNames.append('RLinterval'); pRanges.append([50,100])
#pNames.append('maxweight'); pRanges.append([15,75])
pNames.append('trainBackground'); pRanges.append([50,200])
pNames.append('testBackground'); pRanges.append([50,200])
#pNames.append('minRLerror'); pRanges.append([0.0,0.01])
pNames.append('cmdmaxrate'); pRanges.append([5,20])
pNames.append('cmdtimewin'); pRanges.append([50,150])
pNames.append('explorMovsFactor'); pRanges.append([1,10])
#pNames.append('explorMovsDur'); pRanges.append([500,1500])

** results (bugs)
*** bug
-- bug: not calculating arm angles properly --due to self.startAng=self.ang and modifying self.ang -- need to use
list(self.ang)
-- bug:  lasttimes were not reseted during setup
*** island_3 gen_18_can_5 (Target errors:  [ 0.15352302  0.06460844]  avg:  0.109065727645)
*** .run
Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
/tera/salvadord/m1ms/sim
numprocs=4

Setting parameters...
Benchmarking...
  Running at 45% default speed (182% total)
  Setting outfilestem=../data/15feb21_evol_island_3/gen_18_cand_5
  Setting trainTime=180000.0
  Setting plastConnsType=3.0
  Setting stdpFactor=0
  Setting RLfactor=4
  Setting eligwin=102.96167326867445
  Setting trainBackground=78.96491328878155
  Setting testBackground=132.36448561938408
  Setting cmdmaxrate=17.369441210963718
  Setting cmdtimewin=52.541630731296245
  Setting explorMovsFactor=1.262821609816099

Creating simulation of 967 cells for 1.0 s on 4 hosts...
  Number of cells on node 2: 242 
  Number of cells on node 1: 242 
  Number of cells on node 3: 241 
  Number of cells on node 0: 242 
Calculating connection probabilities (est. time: 88 s)...
  Number of connections on host 1: 3959
  Number of connections on host 2: 3842
  Number of connections on host 3: 3771
  Done; time = 0.1 s
Making connections (est. time: 198 s)...
  Number of connections on host 0: 3938
  Number of STDP connections on host 3: 1665
  Number of STDP connections on host 2: 1715
  Number created on host 3: 193
  Number of STDP connections on host 1: 1797
  Number of STDP connections on host 0: 1772
  Done; time = 0.2 s
Creating background inputs...
  Number created on host 2: 194
  Number created on host 1: 194
  Number created on host 0: 194

Setting up STDP...

Setting up virtual arm...

Running...
  t = 5.0 s (2%; time remaining: 202.0 s)
  t = 10.0 s (5%; time remaining: 160.3 s)
  t = 15.0 s (8%; time remaining: 173.6 s)
  t = 20.0 s (11%; time remaining: 185.0 s)
  t = 25.0 s (13%; time remaining: 187.9 s)
  t = 30.0 s (16%; time remaining: 193.1 s)
  t = 35.0 s (19%; time remaining: 190.0 s)
  t = 40.0 s (22%; time remaining: 186.2 s)
  t = 45.0 s (25%; time remaining: 181.0 s)
  t = 50.0 s (27%; time remaining: 177.9 s)
  t = 55.0 s (30%; time remaining: 173.3 s)
  t = 60.0 s (33%; time remaining: 167.8 s)
  t = 65.0 s (36%; time remaining: 160.7 s)
  t = 70.0 s (38%; time remaining: 153.0 s)
  t = 75.0 s (41%; time remaining: 146.3 s)
  t = 80.0 s (44%; time remaining: 139.7 s)
  t = 85.0 s (47%; time remaining: 134.4 s)
  t = 90.0 s (50%; time remaining: 127.3 s)
  t = 95.0 s (52%; time remaining: 119.1 s)
  t = 100.0 s (55%; time remaining: 112.2 s)
  t = 105.0 s (58%; time remaining: 105.5 s)
  t = 110.0 s (61%; time remaining: 98.2 s)
  t = 115.0 s (63%; time remaining: 91.2 s)
  t = 120.0 s (66%; time remaining: 84.1 s)
  t = 125.0 s (69%; time remaining: 77.3 s)
  t = 130.0 s (72%; time remaining: 70.5 s)
  t = 135.0 s (75%; time remaining: 63.2 s)
  t = 140.0 s (77%; time remaining: 56.0 s)
  t = 145.0 s (80%; time remaining: 49.0 s)
  t = 150.0 s (83%; time remaining: 41.8 s)
  t = 155.0 s (86%; time remaining: 34.7 s)
  t = 160.0 s (88%; time remaining: 27.8 s)
  t = 165.0 s (91%; time remaining: 20.9 s)
  t = 170.0 s (94%; time remaining: 13.9 s)
  t = 175.0 s (97%; time remaining: 6.9 s)
  t = 180.0 s (100%; time remaining: 0.0 s)
  Done; run time = 247.7 s; real-time ratio: 0.73.

Gathering spikes...
  Done; gather time = 7.5 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 247.7 s (180-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 4561007 (26.20 Hz)
  Connections: 15510 (6949 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms

Setting up virtual arm...

Running...
  Done; run time = 0.7 s; real-time ratio: 1.38.

Gathering spikes...
  Done; gather time = 3.7 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 0.7 s (1-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 27777 (28.72 Hz)
  Connections: 15510 (0 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms
Saving output as data/m1ms...
  Done; time = 0.0 s

Setting up STDP...

Setting up virtual arm...

Running...
  t = 5.0 s (2%; time remaining: 235.5 s)
  t = 10.0 s (5%; time remaining: 242.3 s)
  t = 15.0 s (8%; time remaining: 217.4 s)
  t = 20.0 s (11%; time remaining: 208.3 s)
  t = 25.0 s (13%; time remaining: 197.5 s)
  t = 30.0 s (16%; time remaining: 186.5 s)
  t = 35.0 s (19%; time remaining: 183.7 s)
  t = 40.0 s (22%; time remaining: 175.9 s)
  t = 45.0 s (25%; time remaining: 165.5 s)
  t = 50.0 s (27%; time remaining: 158.3 s)
  t = 55.0 s (30%; time remaining: 149.9 s)
  t = 60.0 s (33%; time remaining: 145.8 s)
  t = 65.0 s (36%; time remaining: 139.4 s)
  t = 70.0 s (38%; time remaining: 132.3 s)
  t = 75.0 s (41%; time remaining: 128.1 s)
  t = 80.0 s (44%; time remaining: 124.3 s)
  t = 85.0 s (47%; time remaining: 121.2 s)
  t = 90.0 s (50%; time remaining: 116.5 s)
  t = 95.0 s (52%; time remaining: 113.2 s)
  t = 100.0 s (55%; time remaining: 107.3 s)
  t = 105.0 s (58%; time remaining: 101.5 s)
  t = 110.0 s (61%; time remaining: 96.1 s)
  t = 115.0 s (63%; time remaining: 90.5 s)
  t = 120.0 s (66%; time remaining: 85.3 s)
  t = 125.0 s (69%; time remaining: 78.8 s)
  t = 130.0 s (72%; time remaining: 72.0 s)
  t = 135.0 s (75%; time remaining: 65.3 s)
  t = 140.0 s (77%; time remaining: 58.9 s)
  t = 145.0 s (80%; time remaining: 51.8 s)
  t = 150.0 s (83%; time remaining: 44.5 s)
  t = 155.0 s (86%; time remaining: 37.1 s)
  t = 160.0 s (88%; time remaining: 29.8 s)
  t = 165.0 s (91%; time remaining: 22.4 s)
  t = 170.0 s (94%; time remaining: 15.0 s)
  t = 175.0 s (97%; time remaining: 7.5 s)
  t = 180.0 s (100%; time remaining: 0.0 s)
  Done; run time = 271.1 s; real-time ratio: 0.66.

Gathering spikes...
  Done; gather time = 8.7 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 271.1 s (180-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 4778774 (27.45 Hz)
  Connections: 15510 (6949 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms

Setting up virtual arm...

Running...
  Done; run time = 1.2 s; real-time ratio: 0.87.

Gathering spikes...
  Done; gather time = 3.4 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 1.2 s (1-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 26462 (27.37 Hz)
  Connections: 15510 (0 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms
Saving output as ../data/15feb21_evol_island_3/gen_18_cand_5_target_0...
  Done; time = 0.0 s
Target errors:  [ 0.15352302  0.06460844]  avg:  0.109065727645

Done; total time = 544.6 s.

*** right target consistently better (found multiple bugs)
- check what is issue by running locally with same params
- output is different each time - maybe due to id32 hash in rand num generation -- NO (with hash
  still reproducible); was due to arm explor moves randomness 
- the amount of spikes was 50% or 25% x the hpc output -- could be just random
- bias towards left target still present
- Tried increasing EB5->DSC weight to 2.0
- Need to check RL is updating weights properly -- do small systematic test
- plotweightchanges was showing absolute final weight; not absolute change --> negative weight changes to RL working
- check RL + error signal -- didn't seem to be consistent with target distance -- fixed small bug

- Hypothesis of why RL not modifying weights selectively: all motor subpops getting activated similarly (= similar elig
  traces), explor movements being implemented at the level of pop readout
-- solution: implement explor movements by increasing background noise to motor subpops
-- did that but couldn't modify rates on-the-fly because using NetStims instead of NSLOCs
-- having issue with NSLOCs-- doesnt recognize some params -- cannot connect using netcon! compare with netstim.mod -- not sure how fixed 
- Added backgrundgid so can target specific netstim based on gid
- EB5->DSC initial weight was too high (4) so modifying background noise didn't make much of a difference -- lowered to 0.5

- Background input for train and test is the same -- not modified before testing because don't call addBackground

- After first target something makes the stdpmechs stop working -- no change in synWeight
-- not in finalizeSim() or plotData()
-- found! after init() the pre and postcell of the stdpmech gets disconnected so tlastpost always = 0

- Cannot restart sim -- weight changes are different second time around even if I do all same steps (+ clear_gid)
-- even if only stdp (RL off and explor movs off) !! 
-- doesn't even work properly on multiple hosts when restarting everything
-- tried removing s and reloading shared again, but apparently python doesnt support unloading of modules

- Only solution left is to run on single target and sum targets in evol.py -- done

- Check if higher conn for certain muscles is just random -- yes (start with low weights so it doesnt have major effect)

* 15feb22 exploratory movements via E5B; and inhib spinal cord pop
** explor movs via E5B noise
- now dont require direct stimulation of each muscle group randomly, because E5B connected randomly to DSC so can just increase
noise in E5B = more realistic
- also better because makes more sense for RL+stdp: presyn spike before post
- however since homog random noise, all neurons get excited similarly and benefits the initial biased connection to some
  muscles
-- stimulate up to 50% of the EB5 cells - with random duration, and fixed strength (s.backgroundrateExplor = 1000)
-- still difficult to get good exploratory movements
** inhib spinal cord pop
Ia inhibitory interneuron[edit]
Role in reciprocal inhibition during the stretch reflex[edit]
Joints are controlled by two opposing sets of muscles called extensors and flexors that must work in synchrony to allow
proper and desired movement.[11] When a muscle spindle is stretched and the stretch reflex is activated, the opposing muscle
group must be inhibited to prevent from working against the agonist muscle.[9][11] The spinal interneuron called Ia
inhibitory interneuron is responsible for this inhibition of the antagonist muscle.[11] The Ia afferent of the muscle spindle
enters the spinal cord, and one branch synapses on to the alpha motor neuron that causes the agonist muscle to contract.[11]
Thus, it results in creating the behavioral reflex. At the same time, the other branch of the Ia afferent synapses on to the
Ia inhibitory interneuron, which in turn synapses the alpha motor neuron of the antagonist muscle.[11] Since Ia interneuron
is inhibitory, it prevents the opposing alpha motor neuron from firing. Thus, it prevents the antagonist muscle from
contracting.[11] Without having this system of reciprocal inhibition, both groups of muscles may contract at the same time
and work against each other. This results in spending a greater amount of energy as well. In addition, the reciprocal
inhibition is important for mechanism underlying voluntary movement.[11] When the antagonist muscle relaxes during movement,
this increases efficiency and speed. This prevents moving muscles from working against the contraction force of antagonist
muscles.[11] Thus, during voluntary movement, the Ia inhibitory interneurons are used to coordinate muscle
contraction. Further, the Ia inhibitory interneurons allow the higher centers to coordinate commands sent to the two muscles
working opposite of each other at a single joint via a single command.[11] The interneuron receives the input command from
the corticospinal descending axons in such a way that the descending signal, which activates the contraction of one muscle,
causes relaxation of the other muscles.[9][10][11][12]

Added IDSC
E5B->IDSC = E5B->EDSC
IDSC->EDSC = antagonistic muscle
*** playing with params
-Aim is to excite subsets of E5B that excite EDSC+IDSC and activate different muscles (+inhibit antagonistic muscle) to
perform varied exploratory movements.

- One issue is that each E5B neuron projects to different muscles so there is coactivation and little movement.
-- exciting only 2 E5B neuron at a time seems to produce better results. eg. one E5B excites 2 IDSC of ext+flex, so both EDSC
-- will be inhibited
-- maybe can generate random list without repetition so chance to excite all

- Other issue is that oscillatory activity seems to dominate E5B and thus EDSC 
-- if disconnect E5R->E5B and E2R->E5B works better
-- to avoid, increased E5B->DSC background weight to backgroundWeightExplor = 10 and backgroundrateExplor = 3000

- Also EDSC pyramidal neurons slower to respond to E5B input (if replace with inh FS, response is faster)
-- increased connweights[EB5,EDSC,AMPA]=8.0 compared to connweights[EB5,IDSC,AMPA]=3.0

- Initial burst of excitatory activity due to transitory regime (delay E5B->IDSC->EDSC)
-- start moving arm only 100 ms after onset

- Inhibition IDSC->EDSC not working 
-- maybe becasue each IDSC inhibits a single EDSC unit and that unit may not be the one active -- should inhibit all units of
antagonistic pop -- yes, that worked

- Works well wehn use exploratory movement directly in ESDC -- but had to set noise to 0 so all neurons were recruited!!
  [[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150303_215937.png][fig]]



backgroundrateExplor = 3000 # weight for background input for exploratory movements
backgroundweightExplor = 10 # weight for background input for exploratory movements (fixed)
explorCellsFraction = 0.1 # fraction of E5B cells to be actiavated at a time during explor movs
connweights[EB5,EDSC,AMPA]=8.0
connweights[EB5,IDSC,AMPA]=3.0
connweights[IDSC,EDSC,GABAA]=4.0 

* 15feb23 PMd data and decoders
** Austin - Kernel method (60% 8 targets)
*** chat with Adi
i am running the pmd analysis right now
Austin â€¢ 2/5/14, 1:50 PM
soundsgood
Aditya Tarigoppula
Austin Brockmeier
looks like I can get 60% using spike kernels to classifiy the first 0.5 s
or closer to 50%
actually
Austin â€¢ 2/5/14, 1:50 PM
and this center out ?
Aditya Tarigoppula
Austin Brockmeier
yeah
Austin â€¢ 2/5/14, 1:50 PM
or reward?
Aditya Tarigoppula
Austin Brockmeier
Zee
Austin â€¢ 2/5/14, 1:50 PM
ohh ok zee
Aditya Tarigoppula
Austin Brockmeier
so 1/8 chance
Austin â€¢ 2/5/14, 1:50 PM
yeah
Aditya Tarigoppula
Austin Brockmeier
what was marius getting?
Austin â€¢ 2/5/14, 1:51 PM
thats pretty good
let me check
whats the file name again
?
Aditya Tarigoppula
Austin Brockmeier
58% is what i got on this run across 20 monte carlos divisions of the trials into 2/3 for training and 1/3 for testing
Austin â€¢ 2/5/14, 1:51 PM
is it ZEE from march
Aditya Tarigoppula
Austin Brockmeier
MAP2zee03252010003-01
Austin â€¢ 2/5/14, 1:51 PM
of 2010?
ok
Aditya Tarigoppula
Austin Brockmeier
yeah
Austin â€¢ 2/5/14, 1:52 PM
so here is what he said but I am not sure how much he got on this file specifically
For the manual task, the decoder goes to about 25-30% at 300ms after the target appears, 300ms being the fixed time before the centre cue disappears and the NHP is free to move. Performance keeps improving to about 40-50% at 500ms and levels off at about this value. This kind of performance seems in line with what we were getting on the old center-out data from the Shenoy group. With 100 units and longer delay periods (500-1000ms), we typically decode around 60-70% (also out of 8 targets), though in the best dataset this goes up to 100% fairly quickly after target onset.
so the ZEE march data was collected by pratik
just FYI
2/5/14, 1:53 PM
Aditya Tarigoppula
Austin Brockmeier
ok
i will get you the results and pretty plots by end of week?
*** email to austin
Hi Austin,

How is it going? How's your new job in the UK?

Here we are getting ready for DARPA's "super final demo" probably in a month or so. After that I'm starting in a new NIH project with Bill (modeling M1 in detail), so I'll stay in the cold NY a couple more years :)

I'm trying to decode target position from PMd data to feed into our spiking models, and Adi forwarded me some of the results you were getting using the kernel method... here's what you said:

"Austin Brockmeier
looks like I can get 60% using spike kernels to classifiy the first 0.5 s
Austin Brockmeier
58% is what i got on this run across 20 monte carlos divisions of the trials into 2/3 for training and 1/3 for testing
Austin â€¢ 2/5/14, 1:51 PM
is it ZEE from march
MAP2zee03252010003-01"

It would be great if you could send me that code so I can try it out. Since Stanford/UCL are not the most collaborative guys, we might as well end up using our own code :) Of course if we end up using it for the final demo we will include your name in the paper.

hope everything is going great
cheers mate! ;)
salva

*** emails from Austin and link to code
It will take me a little bit to extract all of the utility functions that go into it (a couple common packages such as SVM are also used).

I have attached some of the text and results extracted from a draft of my dissertation. They are probably what Adi is
referring too. From the visualizations, tou can see that the 60% decoding comes from mostly a left versus right sort of
division.

Yes, the matlab code would be great. I will need to plug the output of your decoder into the python/NEURON-based BMM somehow,
so might have to play with your code. Regarding the funcs, if easier you can just send me the full set of utility functions,
and I can download the common packages (eg. SVM). Thanks, I appreciate it.

Thanks for the thesis results too -- they look good, especially since I will probably end up using only either 2 or 4 targets
(since its a pretty complex task combining everything).

So far I have found the code that (1) extracts windows of spike trains from the data Adi provided recorded from Zee (2) then
I have a script that works in batch mode to compute the full distance matrix (slowest part) (3) runs 10 monte carlo cases to
get test set error

Here is a dropbox link to the code for the spike kernel based target classifier

https://www.dropbox.com/sh/vbrxj3sq4ftox1l/AADHWziKp98WRwPU7lkKm2Yfa?dl=0


The kernel I used is called the memoryless cross-intensity.. It is nothing more than smoothing the spike trains with Gaussian
window and computing the inner-product. The kernel size on the Gaussian is 1/10 Hz and I use a 1 second window following
target presentation

I didn't include the original data from Adi, but I did include the script I used to extract the windows and the extracted
windows themselves (as MATLAB datafilee)

Requires:  minFunc     and  libsvm    
               
Hope this helps and let me know of what is missing or further explanation
** Marius (UCL) - Poisson SSM, Recursive Linear Model (RLM) -- better+faster than GPFA
*** email 1
I am afraid I was not able to get any decoding at all out of the passive observation task. In this email I will tell you what I did, maybe you can tell me if I am not using the data correctly. First, some information about the decoder. 

I used the (target) decoder that I have had most success with on some of the old datasets. I have found this type of decoder to work better than various neural network based decoders and SVM based decoders, mostly because it directly takes into account the temporal dynamics of the responses, both the dynamics in response to stimuli/behaviour, and the noisy and shared dynamics on a trial by trial basis. 

The decoder models the spiking data with one of our Poisson state-space model (the RLM which I have been using in the work with Cliff as well), which has a different PSTH for each condition K, but the same dynamics model to account for the noise correlations. This model gives each trial a probability P(data(1:t) | condition) where t is any time index, and on test data we can decode by picking the condition which gives the data the highest probability.

For the manual task, the decoder goes to about 25-30% at 300ms after the target appears, 300ms being the fixed time before the centre cue disappears and the NHP is free to move. Performance keeps improving to about 40-50% at 500ms and levels off at about this value. This kind of performance seems in line with what we were getting on the old center-out data from the Shenoy group. With 100 units and longer delay periods (500-1000ms), we typically decode around 60-70% (also out of 8 targets), though in the best dataset this goes up to 100% fairly quickly after target onset. 
*** email 2
Hi Adi,

I thought I had replied to this, apparently not. Sorry about the delay. Please see my comments below!

Cheers, Marius


On 7 January 2014 17:23, Aditya Tarigoppula <aditya30887@gmail.com> wrote: Good Morning Marius,

Happy New Year to you too! I have included Salva in this conversation because he will be directly involved in integrating
SSMs with BMMs.

The initial results stated by you are not entirely unexpected. The monkey from which I am recording now is naive with respect
to the cursor. This monkey was never trained to manually perform the center out reaching task, hence he never got a chance to
incorporate the cursor into his motor map (i.e form an association between the cursor and his hand's end point). These files
were recorded as part of a different line of study which also requires us to see if the data showed any modulation with
respect to target/direction.

The preliminary results we have on the passive data are as follows: 

To begin with, We treated it as a binary classification problem of left versus right (we excluded targets 3 and 4), targets
1,5, and 8 were considered as right and targets 2,6,7 were considered left.  Austin (a PHd student in Dr. Principe's lab) ran
multiple Monte Carlo divisions of the dataset into training and test sets of size 2/3 samples per class and 1/3 samples per
class, respectively. He computed the average across 100 Monte Carlo divisions and also computed the chance rate. This work
was done on LFPs of coupe of files and he is working on improving these results.

On dataset 12/4 (which I can send over to you if you desire) file 1, both M1 and Pmd show above chance classification 58% for
PmD and 57% for motor.

Motor cortex shows 56% and 60% on files 12/4 file 2 and 12/9 file 3 respectively. All the rest of the classification rates
are below chance.  If you can send these over I can tell you the performance of the RLM decoder on them, to make sure we are
looking at the same data in similar ways. The RLM decoder can also take LFPs as input, and even combine the LFP information
with the spiking information. However, maybe I'm missing some obvious information, or maybe the RLM decoder is for some
reason crap on this kind of data --- we will know it If I cannot achieve your classification performance.

Having said that, there is one thing that can be done to improve the quality of the input data. I should have included this
information in my last email, sorry about that! Eye tracking was used to make sure that the cursor moved towards the target
only when the monkey's gaze was maintained on the task plane (i.e. I am not sure if he looking at he cursor but I am sure
that he is looking at the monitor where the cursor is moving). The trial is not aborted unless the maximum trial time alloted
to the task runs out. Therefore, there is a possibility that you might have also used the neural data pertaining to the error
trials while you performed your analysis. I would suggest considering the trials that end up with states greater than 8 as
INVALID trials (its valid even for the file corresponding to the manual task). i.e. A successfully completed trial usually
has the following sequence of states 1-2-3-4-5-6/7-8-0. If a given trial has a state above 8 or another way to put it is if a
given trial does not have a state 8 then the trial was an incomplete/aborted trial.

I have removed that already. 

Do you think you have enough trials (statistically speaking) for each target? (for both manual and passive files) 

There are enough trials to observe statistically significant differences, yes. I interpret the fact that the RLM decoder is
completely insignificant on the previous data as saying there's really no information there, at least no linearly encoded
information. I think almost all of the time even simple linear decoders will have some significant classification
performance. Nonlinear / more fancy decoders can improve on this, but it's very rare in a classification task to have an
insignificant linear decoder and a significant nonlinear decoder.

I recently recorded a passive observation 2 target center out reaching task. This allows us to have atleast 40 trials for
each target. Would you like to have a look at the PMd data for this file?
 
Would you like to try your decoder on the M1 data to see if that gives a better performance. As far as the milestone is
concerned it states that BMMs can use input from the unlesioned part of M1. This allows us to treat the M1 region that we
recorded from as the unlesioned region.

Sure, I can tell you if the RLM finds anything significant on both these PmD and M1 files, but as I said above, we should
first make sure I calibrate it in comparison to your previous positive results for those M1 and PmD datasets.
 
How would you compare RLM with other models that Dr. Sahani's lab has proposed in the past such as HSLDS?

It's important to distinguish the use of the RLM as a statistical model of the spiking and as a decoder, although on the same
data, the same RLM can be trained to perform both tasks.

As a statistical model of the spiking, the RLM seems to do better than Poisson LDS, but we've never directly compared it to
Biljana's hidden switching model (if that's what you meant by HSLDS). The RLM is however in the same broad class of hidden
state space models, with the advantage of being much faster and direct to train. In particular we can optimize the model's
likelihood exactly.

As a decoder, on the old center-out data from the Shenoy lab, the RLM achieves 6.5mm average error, where our best previous
decoder (mixture of trajectories model, Byron Yu's work, cover of J Neurphys methods a couple of years ago) achieved 11mm
average error.

I agree with your view on using a longer delay period. I also agree with your view on making the monkey more invested in the
task and to basically make him aware of the task paradigm. We are moving towards it.

Cool. I think it will be scientifically very interesting to see the passive and active neural activity in well-trained
attentive monkeys.
** Adi/John - Byron's GPFA, John's GLDS, PLDS
*** email
I believe all of you have access to my Google folder called "shared with Babak (Manual files)". This folder consists of the .plx files (M1 files), .mat files which contains information extracted from the plx files using code present in the sub folder 'loading routines', the definitions and description of the variables in the mat files are stated in the 'var_defs.txt' in the loading routines folder. Feel free to call me or email me if you have any further questions with regard to these files. 

There are 8 targets in the task. The targets are located 45 deg apart from each other.The Target locations of each of the targets and the center circle can be found in the variable called "c3d_Struct.TP_TABLE.X_Global / Y_Global".

c3d_Struct.TP_TABLE.X_Global(1), c3d_Struct.TP_TABLE.Y_Global(1) = center location

Following which index 2 - 9 gives you location of the 8 targets. 
 
% for visualization purposes of the targets
for i = 1 : 8
plot(c3dstruct.TARGET_TABLE.X_GLOBAL(c3dstruct.TP_TABLE.EndTarget(i)),c3dstruct.TARGET_TABLE.Y_GLOBAL(c3dstruct.TP_TABLE.EndTarget(i)),'*'); 
pause();hold on;
end

targets are arranged as follows with respect to the center: 1 is to the right (0 deg), 2 is to the left (180), 3 is up (90), 4 is down (270), 5 is at 45 deg, and then move anticlockwise for 6 (135), 7 (225) and 8th (315) target. 

There is more information about the target radius, task paradigm etc in c3d_struct. I will guide you to it if you ever need it.

c3dstruct.CALIBRATION has the lengths of monkey arm. Look at 'Description' for more details. For each sub variable in struct "c3d_struct" there is description available. 

Also attached is the GPFA code provided to us by Dr. Yu (Byron Yu). John developed codes for GLDS and PLDS using their papers as guide. Please ask John to send you those codes too. I am not sure if I applied GPFA to these files. But I can always apply it and send you the results when I do. 

Please keep me posted on the evolving collaboration with Dr. Shenoy's group. I would like to help in any way possible to make
this successful. 

*** chat with john - bitbucket link 
the farthest i got was applying it to predicting shoulder and elbow kinematics
i think the top level scripts for testing were
runEncodignModels.m
and runEncodingModelsWithPos.m
https://bitbucket.org/nasnysom/pldsglds

i think it requires minFunc - http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html
also: http://research.microsoft.com/en-us/um/people/minka/software/lightspeed/
*** GPFA code
- well documented but many files/funcs
- example script helpful
- not target decoder -- models neural dyanmics in lower dim space (like jPCA)
- can decode target using: " This model gives each trial a probability P(data(1:t) | condition) where t is any time index,
  and on test data we can decode by picking the condition which gives the data the highest probability."
- alternatively can use lower-dimension represntation as input to BMM - STDP between SSM output units and BMM

*** Johns PLDS GLDS code
- trouble isntalling lightspeed

Compiling lightspeed 2.7 mex files...
xcodebuild: error: SDK "macosx10.7" cannot be located.
xcrun: error: unable to find utility "clang", not a developer tool or in PATH

    mex: compile of ' "flops.c"' failed.

tried: http://www.mathworks.com/matlabcentral/answers/103904-can-i-use-xcode-5-as-my-c-or-c-compiler-in-matlab-8-1-r2013a-or-matlab-8-2-r2013b

now get:
>> install_lightspeed
Compiling lightspeed 2.7 mex files...
/u/salvadord/.matlab/R2013a/mexopts.sh: line 176: unexpected EOF while looking for matching `''
/u/salvadord/.matlab/R2013a/mexopts.sh: line 205: syntax error: unexpected end of file
/u/salvadord/.matlab/R2013a/mexopts.sh: line 176: unexpected EOF while looking for matching `''
/u/salvadord/.matlab/R2013a/mexopts.sh: line 205: syntax error: unexpected end of file
/Applications/MATLAB_R2013a.app/bin/mex: line 1343: -c: command not found

    mex: compile of ' "flops.c"' failed.

Error using mex (line 206)
Unable to complete successfully.

Error in install_lightspeed (line 52)
eval(['mex' flags '-c flops.c']);
** EMG data
*** Austin: MAP2zee03252010003-01
- same as M1 data used in paper -- would be nice to use the same PMd data!!
**** notes (Brandi)
Brandi & Shaohua
======================

3/25/2010

Task: Mar06NewAngF50cart4cmR50

KinARM was recalibrated today by Mulugeta and Brandi.

Zee spent about 20 minutes on the task before recording started. [110]
File 1: about 7.5 minutes, Zee performs the task well. 75 rewards [185]
File 2: about 7.5 minutes, performs the task well. about 70 rewards [260]
File 3: about 10 minutes, performs the tasks well. about 95 rewards [361]
File 4: about 9-10 minutes, performs the tasks well. about 95 rewards [458]
File 5: about 8-9 minutes, performs the tasks well. about 80 rewards [543]
File 6: about 8-9 minutes, performs the tasks well. about 80 rewards [628]
File 7: about 9 minutes, performs the tasks well. about 90 rewards [725]

video: zee20100325.

c3d file saved today.

She got about 140 ml water today.

Brandi & Shaohua
======================


*** Gil: MAP2zee04292009002_6sec.plx
** Playing back PMd data 
*** vec.play
- also vec.record
- not sure why need patternStim -- seems to be a unit (similar to netstim / NSLOC)
*** bill's new method?
patternstim
yes in my mpi example
cellrun.py
/u/billl/nrniv/talks/mpiHHnetTut/cellrun.py
pattern = h.PatternStim() # /usr/site/nrniv/nrn/src/nrnoc/pattern.mod
pattern.play(tvec, idvec)
have to do things in correct order -- i messed that up first time around

* 15mar01 sim - 2 targets (single), 1k neurons, dummyArm  :results:
** changeset
changeset:   119:07393792f1c7
user:        Salvador Dura <salvadordura@gmail.com>
date:        Mon Mar 02 01:59:52 2015 -0500
summary:     prepared for 15mar01_evol - debug
** description
debugged 15feb21
explor movs using E5B noise

* 15mar04 sim - 2 targets (single), 1k neurons, dummyArm - testing different algorithms  :results:
** changesets
*** genetic
changeset:   129:f3d6287f5a3d
user:        Salvador Dura <salvadordura@gmail.com>
date:        Wed Mar 04 01:12:08 2015 -0500
summary:     prepared for 15mar04_sim
*** krichmarEvol
changeset:   130:d525bfa9242b
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Wed Mar 04 10:47:18 2015 -0500
summary:     prepared for 15mar04_sim_krichmarEvol

*** evolutionStrategy
changeset:   132:0d7b4034b783
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Wed Mar 04 22:27:27 2015 -0500
summary:     15mar04_sim evolutionStrategy
*** simulatedAnnealing
changeset:   133:dfce13a74937
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Thu Mar 05 11:42:16 2015 -0500
summary:     15mar04 simulatedAnnealing
*** diffEvolution
changeset:   135:ce27a7a92f0f
user:        Salvador Dura <salvadordura@gmail.com>
date:        Thu Mar 05 17:12:15 2015 -0500
summary:     15mar04 diffEvolution

** description
debugged 15mar01
explor moves via EDSC+IDSC
reduced num params being optim
testing different optim methods from inspyred
** results
*** genetic (0.1462, 0.151)
**** pop=2
0, 2, 0.300133783635, 0.146268855105, 0.22320131937, 0.22320131937, 0.0769324642647
50, 2, 0.20004290363, 0.146268855105, 0.173155879368, 0.173155879368, 0.0268870242623

80, 2, 0.20004290363, 0.146268855105, 0.173155879368, 0.173155879368, 0.0268870242623
**** pop=100
0, 100, 0.317487304691, 0.159470073482, 0.216980743021, 0.222929138415, 0.0367570158357
21, 100, 0.151096833862, 0.151096833862, 0.151096833862, 0.151096833862, 2.22044604925e-16
*** krichmarEvol (0.138,0.139)
**** pop=2
0, 2, 0.300332272954, 0.143344059494, 0.221838166224, 0.221838166224, 0.0784941067298
50, 2, 0.141606055411, 0.138317739195, 0.139961897303, 0.139961897303, 0.00164415810839
**** pop=100
0, 100, 0.320947708087, 0.162105091636, 0.213414640609, 0.222150583396, 0.034445638365
50, 100, 0.193492181148, 0.139979383936, 0.171605491955, 0.171018238002, 0.00812888089593

75, 100, 0.192731596366, 0.139979383936, 0.172123326834, 0.171688023346, 0.0079540669261
*** evolutionStrategy (0.120, 0.149)
**** pop=2
0, 2, 0.29309852766, 0.14321209304, 0.21815531035, 0.21815531035, 0.0749432173103
50, 2, 0.120482750696, 0.120428230984, 0.12045549084, 0.12045549084, 2.72598558803e-05

97, 2, 0.120482750696, 0.120428230984, 0.12045549084, 0.12045549084, 2.72598558803e-05
**** pop=100
0, 100, 0.318741062513, 0.165325490508, 0.215863412366, 0.221879967525, 0.0326180996345
50, 100, 0.161099087304, 0.149440203003, 0.159304010727, 0.1585991366, 0.00234451905547

86, 100, 0.158996363073, 0.149440203003, 0.157320921931, 0.156541186796, 0.00215095191058
*** simulated annealing (pop=1)
Note: population size is fixed to 1
0, 1, 0.147006731409, 0.147006731409, 0.147006731409, 0.147006731409, 0.0
4, 1, 0.152100649955, 0.152100649955, 0.152100649955, 0.152100649955, 0.0
50 = 0.24 (bug and not being logged)
*** differential evolution (0.148, 0.149)
**** pop=2
Note: population size was 2
0, 2, 0.302776430427, 0.148740419821, 0.225758425124, 0.225758425124, 0.0770180053034
50, 2, 0.361121064674, 0.215502442399, 0.288311753537, 0.288311753537, 0.0728093111373
Changed num_selected to 100
0, 2, 0.302776430427, 0.148740419821, 0.225758425124, 0.225758425124, 0.0770180053034
35, 2, 0.378108681455, 0.190169327724, 0.28413900459, 0.28413900459, 0.0939696768656
**** pop=100
0, 100, 0.315004100446, 0.152035007756, 0.2148703177, 0.220430208369, 0.0354757774148
30, 100, 0.187584364802, 0.149097688801, 0.170677041856, 0.170903622147, 0.00694698013731
50, 100, 0.19157735916, 0.157253114409, 0.172914206251, 0.172316661561, 0.00703620612567
*** Estimation of Distribution 
**** pop=2
0, 2, 0.291259623486, 0.144390895694, 0.21782525959, 0.21782525959, 0.0734343638961
35, 2, 0.16158109519, 0.144390895694, 0.152985995442, 0.152985995442, 0.00859509974791
**** pop=100

*** particle swarm optimization (0.125,0.146)
**** pop=2
0, 2, 0.310104614953, 0.149800351062, 0.229952483008, 0.229952483008, 0.0801521319456
48, 2, 0.265234153844, 0.125961663225, 0.195597908535, 0.195597908535, 0.0696362453097
200, 2, 0.298458511621, 0.143756170165, 0.221107340893, 0.221107340893, 0.0773511707283
**** pop=100
0, 100, 0.328178005284, 0.173645174581, 0.21452193834, 0.225958888439, 0.0354591936435
50, 100, 0.258803421996, 0.159739990011, 0.175131262589, 0.176260967971, 0.0117608002761
107, 100, 0.207770227251, 0.146541079278, 0.173381643313, 0.17386918623, 0.0109426519273
141, 100, 0.193864849154, 0.151181402993, 0.172267220823, 0.172158496788, 0.0100510580599

**** BUG - fixed
cand and target were wrong way around in parallel_eval (polling from file)

*** ant colony optimization (requires components=
requires initialization of 'components' - not clear what this means for our problem

* TODO port msarm to M1 model and prepare final demo
** DONE add the input Proprioceptive population, which is actually really a set of netstims with location (NSLOCs), and connect them to layer 2 
** DONE reorganize definition of population/cell parameters
*** DONE set z location based on new yfrac property (0 to 1) for each population
*** DONE population and receptor names within cellpopdata module imported as p (ER2 -> p.ER2)
** DONE Add RL: weight changes at synapses, eligibility traces, stdp-like rule, keep track of target location and arm position (receive via udp every 10ms) to calculate error periodically,
*** DONE Make plexon input be optional 
*** DONE replace arminterface with arminterface_pipe.py - set an option so can use dummy virtual arm for testing!
*** DONE implement msarm.hoc (arm apparatus such as target location, arm position, error etc) in new python-based M1 model
*** DONE fix mpi bug - different motor commands for different nhosts (check if spikes diff as well!!!)
*** DONE RL and eligibility traces using George's PYNDL code
*** DONE plot weightchanges 
** DONE add interface to musculoskel arm (currently dummyArm) - 15 Feb 
*** DONE Assign SPI (spinal cord) subpopulations to different muscles (or maybe just random, which would make more realistic), convert from firing rates to muscle excitation (currently, just sum+threshold), and send udp packet with muscle excitations to arm every 10ms.
***  DONE Assign muscle lengths to the proprioceptive neurons and make them fire accordingly; requires updating muscle lengths every 10ms (via received udp messages)

*** DONE make DSC an izhi population and add connectivity to E5B
*** DONE check if can have plasticity between NSLOC and izhi -yes
** TODO replace pc.post() with pc.py_alltoall() or other ?? - maybe not required
** DONE Modularize using shared parameters+variables (shared.py)
** DONE Add the training and testing wrappers  -  17 Feb
** TODO Train and test to reach single target
*** DONE decide where plasticity will happen and restrict learning to that subset of pops - 19 Feb
-should modify the connectivity and weights tuned to M1!?  
-maybe avoid by adding plastic connections ONLY between
proprioception to L2/3? and SPI to spinal cord interneurons; use spinal cord to project to muscles - see 15jan11
- demonstrate that it is enought to learn a target
*** DONE Set izhikevich neuron type for spinal cord 
- pyramidal for now, maybe tune rate
*** DONE set yfrac appropriately
*** DONE Add proper exploratory movements
*** DONE Add antagonistic muscle inhibition

*** DONE Check why target not plot in test phase!!!
- targetid wasn't being setup
*** DONE Reproducibility - use same random seeds
*** DONE add inhibitory spinal cord pop (IDSC)
*** DONE make exploratory movements by increasing E5B noise
*** TODO setup evol alg - find params for 1k cells, 1 target, dummy arm 
** TODO Test that can learn multiple targets with artificial PMd input
** TODO Encode target using PMd activity as input - 25 Feb 
*** Find good PMd data where target can be decoded
*** Convert into vector of spikes (bill) that can be easily played back (trial/trial basis)
*** Optionally use SSM, and have plasticity between SSM neurons (can argue equivalent to PMd activity !) 
- PMd poisson from neural field model/SSM?)  
- use SSM from Marius, and convert to Poisson(?) a la cliff - Train with SSM
noise for each of the targets 
** TODO Use evol alg to find optimum set of params (first with dummyArm, the musculoskel arm) - 5 Mar
*** TODO Perform short comparison of different types of algos in inspyred:
- Krichmar custom - done
- genetic - done
- evolution strategy
- simulated annealing
- differential evolution
- Estimation of Distribution 
- particle swarm optimization
- ant colony optimization 

** TODO Test real-time with 10k cells, PMd input and virtual arm using BMI in HPC - 10 Mar
** TODO Prepare final demo video - 20 Mar
*** TODO Fix visualization of virtual arm in hpc!!
*** TODO Make something like this:
[[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150211_013249.png][fig]]
- NOTE: PMd input can be replaced with low-dim SSM representation (of PMd) and apply similar plasticity concept â€“ maybe
  can compare both methods -- if direct plasticity between bio+model neurons 
** TODO If have time (doubt it!) simulate perturbation and repair - 30 Mar
https://bbs.archlinux.org/viewtopic.php?pid=684936#p684936
