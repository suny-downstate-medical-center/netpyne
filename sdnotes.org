* 14jun18 Created repo
* 14sep19 First improvements by Giljael and me
** README by Gil
1.How to add new cell types in the model: <plx_cellpopdata.py> Insert cell info in lists like PMd case.  popnames = ['PMd',
'ER2', 'IF2', 'IL2', 'ER5', 'EB5', 'IF5', 'IL5', 'ER6', 'IF6', 'IL6'] popclasses = [-1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3] #
Izhikevich population type popEorI = [ 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1] # Whether it's excitatory or inhibitory popratios =
[numPMd,150, 25, 25, 167, 72, 40, 40, 192, 32, 32] Prior cells will have lower gids. E.g., PMd.gid < ER2.gid <...<IL6.gid

Change cellproperties() according to the cell types: cell info is returned to plx_model and the info is used for simulations.
For PMd type cells, the following is added, because the number of PMd cells are not changed as scale changes.  If cells
increase in the cell types as scale increases, the following modification is not needed.  indexPopName =
checkIndexPopName('PMd', popnames) # checkIndexPopName returns PMd index in popnames if not indexPopName == -1:
popnumbers[indexPopName] = numPMd # Number of PMds is fixed. # return back the number of PMd cells to numPMd.  "ncells" is a
global value, and total number of cells in the model.

According to the cell types added, return statement in names2inds() and its call in plx_cellpopdata.py and plx_model.py
should be updated.

def setconnprobs() needs connection probabilities for new cell types. Currently inserting PMds doesn't modify this function.

def setconnweights() needs new connection weight. For the PMds, connweights[PMd,ER2,AMPA]=10 was added.

<In plx_model.py> Update "## set cell types." @Line 189 For PMd, "elif cellclasses[c]==-1: celltypes.append(pmdnsloc)" is
added. pmdnsloc is defined in nsloc.py

Update "## set positions." @Line 202 Positions are changed. That is, if new cell types are added in the model, existing cells
in the model will have different positions because of randomness change.  xlocs = modelsize*rand(ncells) # Create random x
locations ylocs = modelsize*rand(ncells) # Create random y locations zlocs = verticalextent*rand(ncells) # Create random z
locations Add zlayer position update. For PMd, "elif cellnames[c][-1]=='d': zlocs[c]+=zlayerpositions[5]" is added. 'd' is in
'PMd'.

Update "## Actually create the cells." @Line 221 In this version(r2206 in SVN), all cells including new cells inserted are
distributed evenly in a round-robin way by "for c in xrange(int(pc.id()), ncells, nhosts):."  "cellsperhost" indicates how
many cells including new cell types inserted each worker created. Each worker might have different value of cellsperhost.
For PMd, the following code snippet is executed for NULL->NetCon->PMd connection. NSLOC-based cell types will follow the code
snippet, but inncl is only to feed PMds with external PMd spikes: if cellnames[gid] == 'PMd': cell = celltypes[gid](cellid =
gid) # create an NSLOC inncl.append(h.NetCon(None, cell)) # This netcon will receive external PMd spikes innclDic[gid] =
ninnclDic # This dictionary will be used for NetCon search.  ninnclDic += 1 else:

Update ##calculate distance and probabilities. @Line 256 Connection probabilities among cells are calculated prior to making
connections, However, PMds won't be post synaptic cells in the connections. So the following code snippet is only for PMds:
if cellnames[gid] == 'PMd': # There is no connection for cells -> PMds continue In order to make connections between the new
cells added and others based on probabilities, def setconnprobs() in plx_cellpopdata.py should be modified accordingly.
Connection between a ER2 and PMd is controlled explicitly by PMd[gid%numPMd]->ER2[gid]. So, if you want to control the
connections for other cells, follow the code for PMds:

pmdStart = cpd.popGidStart[PMd] # get pmd's start gid by using cpd.popGidStart[cellname] pmdEnd = cpd.popGidEnd[PMd] # get
PMd's end gid for c in xrange(pmdStart, pmdEnd + 1): allrands[c] = 1 # set all PMd values in allrands to 1.  if
cellnames[gid] == 'ER2': pMdId = (gid % numPMd) # select PMd being connected to this ER2 cell.  allconnprobs[pMdId] = 1 # to
make this PMd connected to the ER2 cell allrands[pMdId] = 0 # to make this PMd connected to the ER2 cell distances[pMdId] =
300 # to make the NetCon delay for this connection 5ms

Update ## Add background inputs @Line 447 ER2 and PMd cells won't be fired by background spikes. The following avoid them not
to be fired by background spikes: gid = gidvec[c] if isOriginal == 0: if cellnames[gid] == 'ER2' or cellnames[gid] ==
'PMd': # 'ER2' won't be fired by background stimulations.  continue

2.How to connect m1ms with Plexon?  # Connect m1ms with Plexon
- Copy m1ms/sim/Client to Windows machine having MATLAB and Plexon software.
- Open Client/plx_mat_interface.m on the Windows machine, and set up "remoteAddr" to the IP address m1ms runs on. In
  addition, set up "addapth" with the path for the library required for the Plexon software.
- Set up parameters in m1ms/sim/config.py accordingly.  isOriginal|isCommunication|isQueueTest a. 1 | x | x - To run the
  original m1ms (Cliff's parallelized model). X means don't care b. 0 | 1 | 1 - To run m1ms w/o connection to Plexon, but
  with PMd spike files c. 0 | 1 | 0 - To run m1ms, getting spikes from Plexon through the communication program Note: for b
  and c, check if PMd spike file (spikePMd-6sec.txv) is in data/.

3. How to run m1ms?  For 2.a, 2.b: $plx_runsim <# of workers>

For 2.c, 1. $plx_runsim <# of workers> 2. Run client in the Windows machine.  3. Run the Plexon softsever.

4. How to plot raster, lfp and power spectra?  Spikes are stored in m1ms/sim/m1ms-spk.txt and m1ms-spk.txt.mat Just run
python fileplots.py m1ms-spk.txt. It stores plots to files.  $python fileplots.py m1ms-spk.txt



** List of changes by Gil
- Added PMd population receiving external input
- Cells (inlcuidng PMd) distributed over workers using round-robin (each worker doesnt have same number of cells)
- Cells not referenced by realtive id, so easier to add and reference cells
- Master worker gets data from PMd cells and broadcasts it to other workers
- With PMd data, 30 workers over 10 nodes, and 10 scale (7846 cells), this model (6sec sim) runs in real-time (6sec).
- Added P population (proprioceptive from virtual arm) and udp interface to arm
** List of changes by me
- Tidied up code and merged with cliff's tutorial code
- Included generic stimulation code based on classes, eg. class for 'natural touch', class for 'optogenetic'

* 14sep22 RL in M1 model
** stdp.mod implementation
- adapted from george's cleanmodel by cliff
- includes STDP and RL as 2 diff mechanisms -- not dopamine-based STDP! - need to modify
- To implement RL in model need to run reward_punish from each element of stdpmechs (instantiations of stdp.mod = weight
adjuster) eg. every 100 ms: for s in stdpmechs: s.reward_punish
** RL interval?
- error used RL rewards should include difference etween current and previous time step (eg. 5ms) or previous RL update (50
  or 100 ms) ??
- in arm2dms it was errro with previous time step (10 ms) -- but I think it should be prev RL update or maybe Eligibility
  trace interval or motor command window !?
- No EM lag because aready included in the musculoskeletal arm
* 15jan11 Learning targets (reward signal)  :paper:
** reward-modulated STDP between biological neurons and model neurons
- different reward signal to different L2 subpopulations depending on target
- similar to Koca15
- PMd population = 128 neurons = biological neurons
- P population = proprioceptive = PPC / Thalamus (from virtual arm)
- To speed up training: 1) play back vector of PMd inputs; 2) use simple kinematic arm (once working replace with
  musculoskeletal and retrain)
- Start with just 2 targets (left, right); if working move to 4 targets
*** Training: different options from more realistic to more practical
- Feed PMd data (for different targets) and for each one enforce exploratory movements over all targets
- STDP + RL when hand getting closer to correct target
- Plasticity only between PMd->L2; L5/CSP -> Spinal Cord; P->L2 ??
- What connections will be reinforced: those linking PMd data corresponding to target X, with the arm movements to target X
- Need to divide training and testing dataset?
- In L2/3, the accuracy of neuronal ensemble prediction of lever trajectory remained unchanged globally, with a subset of
  individual neurons retaining high prediction accuracy throughout the training period. However, in L5a, the ensemble
  prediction accuracy steadily improved, and one-third of neurons, including subcortical projection neurons, evolved to
  contribute substantially to ensemble prediction in the late stage of learning. The L2/3 network may represent coordination
  of signals from other areas throughout learning, whereas L5a may participate in the evolving network representing
  well-learned movements.(Masamizu et al, 2014)

*** Connectivity: different options from more realistic/autonomous, to more hard-wired/easy to learn
**** PMd -> L2 (all-to-all) overlapping with P -> L2 (all-to-all)
**** PMd -> L2 (all-to-50%), P -> L2 (all-to-other 50%)

* 15jan28 Focus on this model now (2 months to final demo) first steps: mpi, circuitry, conceptual framework
** Debug msarm.py so can run model
- modified msarm to use self. for most variables in run method
- 'randomOutput' arm runs fast, but dummyArm quite slow because has to search/collect spikes to generate motor command
** Test mpi in mac
*** Salvador-Duras-MacBook-Pro% mpiexec -n 4 nrniv -python -mpi model.py
ssh: Could not resolve hostname Salvador-Duras-MacBook-Pro: nodename nor servname provided, or not known
^C[mpiexec@Salvador-Duras-MacBook-Pro] Sending Ctrl-C to processes as requested [mpiexec@Salvador-Duras-MacBook-Pro] Press
Ctrl-C again to force abort [mpiexec@Salvador-Duras-MacBook-Pro] HYDU_sock_write (./utils/sock/sock.c:291): write error (Bad
file descriptor) [mpiexec@Salvador-Duras-MacBook-Pro] HYD_pmcd_pmiserv_send_signal (./pm/pmiserv/pmiserv_cb.c:170): unable to
write data to proxy [mpiexec@Salvador-Duras-MacBook-Pro] ui_cmd_cb (./pm/pmiserv/pmiserv_pmci.c:79): unable to send signal
downstream [mpiexec@Salvador-Duras-MacBook-Pro] HYDT_dmxu_poll_wait_for_event (./tools/demux/demux_poll.c:77): callback
returned error status [mpiexec@Salvador-Duras-MacBook-Pro] HYD_pmci_wait_for_completion (./pm/pmiserv/pmiserv_pmci.c:197):
error waiting for event [mpiexec@Salvador-Duras-MacBook-Pro] main (./ui/mpich/mpiexec.c:331): process manager error waiting
for completion
*** Salvador-Duras-MacBook-Pro% mpiexec
[mpiexec@Salvador-Duras-MacBook-Pro] set_default_values (./ui/mpich/utils.c:1542): no executable provided
[mpiexec@Salvador-Duras-MacBook-Pro] HYD_uii_mpx_get_parameters (./ui/mpich/utils.c:1751): setting default values failed
[mpiexec@Salvador-Duras-MacBook-Pro] main (./ui/mpich/mpiexec.c:153): error parsing parameters Salvador-Duras-MacBook-Pro%
which mpiexec /usr/local/bin/mpich3/bin/mpiexec

*** Salvador-Duras-MacBook-Pro% brew install mpich - works
==> Installing dependencies for mpich2: cloog, gfortran Error: You must `brew link isl' before cloog can be installed
Salvador-Duras-MacBook-Pro% brew link isl Linking /usr/local/Cellar/isl/0.12.1... 9 symlinks created
Salvador-Duras-MacBook-Pro% brew install mpich ==> Installing dependencies for mpich2: cloog, gfortran ==> Installing mpich2
dependency: cloog ==> Downloading https://downloads.sf.net/project/machomebrew/Bottles/cloog-0.18.1.mavericks.bottle.1.tar.gz
######################################################################## 100.0% ==> Pouring
cloog-0.18.1.mavericks.bottle.1.tar.gz ðŸº /usr/local/Cellar/cloog/0.18.1: 33 files, 556K ==> Installing mpich2 dependency:
gfortran ==> Downloading https://downloads.sf.net/project/machomebrew/Bottles/gfortran-4.8.2.mavericks.bottle.1.tar.gz
######################################################################## 100.0% ==> Pouring
gfortran-4.8.2.mavericks.bottle.1.tar.gz ==> Caveats Formulae that require a Fortran compiler should use: depends_on :fortran
==> Summary ðŸº /usr/local/Cellar/gfortran/4.8.2: 960 files, 113M ==> Installing mpich2 ==> Using Homebrew-provided fortran
compiler.  This may be changed by setting the FC environment variable.  ==> Downloading
http://www.mpich.org/static/downloads/3.1/mpich-3.1.tar.gz
######################################################################## 100.0% ==> ./configure --disable-silent-rules
--prefix=/usr/local/Cellar/mpich2/3.1 --mandir=/usr/local/Cellar/mpich2/3.1/share/man ==> make ==> make install ðŸº
/usr/local/Cellar/mpich2/3.1: 457 files, 15M, built in 3.3 minutes Salvador-Duras-MacBook-Pro%
*** Warning: detected user attempt to enable MPI, but MPI support was disabled at build time.

** Install NEURON in mac
http://www.neuron.yale.edu/neuron/download/compilestd_osx

brew install mpich build.sh config: ./configure --with-iv=$IVB --prefix=$ND --with-nrnpython=dynamic CC='gcc-4.6'
CCX='g++-4.6' --with-paranrn https://discussions.apple.com/thread/3406578 make make install python setup.py install
--home=/usr/arch/nrn/share/python

*** clean steps from scratch
- install required libraries via brew (list of libs?)
- brew install open-mpi
- cd $NSRC; ./build.sh
-./configure --with-iv=$IVB/iv --prefix=$ND --with-nrnpython=dynamic CC='gcc-4.6' CCX='g++-4.6' --with-paranrn=dynamic
- sudo bash
- export ARCHFLAGS='-arch i386 -arch x86_64'
- cd $ND
- make
- make install
- cd $NB/src/nrnpython
- python setup.py install --home=/usr/arch/nrn/share/python

**** didnt work (see error here)
Salvador-Duras-MacBook-Pro% m1ms Salvador-Duras-MacBook-Pro% /usr/local/Cellar/open-mpi/1.7.4/bin/mpirun -n 4 nrniv -python
-mpi model.py dyld: Library not loaded: /usr/local/lib/libpmpich.12.dylib Referenced from:
/usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not found dyld: Library not loaded: /usr/local/lib/libpmpich.12.dylib
Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not found dyld: Library not loaded:
/usr/local/lib/libpmpich.12.dylib Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not found dyld:
Library not loaded: /usr/local/lib/libpmpich.12.dylib Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason:
image not found -------------------------------------------------------------------------- mpirun noticed that process rank 2
with PID 24333 on node Salvador-Duras-MacBook-Pro exited on signal 5 (Trace/BPT trap: 5).
-------------------------------------------------------------------------- Salvador-Duras-MacBook-Pro% nrniv dyld: Library
not loaded: /usr/local/lib/libpmpich.12.dylib Referenced from: /usr/arch/nrn/x86_64/lib/libnrnoc.0.dylib Reason: image not
found Trace/BPT trap

**** try specifying open-mpi folder - worked!
 ./configure --with-iv=$IVB/iv --prefix=$ND --with-nrnpython=dynamic CC='gcc-4.6' CCX='g++-4.6'
 --with-paranrn=/usr/local/Cellar/open-mpi/1.7.4/

** Define inputs to M1 circuitry :paper:
*** what layer do proprioceptive inputs (via spinal cord+thalamus) target?
- thalamic inputs to upper layers (Weiler et al,2008; Kiritani et al, 2010)
- Thalamocortical inputs from anterior, motor-related thalamic regions (VA/VL) with cerebellar afferents -> L2/3, L5A, L5B (IT+PT)
  (Hooks et al, 2013)
- Posterior sensory-related thalamic areas (POm) -> L2/3 and L5A (Hooks et al, 2013)
- Inputs from sensory-related cortical and thalamic areas preferentially target the upper-layer pyramidal neurons in
  vM1.(Hooks et al, 2013)
- VL axons in the cortex excited both IT and PT neurons (Yamawaki & Shepherd, 2015)
- Area 2 receives its main input from area 1 as well as from the VPS, which is the main relay nucleus for proprioceptive
  information in the monkey. (Francis 2009); Until recently, the rat homolog of the VPS had not been identified, which is
  surprising given the wide spread use of the rat as an animal model. (Francis 2009) --> In macaque proprioceptive info via
  VPS
- Mapped out a region in the rostral VPL of the rat that responds preferentially to joint manipulation and muscle palpation
  (Francis et al. 2008) --> In rat proprioceptive info via VPL

*** what layer do PMd inputs target?
- cortical inputs to upper layers (Weiler et al,2008; Kiritani et al, 2010)
- Orbital cortex (OC) -> L6
- Secondary motor cortex (M2) -> L5B
- Inputs from OC and M2, areas associated with volitional and cognitive aspects of movements, bypass local circuitry and have
  direct monosynaptic access to neurons projecting to brainstem and thalamus.
- In macaque input from PMd seems to go primarily to upper layers (layer 1?) and spread across the rest (based on fig 2 Shipp, 2005) 
- In macaque PMd->M1 target ~55% deep layers and ~45% superficial layers (1-3) (Dum & Strick, 2005)
- In rhesus monkey more than 90% of all labeled neurons within the premotor and motor cortices were found in layer 3; the rest in
  layers 5 and 6 (Barbas & Panday, 1987)

- "In L2/3, the accuracy of neuronal ensemble prediction of lever trajectory remained unchanged globally, with a subset of
  individual neurons retaining high prediction accuracy throughout the training period. However, in L5a, the ensemble
  prediction accuracy steadily improved, and one-third of neurons, including subcortical projection neurons, evolved to
  contribute substantially to ensemble prediction in the late stage of learning. The L2/3 network may represent coordination
  of signals from other areas throughout learning, whereas L5a may participate in the evolving network representing
  well-learned movements" (Masamizu et al, 2014)

** Cell and neuron densities in the primary motor cortex of primates (Young,2013)
- 50,000 neurons / mm2
** Added new spinal cord populations
*** code
-popnames = ['PMd', 'ASC', 'DSC', 'ER2', 'IF2', 'IL2', 'ER5', 'EB5', 'IF5', 'IL5', 'ER6', 'IF6', 'IL6'] popclasses = [-1, -1,
--1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3] # Izhikevich population type popEorI = [ 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1] # Whether
-it's excitatory or inhibitory popratios = [numPMd, 48, 48, 150, 25, 25, 167, 72, 40, 40, 192, 32, 32] # Cell population
-numbers
*** description
- PMd = input from PMd with target/reward information; NSLOCs; reproduces PMd recorded data
- ASC = Ascending Spinal Cord; proprioceptive info; encode x-y speed (previously P population encoding muscle lengths); 
- DSC = Descending Spinal Cord; muscle excitations; receives input from EB5 (all-to-all mapping)
  
** Proprioceptive population encodes cartesian direction and velocity :papers:
*** From Roll04
It is widely recognized nowadays that sensory information produced by muscle spindles constitutes a crucial part of
proprioception (Cordo 1990; Gandevia 1996; Gandevia and Burke 1992; Roll 2003). As far as the sensory level is concerned, one
might mention recent studies examining the coding of two-dimensional pointing and drawing movements, the results of which
have shown that muscle spindle population activity is strongly correlated with both the direction and the velocity of the
ongoing movement under both passive and active conditions (Bergenheim et al. 2000; Roll et al. 2000; Jones et al. 2001). In
addition, these studies showed that each muscle spindle is sensitive to a specific range of movement directions (the
so-called preferred sensory sector, PSS), and shows maximum sensitivity to a specific direction (denoted the preferred
sensory direction, PSD). The PSS and the PSD of the various muscle spindles within a given muscle are quite similar, which
makes it possible to calculate an average PSD and PSS for that muscle. Each muscle has its own PSD and PSS which differs from
those of other muscles.  When examining the PSS of the muscle groups acting on the ankle joint, it was observed that they
overlapped in such a way that, together, they covered the whole range of possible movement directions in that particular
joint (Bergenheim et al. 2000; Roll et al. 2000). The authors of the latter studies concluded that the proprioceptive
information arising from all muscles surrounding a joint was needed for accurate sensory and perceptual coding to be
performed throughout the whole movement.  In other studies, using a similar population vector model to that used by Schwartz
(1992, 1993) at the cortical level, it was established that the â€œsum vectorâ€ of all the oriented and weighted activity from
the whole population of muscle spindles in all the muscles acting on a given joint, accurately describes the instantaneous
direction and velocity of the ongoing movement in two dimensional space (Bergenheim et al. 2000; Roll et al. 2000; Jones et
al. 2001; Ribot-Ciscar et al. 2002).

*** From Bosc01 cited in Fran09
XV. SUMMARY

A.â€‚ Role of Limb Biomechanics in Global Limb Representations

We have presentedhere a possible framework for interpreting proprioceptive signals at the spinal level. It is based on the
premise that global limb information rather than localized receptor-like proprioceptive information is encoded by the nervous
system. Within a basic global framework, information is encoded by a distributed system in which each neural element may
still bias the global information according to some local detail. For example, the DSCT data suggest that details about
stiffness at a single joint might be contained in a population signal that encodes a representation of the limb end point
that may then depend on the joint covariance resulting from specific levels of joint stiffness.

This global sensory representation is not organized entirely by the neural circuitry however. It begins in the periphery with
the biomechanical structure of the limb. Biomechanical constraints ensure that theactivity from individual sensory receptors
will be correlated in certain ways that depend on whole limb parameters. Therefore, even a minimum of central sensory
convergence could lead to global representations with this peripheral apparatus (37,38, 281).

B.â€‚ Significance of Kinematic-Based Representations

We also suggest that this framework could be based on limb kinematics. If so, it is noteworthy that while many participating
sensory receptors are associated with muscles and some even specifically tuned to muscle force, nevertheless their ensemble
is capable of encoding limb kinematics. In other words, inputs from receptors located in individual muscles or associated
deep structures as well as in the skin are assembled at very early stages of central processing to provide a representation
of limb kinematics. Because this occurs at these earliest stages, it suggests that the peripheral apparatus may also in some
way play a role in itsdetermination. The result, however, is that centrally directed sensory information may be encoded in a
framework common to that of central motor activity that relates to limb kinematics. It may therefore be analogous to the
situation in the superior colliculus where sensory information from various modalities is mapped congruently within a
retinotopic map (227, 306) that may be modified or transformed by gaze (123, 124,157, 158); that is, the sensory information
is combined and integrated through a common coding framework. Although the retinal projection may provide the basis for a
common framework for eye, head, and body movement control, limb biomechanics and associated proprioceptors appear to provide
the basis for a common framework for limb movement control.
*** From Berg00
The results show that each muscle spindle afferent, and likewise each muscle, has a specific preferred sensory direction, as
well as a preferred sensory sector within which it is capable of sending sensory information to the central nervous
system. Interestingly, the results also demonstrate that the preferred directions are the same as the directions of
vibration-induced illusions. In addition, the results show that the neuronal population vector model describes the
multipopulation proprioceptive coding of spatially oriented 2D limb movements, even at the peripheral sensory level, based on
the sum vectors calculated from all the muscles involved in the movement.
 
** PMd input with multiple targets requires new conceptual framework! :papers:
- PMd provides preparation activity -- where to go; target info with respect to hand
- Learning adpats weight to map different PMd activity to M1 activity that directs arm to different targets
- ASC population encodes proprioceptive info from arm (direction and velocity) and visual feedback from eyes (arm position;
  or arm - target position) -- doesn't make sense because ASC (=spinal cord) doesn't contain vision; would have to call it
PPC -- if only encode direction (population code for angle) and velocity (amplitude as firing rate), system shouldn't be able
to tell difference if placed in different starting point; however this may not be required because input from PMd is guiding
movement (ie. telling system, go to the right/left etc., not specifying target location!), so can argue hand/target position
more relevant for PMd (?!)  -- when pmd activity dies off (reached target), so should M1 activity?


- "We demonstrate that an MI ensemble can reconstruct hand or joint trajectory more accurately than an equally sized PMd
  ensemble.In contrast, PMd can more precisely predict the future occurrence of one of several discrete targets to be
  reached.These results also support the hierarchical view that MI ensembles are involved in lower-level movement execution,
  whereas PMd populations represent the early intention to move to visually presented targets." Hatsopoulos, 2004

- Above also possible argument for developing model of M1 -- PMd activity itself doesn't provide accurate position/vel representation

* 15feb06 dummyArm
- dummyArm was independent python executable which used udp messages to communicate with model
- udp obsolete (now pipes)
- replaced dummyArm with same code but running within model (no udp or pipes)
- requires all same RL apparatus; after that working test muscskel
* 15feb09 MPI issues: gidDic, motor commands, and RL
** gidVec vs gidDic
- CHECK gidVec vs gidDic
- gidVec is vector local to each node, where index=local id, and value = gid
- gidDic is dictionary local to each node, where key=global id, value = local id 
- redundant! but using gidVec.index() to get the local id is very slow (~300x slower) -- so use gidDic to get local id
*** speed comparison from gil
gidvec.index() takes so long time.  Test for vec.index and dictionary import time import random vec = [] for c in
range(10000): vec.append(c) dic = { x:x for x in range(10000)} seq = range(10000) random.shuffle(seq) measure = time.time()
for c in seq: a = vec.index(c) measure = time.time() - measure print 'vec.index:', measure measure = time.time() for c in
seq: b = dic[c] measure = time.time() - measure print 'dic:', measure

vec.index: 1.80838108063 dic: 0.00332403182983

*** speed comparison in m1 model
10k cells, 1sec sim, 16 cores, with gidVec.index() = 153 sec 10k cells, 1sec sim, 16 cores, with gidDic = 25.9 sec = x5.9
speedup
** differences when using mpi with different number of nodes due to motor commands implementation
- not present when arm is off
- not present when proprio is off -> due to proprio -- not true, also present with propio off 
- joint angles different in 1 vs 16 cores
- not due to broadcasting error - same before and after
- difference in motor commands! - maybe due to timing?
- FOUND: error originates in these 2 lines used to speed up sim by removing past spikes from list:

#self.hostspiketimes = self.hostspiketimes[self.hostspiketimes > (h.t - 2*max(self.shtimewin,self.eltimewin))] # remove
#unncessary old spikes self.hostspikecells = self.hostspikecells[self.hostspiketimes > (h.t - 
2*max(self.shtimewin,self.eltimewin))] # remove unncessary old spikes

JUST HAD TO SWITCH AROUND TO AVOID SPIKETIMES GETTING DELETED BEFORE USING IT TO SEARCH SPIKECELLS!!

- still diffs when add RL

** differences when using mpi with different number of nodes due to RL (STDP BUG AND FIX)
- different num of spikes when using 1 vs >1 cores
- was only changing weights in worker0
- seems error related to STDP  -- also happens when RL off
- doesn't happen when STDP off
- also happens in cliff tutorial code
- can reproduce easily with this code: http://neuron.yale.edu/neuron/static/courses/cns2014/large-scale.zip , scale=4,
 duration=3, 1 vs 8 mpi cores 
- also tested in neurosim (zn) /u/salvadord/Documents/ISB/Models/large-scale/
- also happens when using scale=3, dur=3, 4 vs 12 cores (Spikes: 14031 vs 14003)
- compared output: difference in weightchanges (38/83311), spikes (28/14031) and lfp; rest the same:
        distances: [164138x1 double]
             EorI: [3000x1 int64]
          lfptime: [600x1 double]
            ylocs: [3000x1 double]
         cellpops: [3000x1 int64]
         stimdata: []
           delays: [164138x1 double]
      cellclasses: [3000x1 int64]
        cellnames: [3000x3 char]
      connweights: [15x15x4 double]
        connprobs: [15x15 double]
      connections: [164138x2 double]
            xlocs: [3000x1 double]
          weights: [164138x4 double]
          simcode: {7x1 cell}
    weightchanges: {83311x1 cell}
         stdpdata: [83311x3 double]
        spikedata: [14031x2 double]
            zlocs: [3000x1 double]
             lfps: [600x6 double]
*** chat with cliff
cliff I find differences in the number of spikes when using 1 core vs >1 core did u have this problem?  I tested the sim you
used for tutorial and also reproduced it there eg. 1 core = 19771 spikes; 8 cores = 19758 spikes Cliff Kerr hmm that's
strange -- i used to have that problem but it got fixed at some point, could've gotten broken again though...  it had to do
with the random seeds being initialized differently Salvador Dura so how did u fix? maybe I ahve old version hmm ok, I'll
check that I was thinking it was stdp related cause doesn't seem to happen when stdp off -- but need to check more thoroughly
was thinking maybe related to stdp happening between cells in different cores, but just speculation Cliff Kerr each cell
should have its own random number generator linked to gid but it's possible i haven't checked for stdp Salvador Dura so the
random generator error u had, was related to using different number of cores?  Cliff Kerr yeah Salvador Dura cause if I use
the same number of cores, the result is always the same ok it seems when stdp off, error doesnt happen Cliff Kerr
interesting...  Salvador Dura I'll check the rand gen and the stdp code, see if I can find anything Cliff Kerr anyway my
feeling is that it's probably not a big deal, i.e. each one is equally valid, but yeah agree they should match Salvador Dura
@equally valid - yeah probably, but just need for reproducibility of results -- small error could carry forward in time I
guess
sal:can reproduce easily with this code: http://neuron.yale.edu/neuron/static/courses/cns2014/large-scale.zip , scale=4, duration=3, 1 vs 8 mpi cores
cliff: i guess you could check that the stdp connections and weights are the same in the 1 and 8 core cases?

*** more systematic tests
**** scale=2, dur=2, 1 vs 2 cores
- differences in weightchanges:
conn    pre    post     weightchanges (1 core)                          weightchanges (2 cores) gid (1core) gid (2cores)
645	602	35	[0,2.63385014695182;1005,2.61453901661609]	[0,2.63385014695182]	[0,0]	[1,0]
646	672	35	[0,0.486465429624879;1005,0.467154299289152]	[0,0.486465429624879]	[0,0]	[1,0]
3326	532	178	[0,2.65632182511313;1005,2.64518027212770]	[0,2.65632182511313]	[0,0]	[1,0]
3540	639	188	[0,1.82277902626864;1005,1.81531061995854]	[0,1.82277902626864]	[0,0]	[1,0]

- weight decreases due to antiHebb learning
- spike times:
cell    spk time
602	540

35	514
35	540

672	540
672	1278.50000000000
672	1689.50000000000

- occurs when spk time same in pre and post
- fixed (do differences now) by changing stdp.mod :
if  ((tlastpost > -1) && (interval != 0))  -->  if  ((tlastpost > -1) && (interval > 0.0))
if  ((tlastpre > -1) && (interval != 0.0))  -->  if  ((tlastpre > -1) && (interval > 0.0)) 

**** tested using scale=4, dur=4, 1 vs 16 cores --> >10k weightchange diffs and 82 more spikes

**** test scale scale=2, dur=2, 1 vs 16 cores --> 22 wc diffs, 1 spike more:
conn    pre    post     weightchanges (1 core)                          weightchanges (2 cores) gid (1core) gid (2cores)
349	134	11	[0,2.86023987946172]	[0,2.86023987946172;1005,2.90268947422707]	[0,0]	[1,0]
654	358	22	[0,0.149294195666084;1005,0.305054352280365]	[0,0.149294195666084]	[0,0]	[2,0]
3246	536	106	[0,2.73408129675365]	[0,2.73408129675365;1005,2.73408129675365]	[0,0]	[4,0]
3278	1269	107	[0,1.74122404294975]	[0,1.74122404294975;1005,1.74122404359625]	[0,0]	[10,0]
6487	362	218	[0,1.34847151295259]	[0,1.34847151295259;1005,1.37991894621531]	[0,0]	[2,1]
7360	499	249	[0,1.79714956561962]	[0,1.79714956561962;1005,1.85738840800206]	[0,0]	[3,1]
8881	1203	300	[0,1.68536212703375]	[0,1.68536212703375;1005,1.74266308640579]	[0,0]	[9,2]
17022	9	578	[0,1.61835650498962]	[0,1.61835650498962;1005,1.62882444617930]	[0,0]	[0,4]
17023	22	578	[0,2.35002265490450]	[0,2.35002265490450;1005,2.36728137220437]	[0,0]	[0,4]
17024	29	578	[0,1.70413279263212]	[0,1.70413279263212;1005,1.70487236537541]	[0,0]	[0,4]
17025	121	578	[0,1.33952623608263]	[0,1.33952623608263;1005,1.34080810277188]	[0,0]	[0,4]
17028	202	578	[0,1.07834345749672]	[0,1.07834345749672;1005,1.07920271843487]	[0,0]	[1,4]
17029	217	578	[0,0.629349148726955]	[0,0.629349148726955;1005,0.648422981170065]	[0,0]	[1,4]
17032	325	578	[0,1.41251536458843]	[0,1.41251536458843;1005,1.41367524553380]	[0,0]	[2,4]
17033	363	578	[0,0.665475706621114]	[0,0.665475706621114;1005,0.666293060908807]	[0,0]	[2,4]
17034	397	578	[0,2.45872497666071]	[0,2.45872497666071;1005,2.47285521927279]	[0,0]	[3,4]
17036	434	578	[0,1.07763999597961]	[0,1.07763999597961;1005,1.07834349933459]	[0,0]	[3,4]
17037	493	578	[0,0.145238375107857]	[0,0.145238375107857;1005,0.146015866556552]	[0,0]	[3,4]
17038	525	578	[0,1.42405633633909]	[0,1.42405633633909;1005,1.42491559727724]	[0,0]	[4,4]
17043	849	578	[0,20.7127766032884]	[0,20.7127766032884;1005,20.7249386158134]	[0,0]	[6,4]
23626	896	827	[0,2.36270498436227]	[0,2.36270498436227;1005,2.36270498436295]	[0,0]	[7,6]
30949	1285	1221	[0,0.0225237576864728]	[0,0.0225237576864728;1005,0.0225237576892504]	[0,0]	[10,9]

- spike times:
---------------- (same t)
134	709
134	724.500000000000 *
134	1215
134	1507.50000000000
134	1522
134	1936.50000000000

11	704
11	724.500000000000 *
11	1220
11	1502
11	1522.50000000000
11	1932

-------------- (different t, but diff spikes)
358	761.500000000000 *
358	1264.50000000000
358	1559

(2 cores)
22	728
22	764 *
22	1267
22	1524
22	1548

(1 core)
22	728
22	756.500000000000
22	1267
22	1524
22	1548

---------------- (same t)
536	553.500000000000 *
536	570.500000000000
536	902 *
536	1198.50000000000

106	546.500000000000 *
106	902 *
106	1202

---------------- (same t)
362	728 *
362	1242
362	1505
362	1526
362	1936

218	728 *
218	1139
218	1216
218	1524.50000000000

*** tried modifying stdp.mod to increase interval
if  ((tlastpre > -1) && (interval > 0.01)) - 22 diffs
if  ((tlastpre > -1) && (interval > 0.1)) - 22 diffs 
if  ((tlastpre > -1) && (interval > 1)) - 26 diffs
if  ((tlastpre > -1) && (interval > 2.0)) - 26 diffs
if  ((tlastpre > -1) && (interval > 2.0)) - 13 diffs

*** found problem!
17.47 found problem: when pre is in different node and happens same time as post, the pre net_receive event sometimes arrives after post, so pre time is still previous spike time of that cell and thus stdp happens this is quite common since all spk times interval of 0.5 ms !

*** fixed problem!
20.44 found fix for stdp bug: instead of updating w directly use net_send() to update 1ms later and check for simultaneous
spike; tested with 10-sec 10k cell sim for 1 vs 10 cores and both identical; code here:
/u/salvadord/Documents/ISB/Models/large-scale/stdp.mod  

*** Code with fix for bug
COMMENT

STDP + RL weight adjuster mechanism

Original STDP code adapted from:
http://senselab.med.yale.edu/modeldb/showmodel.asp?model=64261&file=\bfstdp\stdwa_songabbott.mod

Adapted to implement a "nearest-neighbor spike-interaction" model (see 
Scholarpedia article on STDP) that just looks at the last-seen pre- and 
post-synaptic spikes, and implementing a reinforcement learning algorithm based
on (Chadderdon et al., 2012):
http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0047251

Modified by salvadord to avoid bug when simultaneous pre and post spikes occur in different nodes (with mpi)

Example Python usage:

from neuron import h

## Create cells
dummy = h.Section() # Create a dummy section to put the point processes in
ncells = 2
cells = []
for c in range(ncells): cells.append(h.IntFire4(0,sec=dummy)) # Create the cells

## Create synapses
threshold = 10 # Set voltage threshold
delay = 1 # Set connection delay
singlesyn = h.NetCon(cells[0],cells[1], threshold, delay, 0.5) # Create a connection between the cells
stdpmech = h.STDP(0,sec=dummy) # Create the STDP mechanism
presyn = h.NetCon(cells[0],stdpmech, threshold, delay, 1) # Feed presynaptic spikes to the STDP mechanism -- must have weight >0
pstsyn = h.NetCon(cells[1],stdpmech, threshold, delay, -1) # Feed postsynaptic spikes to the STDP mechanism -- must have weight <0
h.setpointer(singlesyn._ref_weight[0],'synweight',stdpmech) # Point the STDP mechanism to the connection weight

Version: 2013oct24 by cliffk

ENDCOMMENT

NEURON {
    POINT_PROCESS STDP : Definition of mechanism
    POINTER synweight : Pointer to the weight (in a NetCon object) to be adjusted.
    RANGE tauhebb, tauanti : LTP/LTD decay time constants (in ms) for the Hebbian (pre-before-post-synaptic spikes), and anti-Hebbian (post-before-pre-synaptic) cases. 
    RANGE potrate, deprate : Maximal adjustment (can be positive or negative) for Hebbian and anti-Hebbian cases (i.e., as inter-spike interval approaches zero).  This should be set positive for LTP and negative for LTD.
    RANGE RLwindhebb, RLwindanti : Maximum interval between pre- and post-synaptic events for an starting an eligibility trace.  There are separate ones for the Hebbian and anti-Hebbian events.
    RANGE useRLexp : Use exponentially decaying eligibility traces?  If 0, then the eligibility traces are binary, turning on at the beginning and completely off after time has passed corresponding to RLlen.
    RANGE RLlenhebb, RLlenanti : Length of the eligibility Hebbian and anti-Hebbian eligibility traces, or the decay time constants if the traces are decaying exponentials.
    RANGE RLpotrate, RLdeprate : Maximum gains to be applied to the reward or punishing signal by Hebbian and anti-Hebbian eligibility traces.  
    RANGE wmax : The maximum weight for the synapse.
    RANGE softthresh : Flag turning on "soft thresholding" for the maximal adjustment parameters.
    RANGE STDPon : Flag for turning STDP adjustment on / off.
    RANGE RLon : Flag for turning RL adjustment on / off.
    RANGE verbose : Flag for turning off prints of weight update events for debugging.
    RANGE tlastpre, tlastpost : Remembered times for last pre- and post-synaptic spikes.
    RANGE tlasthebbelig, tlastantielig : Remembered times for Hebbian anti-Hebbian eligibility traces.
    RANGE interval : Interval between current time t and previous spike.
    RANGE deltaw : The calculated weight change.
    RANGE newweight : New calculated weight.
}

ASSIGNED {
    synweight        
    tlastpre   (ms)    
    tlastpost  (ms)   
    tlasthebbelig   (ms)    
    tlastantielig  (ms)        
    interval    (ms)    
    deltaw
    newweight          
}

INITIAL {
    tlastpre = -1            : no spike yet
    tlastpost = -1           : no spike yet
    tlasthebbelig = -1      : no eligibility yet
    tlastantielig = -1  : no eligibility yet   
    interval = 0
    deltaw = 0
    newweight = 0
}

PARAMETER {
    tauhebb  = 10  (ms)   
    tauanti  = 10  (ms)    
    potrate = 1.0
    deprate = -1.0
    RLwindhebb = 10 (ms)
    RLwindanti = 10 (ms)
    useRLexp = 0   : default to using binary eligibility traces
    RLlenhebb = 100 (ms)
    RLlenanti = 100 (ms)
    RLpotrate = 1.0
    RLdeprate = -1.0
    wmax  = 15.0
    softthresh = 0
    STDPon = 1
    RLon = 1
    verbose = 0
}

NET_RECEIVE (w) {
     deltaw = 0.0 : Default the weight change to 0.

    : Hebbian weight update happens 1ms later to check for simultaneous spikes (otherwise bug when using mpi)
    if ((flag == -1) && (tlastpre != t-1)) {   
        w = 0
        deltaw = potrate * exp(-interval / tauhebb)   : Use the Hebbian decay to set the Hebbian weight adjustment. 
        if (softthresh == 1) { deltaw = softthreshold(deltaw) } : If we have soft-thresholding on, apply it.
        if (verbose > 0) { printf("Hebbian STDP event: t = %f ms; tlastpre = %f ms; interval = %f; deltaw = %f\n",t,tlastpre,interval,deltaw) } : Show weight update information if debugging on.
    }

    : Ant-hebbian weight update happens 1ms later to check for simultaneous spikes (otherwise bug when using mpi)
    else if ((flag == 1) && (tlastpost != t-1)) { :update weight 1ms later to check for simultaneous spikes (otherwise bug when using mpi)
        w = 0
        deltaw = deprate * exp(interval / tauanti) : Use the anti-Hebbian decay to set the anti-Hebbian weight adjustment.
        if (softthresh == 1) { deltaw = softthreshold(deltaw) } : If we have soft-thresholding on, apply it.
        if (verbose > 0) { printf("anti-Hebbian STDP event: t = %f ms; deltaw = %f\n",t,deltaw) } : Show weight update information if debugging on. 
    }
     
    : If we receive a non-negative weight value, we are receiving a pre-synaptic spike (and thus need to check for an anti-Hebbian event, since the post-synaptic weight must be earlier).
    if (w > 0) {           
        interval = tlastpost - t  : Get the interval; interval is negative
        if  ((tlastpost > -1) && (interval > 0.0)) { : If we had a post-synaptic spike and a non-zero interval...
            if (STDPon == 1) { : If STDP learning is turned on...
                net_send(1,1) : instead of updating weight directly, use net_send to check if simultaneous spike occurred (otherwise bug when using mpi)    
            }
            if ((RLon == 1) && (-interval <= RLwindanti)) { tlastantielig = t } : If RL and anti-Hebbian eligibility traces are turned on, and the interval falls within the maximum window for eligibility, remember the eligibilty trace start at the current time.
        }
        tlastpre = t : Remember the current spike time for next NET_RECEIVE.  
    
    : Else, if we receive a negative weight value, we are receiving a post-synaptic spike (and thus need to check for an anti-Hebbian event, since the post-synaptic weight must be earlier).    
    } else if (w < 0) {            
        interval = t - tlastpre : Get the interval; interval is positive
        if  ((tlastpre > -1) && (interval > 1.0)) { : If we had a pre-synaptic spike and a non-zero interval...
            if (STDPon == 1) { : If STDP learning is turned on...
                net_send(1,-1) : instead of updating weight directly, use net_send to check if simultaneous spike occurred (otherwise bug when using mpi)
            }
            if ((RLon == 1) && (interval <= RLwindhebb)) { tlasthebbelig = t } : If RL and Hebbian eligibility traces are turned on, and the interval falls within the maximum window for eligibility, remember the eligibilty trace start at the current time.
        }
        tlastpost = t : Remember the current spike time for next NET_RECEIVE.
    }
    adjustweight(deltaw) : Adjust the weight.
}

PROCEDURE reward_punish(reinf) {
    if (RLon == 1) { : If RL is turned on...
        deltaw = 0.0 : Start the weight change as being 0.
        deltaw = deltaw + reinf * hebbRL() : If we have the Hebbian eligibility traces on, add their effect in.   
        deltaw = deltaw + reinf * antiRL() : If we have the anti-Hebbian eligibility traces on, add their effect in.
        if (softthresh == 1) { deltaw = softthreshold(deltaw) }  : If we have soft-thresholding on, apply it.  
        adjustweight(deltaw) : Adjust the weight.
        if (verbose > 0) { printf("RL event: t = %f ms; deltaw = %f\n",t,deltaw) } : Show weight update information if debugging on.     
    }
}

FUNCTION hebbRL() {
    if ((RLon == 0) || (tlasthebbelig < 0.0)) { hebbRL = 0.0  } : If RL is turned off or eligibility has not occurred yet, return 0.0.
    else if (useRLexp == 0) { : If we are using a binary (i.e. square-wave) eligibility traces...
        if (t - tlasthebbelig <= RLlenhebb) { hebbRL = RLpotrate } : If we are within the length of the eligibility trace...
        else { hebbRL = 0.0 } : Otherwise (outside the length), return 0.0.
    } 
    else { hebbRL = RLpotrate * exp((tlasthebbelig - t) / RLlenhebb) } : Otherwise (if we re using an exponential decay traces)...use the Hebbian decay to calculate the gain.
      
}

FUNCTION antiRL() {
    if ((RLon == 0) || (tlastantielig < 0.0)) { antiRL = 0.0 } : If RL is turned off or eligibility has not occurred yet, return 0.0.
    else if (useRLexp == 0) { : If we are using a binary (i.e. square-wave) eligibility traces...
        if (t - tlastantielig <= RLlenanti) { antiRL = RLdeprate } : If we are within the length of the eligibility trace...
        else {antiRL = 0.0 } : Otherwise (outside the length), return 0.0.
    }
    else { antiRL = RLdeprate * exp((tlastantielig - t) / RLlenanti) } : Otherwise (if we re using an exponential decay traces), use the anti-Hebbian decay to calculate the gain.  
}

FUNCTION softthreshold(rawwc) {
    if (rawwc >= 0) { softthreshold = rawwc * (1.0 - synweight / wmax) } : If the weight change is non-negative, scale by 1 - weight / wmax.
    else { softthreshold = rawwc * synweight / wmax } : Otherwise (the weight change is negative), scale by weight / wmax.    
}

PROCEDURE adjustweight(wc) {
   synweight = synweight + wc : apply the synaptic modification, and then clip the weight if necessary to make sure its between 0 and wmax.
   if (synweight > wmax) { synweight = wmax }
   if (synweight < 0) { synweight = 0 }
}

*** Time differences
**** 1 core with fix
  Done; run time = 129.9 s; real-time ratio: 0.08.

Gathering spikes...
  Done; gather time = 27.0 s.
Minimum delay (time-step for queue exchange) is  10.0

Analyzing...
  Spikes: 189371 (1.89 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output1.0...
  Done; time = 28.2 s

Done; total time = 267.4 s.
**** 10 cores with fix 
  Done; run time = 33.7 s; real-time ratio: 0.30.

Gathering spikes...
  Done; gather time = 17.1 s.
Minimum delay (time-step for queue exchange) is  1.0

Analyzing...
  Spikes: 189371 (1.89 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output10.0...
  Done; time = 54.2 s

Done; total time = 118.2 s.
**** 1 core without fix (bug)
  Done; run time = 105.4 s; real-time ratio: 0.09.

Gathering spikes...
  Done; gather time = 71.0 s.
Minimum delay (time-step for queue exchange) is  10.0

Analyzing...
  Spikes: 131236 (1.31 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output1.0...
  Done; time = 55.1 s

Done; total time = 271.1 s.
**** 10 cores without fix (bug)
  Done; run time = 34.0 s; real-time ratio: 0.29.

Gathering spikes...
  Done; gather time = 23.2 s.
Minimum delay (time-step for queue exchange) is  1.0

Analyzing...
  Spikes: 123575 (1.24 Hz)
  Connections: 1113133 (525384 STDP; 111.31 per cell)
  Mean connection distance: 783.84 um
  Mean connection delay: 17.84 ms
Saving output as output10.0...
  Done; time = 56.8 s

Done; total time = 129.3 s.
* 15feb10 Sim working with STDP, RL + musculoskeletal arm in hpc (ma)
** end of output from running in 'ma'
Writing to MSM pipe: packetID=199.000000
[0.5, -0.5, 0.5, -0.5]
read from msm pipe: 184
[0.128276, 0.0939427, 0.118085, 0.0700224, 0.201749, 0.185767, 0.244255, 0.0703298, 0.154234, 0.141332, 0.135775, 0.107342, 0.145956, 0.10446, 0.101539, 0.126831, 0.154701, 0.0776188]
read from msm pipe: 19
[-0.175841, 1.41304]
Received packet 199.000000 from MSM: (0.128,0.094,0.118,0.070,0.202,0.186,0.244,0.070,0.154,0.141,0.136)
Received packet 199.000000 from MSM: (-0.176,1.413)
  t = 2.0 s (100%; time remaining: 0.0 s)

Writing to MSM pipe: packetID=200.000000
[0.5, -0.5, 0.5, -0.5]
read from msm pipe: 184
[0.128332, 0.0939427, 0.118054, 0.0700076, 0.201694, 0.185664, 0.244182, 0.0702832, 0.154242, 0.141379, 0.135819, 0.10737, 0.145849, 0.104454, 0.101537, 0.126407, 0.154472, 0.0776281]
read from msm pipe: 19
[-0.176098, 1.41247]
Received packet 200.000000 from MSM: (0.128,0.094,0.118,0.070,0.202,0.186,0.244,0.070,0.154,0.141,0.136)
Received packet 200.000000 from MSM: (-0.176,1.412)
  Done; run time = 78.8 s; real-time ratio: 0.03.

Gathering spikes...
  Done; gather time = 5.1 s.

Analyzing...
  Run time: 78.8 s (2-s sim; 2 scale; 1838 cells; 1 workers)
  Spikes: 25857 (7.03 Hz)
  Connections: 45993 (45693 STDP; 25.02 per cell)
  Mean connection distance: 794.83 um
  Mean connection delay: 9.95 ms
Saving output as data/m1ms...
  Done; time = 5.4 s
Plotting raster...
  Done; time = 1.8 s
Plotting connectivity matrix...
Plotting weight changes...

Done; total time = 398.9 s.
** output with mpi
 Done; run time = 9.1 s; real-time ratio: 0.22.

Gathering spikes...
  Done; gather time = 2.1 s.

Analyzing...
  Run time: 9.1 s (2-s sim; 2 scale; 1838 cells; 4 workers)
  Spikes: 25870 (7.04 Hz)
  Connections: 45993 (45693 STDP; 25.02 per cell)
  Mean connection distance: 794.83 um
  Mean connection delay: 9.95 ms
Saving output as data/m1ms...
  Done; time = 5.5 s
  Plotting raster despite using too many cores (4)

* 15feb14 Sharing variables between modules using shared.py
- shared.py will contain all objects that need to be shared across modules (network.py, analysis.py, arm.py), including
  static parameters (duration, popnames,...), and variables modified during runtime (cells, spikerecorders, ...)
- https://docs.python.org/2/faq/programming.html#how-do-i-share-global-variables-across-modules
- https://docs.python.org/2/faq/programming.html#what-are-the-best-practices-for-using-import-in-a-module
- http://docs.python-guide.org/en/latest/writing/structure/#modules
- http://stackoverflow.com/questions/19158339/python-why-are-global-variables-evil

* 15feb17 Spinal cord motor neuron using Izhikevich and reciprocal inhibition to antagonistic
** chat
Salvador Dura
trying to find what would be the best izhikevich parameters for spinal cord motor neuronâ€¦ any ideas?
[samnemo] suppose he doesn't describe those types of neurons in his paper...anything specific you want to capture in its firing pattern?
Salvador Dura
@types in paper â€” not specifically
@capture - right now using pyramidal so thought might be better to make a bit more accurate
also right now the spinal population following the L5B oscillationsâ€¦ so thought maybe different cell type would generate different output pattern to muscles
[samnemo] @following - is it helpful for the performance to follow the oscillations?
Salvador Dura
@helpful - I don't really know yet, but doesn't happen in real-life right? (thats the whole hypothesis of m1 grant â€¦ oscillations -> rate coding)
[samnemo] physiological (or parkinson's) tremor...@hypothesis - temporal -> rate code, true, but maybe it's wrong :) , does temporal -> rate code exclude all oscillations at the periphery?
[samnemo] anyway, haven't worked with those neurons...maybe ben/bill can comment
Salvador Dura
@exclude osc - probably not (eg phys tremor) â€¦ but likely to be diff freqs â€” of course my model is a super simplification â€¦ missing eg. all interneurons and a thousand other things, so that also explains why sync to layer 5b
thx
[samnemo] k was just curious about whether osc. helped since that's one of the debates re. oscillations - whether they have a function
Salvador Dura
might doâ€¦ I'm adapting model to run evol alg on hpc and tune params for reaching
neurosim-isb@im.partych.at
[samnemo] sounds cool
** izhi links
http://www.izhikevich.org/publications/spikes.pdf
http://izhikevich.org/publications/nesb.pdf
http://www.izhikevich.org/publications/hybrid_spiking_models.pdf
http://www.izhikevich.org/publications/whichmod.htm
http://www.izhikevich.org/publications/spikes.htm

** reciprocal inhib to antagon muscle
- "The afferent of the muscle spindle bifurcates in the spinal cord. One branch innervates the alpha motor neuron that causes
  the homonymous muscle to contract, producing the reflex. The other branch innervates the inhibitory interneuron, which in
  turn innervates the alpha motor neuron that synapses onto the opposing muscle. Because the interneuron is inhibitory, it
  prevents the opposing alpha motor neuron from firing, thereby reducing the contraction of the opposing muscle. "
- Implemented programatically during readout of DSC pop:
if s.antagInh: # antagonist inhibition
                if self.motorCmd[SH_EXT] > self.motorCmd[SH_FLEX]: # sh ext > sh flex
                    self.motorCmd[SH_FLEX] =  self.motorCmd[SH_FLEX]**2 / self.motorCmd[SH_EXT] / s.antagInh
                elif self.motorCmd[SH_EXT] < self.motorCmd[SH_FLEX]: # sh flex > sh ext
                    self.motorCmd[SH_EXT] = self.motorCmd[SH_EXT]**2 / self.motorCmd[SH_FLEX] / s.antagInh
                if self.motorCmd[EL_EXT] > self.motorCmd[EL_FLEX]: # el ext > el flex
                    self.motorCmd[EL_FLEX] = self.motorCmd[EL_FLEX]**2 / self.motorCmd[EL_EXT] / s.antagInh
                elif self.motorCmd[EL_EXT] < self.motorCmd[EL_FLEX]: # el ext > el flex
                    self.motorCmd[EL_EXT] = self.motorCmd[EL_EXT]**2 / self.motorCmd[EL_FLEX] / s.antagInh
- Alternative would be to implement inhibitory interneuron
** Burke 2013 Spinal motoneurons review
[[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150219_204957.png][fig]] - steady firing 20-60 Hz

** ModelDB motoneurons
- https://senselab.med.yale.edu/ModelDb/ModelList.cshtml?id=276&celldescr=no
- maybe can try to tune izhi params to IF curve if have time!
- online simulation of spinal network and muscles - http://remoto.leb.usp.br/remoto/index.html
- firing rates 10-30 Hz

* 15feb18 Manually tuning network to learn 1 target
- velsh = shflex - shext (by default flexes = increases = shflex higher)
- velel = elflex - elext (by default extends = decreases = elext higher)
- middle subpopulations higher activity -- some bias??
- explor movs now change on a 10ms basis -- too fast 
- added explor movs and antagonistic
- difficult to tune by hand -- will try evol

* 15feb19 Added 3d distance for delays
Added 3d distance:
zpath=(abs(s.zlocs-s.zlocs[gid]))**2
distances3d = sqrt(xpath + ypath + zpath)
s.conndata[3].append(s.mindelay + distances3d[preids]/float(s.velocity))
mean connection delay went from 10.83ms to 11.30ms
in 1k cell 5-sec sim, spikes went from 40979 (8.48 Hz) to 33627 (6.95 Hz)
raster shows pretty similar oscillations

* 15feb20 Setup evol alg
- test input parameters from main -- cannot use * !
- copy over evol island and adapt 
- add params automatically in a loop - make flexible

* 15feb21 sim - 2 targets (single), 1k neurons, dummyArm 	    :results:
** changeset
changeset:   105:a993e07e853c
user:        Salvador Dura <salvadordura@gmail.com>
date:        Mon Feb 23 22:43:14 2015 -0500
summary:     fixed bug - lasttimes were not reseted during setup

** evol.py params
imdatadir = 'data/15feb21_evol' # folder to save sim results
saveMuscles = 0
num_islands = 10 # number of islands
numproc = 4 # number of cores per job
max_migrants = 1 #
migration_interval = 5
pop_size = 10 # population size per island
num_elites = 1 
max_generations = 1000
max_evaluations = max_generations *  num_islands * pop_size
mutation_rate = 0.4
crossover = 0.5

# parameter names and ranges
pNames = []
pRanges = []
pNames.append('trainTime'); pRanges.append([30*1e3,180*1e3]) #int
pNames.append('plastConnsType'); pRanges.append([0,1,2,3]) # int
pNames.append('stdpFactor'); pRanges.append([0,1])
pNames.append('RLfactor'); pRanges.append([0,4])
#pNames.append('stdpwin'); pRanges.append([10,30])
pNames.append('eligwin'); pRanges.append([50,150])
#pNames.append('RLinterval'); pRanges.append([50,100])
#pNames.append('maxweight'); pRanges.append([15,75])
pNames.append('trainBackground'); pRanges.append([50,200])
pNames.append('testBackground'); pRanges.append([50,200])
#pNames.append('minRLerror'); pRanges.append([0.0,0.01])
pNames.append('cmdmaxrate'); pRanges.append([5,20])
pNames.append('cmdtimewin'); pRanges.append([50,150])
pNames.append('explorMovsFactor'); pRanges.append([1,10])
#pNames.append('explorMovsDur'); pRanges.append([500,1500])

** results (bugs)
*** bug
-- bug: not calculating arm angles properly --due to self.startAng=self.ang and modifying self.ang -- need to use
list(self.ang)
-- bug:  lasttimes were not reseted during setup
*** island_3 gen_18_can_5 (Target errors:  [ 0.15352302  0.06460844]  avg:  0.109065727645)
*** .run
Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
/tera/salvadord/m1ms/sim
numprocs=4

Setting parameters...
Benchmarking...
  Running at 45% default speed (182% total)
  Setting outfilestem=../data/15feb21_evol_island_3/gen_18_cand_5
  Setting trainTime=180000.0
  Setting plastConnsType=3.0
  Setting stdpFactor=0
  Setting RLfactor=4
  Setting eligwin=102.96167326867445
  Setting trainBackground=78.96491328878155
  Setting testBackground=132.36448561938408
  Setting cmdmaxrate=17.369441210963718
  Setting cmdtimewin=52.541630731296245
  Setting explorMovsFactor=1.262821609816099

Creating simulation of 967 cells for 1.0 s on 4 hosts...
  Number of cells on node 2: 242 
  Number of cells on node 1: 242 
  Number of cells on node 3: 241 
  Number of cells on node 0: 242 
Calculating connection probabilities (est. time: 88 s)...
  Number of connections on host 1: 3959
  Number of connections on host 2: 3842
  Number of connections on host 3: 3771
  Done; time = 0.1 s
Making connections (est. time: 198 s)...
  Number of connections on host 0: 3938
  Number of STDP connections on host 3: 1665
  Number of STDP connections on host 2: 1715
  Number created on host 3: 193
  Number of STDP connections on host 1: 1797
  Number of STDP connections on host 0: 1772
  Done; time = 0.2 s
Creating background inputs...
  Number created on host 2: 194
  Number created on host 1: 194
  Number created on host 0: 194

Setting up STDP...

Setting up virtual arm...

Running...
  t = 5.0 s (2%; time remaining: 202.0 s)
  t = 10.0 s (5%; time remaining: 160.3 s)
  t = 15.0 s (8%; time remaining: 173.6 s)
  t = 20.0 s (11%; time remaining: 185.0 s)
  t = 25.0 s (13%; time remaining: 187.9 s)
  t = 30.0 s (16%; time remaining: 193.1 s)
  t = 35.0 s (19%; time remaining: 190.0 s)
  t = 40.0 s (22%; time remaining: 186.2 s)
  t = 45.0 s (25%; time remaining: 181.0 s)
  t = 50.0 s (27%; time remaining: 177.9 s)
  t = 55.0 s (30%; time remaining: 173.3 s)
  t = 60.0 s (33%; time remaining: 167.8 s)
  t = 65.0 s (36%; time remaining: 160.7 s)
  t = 70.0 s (38%; time remaining: 153.0 s)
  t = 75.0 s (41%; time remaining: 146.3 s)
  t = 80.0 s (44%; time remaining: 139.7 s)
  t = 85.0 s (47%; time remaining: 134.4 s)
  t = 90.0 s (50%; time remaining: 127.3 s)
  t = 95.0 s (52%; time remaining: 119.1 s)
  t = 100.0 s (55%; time remaining: 112.2 s)
  t = 105.0 s (58%; time remaining: 105.5 s)
  t = 110.0 s (61%; time remaining: 98.2 s)
  t = 115.0 s (63%; time remaining: 91.2 s)
  t = 120.0 s (66%; time remaining: 84.1 s)
  t = 125.0 s (69%; time remaining: 77.3 s)
  t = 130.0 s (72%; time remaining: 70.5 s)
  t = 135.0 s (75%; time remaining: 63.2 s)
  t = 140.0 s (77%; time remaining: 56.0 s)
  t = 145.0 s (80%; time remaining: 49.0 s)
  t = 150.0 s (83%; time remaining: 41.8 s)
  t = 155.0 s (86%; time remaining: 34.7 s)
  t = 160.0 s (88%; time remaining: 27.8 s)
  t = 165.0 s (91%; time remaining: 20.9 s)
  t = 170.0 s (94%; time remaining: 13.9 s)
  t = 175.0 s (97%; time remaining: 6.9 s)
  t = 180.0 s (100%; time remaining: 0.0 s)
  Done; run time = 247.7 s; real-time ratio: 0.73.

Gathering spikes...
  Done; gather time = 7.5 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 247.7 s (180-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 4561007 (26.20 Hz)
  Connections: 15510 (6949 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms

Setting up virtual arm...

Running...
  Done; run time = 0.7 s; real-time ratio: 1.38.

Gathering spikes...
  Done; gather time = 3.7 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 0.7 s (1-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 27777 (28.72 Hz)
  Connections: 15510 (0 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms
Saving output as data/m1ms...
  Done; time = 0.0 s

Setting up STDP...

Setting up virtual arm...

Running...
  t = 5.0 s (2%; time remaining: 235.5 s)
  t = 10.0 s (5%; time remaining: 242.3 s)
  t = 15.0 s (8%; time remaining: 217.4 s)
  t = 20.0 s (11%; time remaining: 208.3 s)
  t = 25.0 s (13%; time remaining: 197.5 s)
  t = 30.0 s (16%; time remaining: 186.5 s)
  t = 35.0 s (19%; time remaining: 183.7 s)
  t = 40.0 s (22%; time remaining: 175.9 s)
  t = 45.0 s (25%; time remaining: 165.5 s)
  t = 50.0 s (27%; time remaining: 158.3 s)
  t = 55.0 s (30%; time remaining: 149.9 s)
  t = 60.0 s (33%; time remaining: 145.8 s)
  t = 65.0 s (36%; time remaining: 139.4 s)
  t = 70.0 s (38%; time remaining: 132.3 s)
  t = 75.0 s (41%; time remaining: 128.1 s)
  t = 80.0 s (44%; time remaining: 124.3 s)
  t = 85.0 s (47%; time remaining: 121.2 s)
  t = 90.0 s (50%; time remaining: 116.5 s)
  t = 95.0 s (52%; time remaining: 113.2 s)
  t = 100.0 s (55%; time remaining: 107.3 s)
  t = 105.0 s (58%; time remaining: 101.5 s)
  t = 110.0 s (61%; time remaining: 96.1 s)
  t = 115.0 s (63%; time remaining: 90.5 s)
  t = 120.0 s (66%; time remaining: 85.3 s)
  t = 125.0 s (69%; time remaining: 78.8 s)
  t = 130.0 s (72%; time remaining: 72.0 s)
  t = 135.0 s (75%; time remaining: 65.3 s)
  t = 140.0 s (77%; time remaining: 58.9 s)
  t = 145.0 s (80%; time remaining: 51.8 s)
  t = 150.0 s (83%; time remaining: 44.5 s)
  t = 155.0 s (86%; time remaining: 37.1 s)
  t = 160.0 s (88%; time remaining: 29.8 s)
  t = 165.0 s (91%; time remaining: 22.4 s)
  t = 170.0 s (94%; time remaining: 15.0 s)
  t = 175.0 s (97%; time remaining: 7.5 s)
  t = 180.0 s (100%; time remaining: 0.0 s)
  Done; run time = 271.1 s; real-time ratio: 0.66.

Gathering spikes...
  Done; gather time = 8.7 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 271.1 s (180-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 4778774 (27.45 Hz)
  Connections: 15510 (6949 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms

Setting up virtual arm...

Running...
  Done; run time = 1.2 s; real-time ratio: 0.87.

Gathering spikes...
  Done; gather time = 3.4 s.
Minimum delay (time-step for queue exchange) is  1.0

Closing dummy virtual arm ...

Analyzing...
  Run time: 1.2 s (1-s sim; 1 scale; 967 cells; 4 workers)
  Spikes: 26462 (27.37 Hz)
  Connections: 15510 (0 STDP; 16.04 per cell)
  Mean connection distance: 882.87 um
  Mean connection delay: 13.51 ms
Saving output as ../data/15feb21_evol_island_3/gen_18_cand_5_target_0...
  Done; time = 0.0 s
Target errors:  [ 0.15352302  0.06460844]  avg:  0.109065727645

Done; total time = 544.6 s.

*** right target consistently better (found multiple bugs)
- check what is issue by running locally with same params
- output is different each time - maybe due to id32 hash in rand num generation -- NO (with hash
  still reproducible); was due to arm explor moves randomness 
- the amount of spikes was 50% or 25% x the hpc output -- could be just random
- bias towards left target still present
- Tried increasing EB5->DSC weight to 2.0
- Need to check RL is updating weights properly -- do small systematic test
- plotweightchanges was showing absolute final weight; not absolute change --> negative weight changes to RL working
- check RL + error signal -- didn't seem to be consistent with target distance -- fixed small bug

- Hypothesis of why RL not modifying weights selectively: all motor subpops getting activated similarly (= similar elig
  traces), explor movements being implemented at the level of pop readout
-- solution: implement explor movements by increasing background noise to motor subpops
-- did that but couldn't modify rates on-the-fly because using NetStims instead of NSLOCs
-- having issue with NSLOCs-- doesnt recognize some params -- cannot connect using netcon! compare with netstim.mod -- not sure how fixed 
- Added backgrundgid so can target specific netstim based on gid
- EB5->DSC initial weight was too high (4) so modifying background noise didn't make much of a difference -- lowered to 0.5

- Background input for train and test is the same -- not modified before testing because don't call addBackground

- After first target something makes the stdpmechs stop working -- no change in synWeight
-- not in finalizeSim() or plotData()
-- found! after init() the pre and postcell of the stdpmech gets disconnected so tlastpost always = 0

- Cannot restart sim -- weight changes are different second time around even if I do all same steps (+ clear_gid)
-- even if only stdp (RL off and explor movs off) !! 
-- doesn't even work properly on multiple hosts when restarting everything
-- tried removing s and reloading shared again, but apparently python doesnt support unloading of modules

- Only solution left is to run on single target and sum targets in evol.py -- done

- Check if higher conn for certain muscles is just random -- yes (start with low weights so it doesnt have major effect)

* 15feb22 exploratory movements via E5B; and inhib spinal cord pop
** explor movs via E5B noise
- now dont require direct stimulation of each muscle group randomly, because E5B connected randomly to DSC so can just increase
noise in E5B = more realistic
- also better because makes more sense for RL+stdp: presyn spike before post
- however since homog random noise, all neurons get excited similarly and benefits the initial biased connection to some
  muscles
-- stimulate up to 50% of the EB5 cells - with random duration, and fixed strength (s.backgroundrateExplor = 1000)
-- still difficult to get good exploratory movements
** inhib spinal cord pop
Ia inhibitory interneuron[edit]
Role in reciprocal inhibition during the stretch reflex[edit]
Joints are controlled by two opposing sets of muscles called extensors and flexors that must work in synchrony to allow
proper and desired movement.[11] When a muscle spindle is stretched and the stretch reflex is activated, the opposing muscle
group must be inhibited to prevent from working against the agonist muscle.[9][11] The spinal interneuron called Ia
inhibitory interneuron is responsible for this inhibition of the antagonist muscle.[11] The Ia afferent of the muscle spindle
enters the spinal cord, and one branch synapses on to the alpha motor neuron that causes the agonist muscle to contract.[11]
Thus, it results in creating the behavioral reflex. At the same time, the other branch of the Ia afferent synapses on to the
Ia inhibitory interneuron, which in turn synapses the alpha motor neuron of the antagonist muscle.[11] Since Ia interneuron
is inhibitory, it prevents the opposing alpha motor neuron from firing. Thus, it prevents the antagonist muscle from
contracting.[11] Without having this system of reciprocal inhibition, both groups of muscles may contract at the same time
and work against each other. This results in spending a greater amount of energy as well. In addition, the reciprocal
inhibition is important for mechanism underlying voluntary movement.[11] When the antagonist muscle relaxes during movement,
this increases efficiency and speed. This prevents moving muscles from working against the contraction force of antagonist
muscles.[11] Thus, during voluntary movement, the Ia inhibitory interneurons are used to coordinate muscle
contraction. Further, the Ia inhibitory interneurons allow the higher centers to coordinate commands sent to the two muscles
working opposite of each other at a single joint via a single command.[11] The interneuron receives the input command from
the corticospinal descending axons in such a way that the descending signal, which activates the contraction of one muscle,
causes relaxation of the other muscles.[9][10][11][12]

Added IDSC
E5B->IDSC = E5B->EDSC
IDSC->EDSC = antagonistic muscle
*** playing with params
-Aim is to excite subsets of E5B that excite EDSC+IDSC and activate different muscles (+inhibit antagonistic muscle) to
perform varied exploratory movements.

- One issue is that each E5B neuron projects to different muscles so there is coactivation and little movement.
-- exciting only 2 E5B neuron at a time seems to produce better results. eg. one E5B excites 2 IDSC of ext+flex, so both EDSC
-- will be inhibited
-- maybe can generate random list without repetition so chance to excite all

- Other issue is that oscillatory activity seems to dominate E5B and thus EDSC 
-- if disconnect E5R->E5B and E2R->E5B works better
-- to avoid, increased E5B->DSC background weight to backgroundWeightExplor = 10 and backgroundrateExplor = 3000

- Also EDSC pyramidal neurons slower to respond to E5B input (if replace with inh FS, response is faster)
-- increased connweights[EB5,EDSC,AMPA]=8.0 compared to connweights[EB5,IDSC,AMPA]=3.0

- Initial burst of excitatory activity due to transitory regime (delay E5B->IDSC->EDSC)
-- start moving arm only 100 ms after onset

- Inhibition IDSC->EDSC not working 
-- maybe becasue each IDSC inhibits a single EDSC unit and that unit may not be the one active -- should inhibit all units of
antagonistic pop -- yes, that worked

- Works well wehn use exploratory movement directly in ESDC -- but had to set noise to 0 so all neurons were recruited!!
  [[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150303_215937.png][fig]]



backgroundrateExplor = 3000 # weight for background input for exploratory movements
backgroundweightExplor = 10 # weight for background input for exploratory movements (fixed)
explorCellsFraction = 0.1 # fraction of E5B cells to be actiavated at a time during explor movs
connweights[EB5,EDSC,AMPA]=8.0
connweights[EB5,IDSC,AMPA]=3.0
connweights[IDSC,EDSC,GABAA]=4.0 

* 15feb23 PMd data and decoders
** Austin - Kernel method (60% 8 targets)
*** chat with Adi
i am running the pmd analysis right now
Austin â€¢ 2/5/14, 1:50 PM
soundsgood
Aditya Tarigoppula
Austin Brockmeier
looks like I can get 60% using spike kernels to classifiy the first 0.5 s
or closer to 50%
actually
Austin â€¢ 2/5/14, 1:50 PM
and this center out ?
Aditya Tarigoppula
Austin Brockmeier
yeah
Austin â€¢ 2/5/14, 1:50 PM
or reward?
Aditya Tarigoppula
Austin Brockmeier
Zee
Austin â€¢ 2/5/14, 1:50 PM
ohh ok zee
Aditya Tarigoppula
Austin Brockmeier
so 1/8 chance
Austin â€¢ 2/5/14, 1:50 PM
yeah
Aditya Tarigoppula
Austin Brockmeier
what was marius getting?
Austin â€¢ 2/5/14, 1:51 PM
thats pretty good
let me check
whats the file name again
?
Aditya Tarigoppula
Austin Brockmeier
58% is what i got on this run across 20 monte carlos divisions of the trials into 2/3 for training and 1/3 for testing
Austin â€¢ 2/5/14, 1:51 PM
is it ZEE from march
Aditya Tarigoppula
Austin Brockmeier
MAP2zee03252010003-01
Austin â€¢ 2/5/14, 1:51 PM
of 2010?
ok
Aditya Tarigoppula
Austin Brockmeier
yeah
Austin â€¢ 2/5/14, 1:52 PM
so here is what he said but I am not sure how much he got on this file specifically
For the manual task, the decoder goes to about 25-30% at 300ms after the target appears, 300ms being the fixed time before the centre cue disappears and the NHP is free to move. Performance keeps improving to about 40-50% at 500ms and levels off at about this value. This kind of performance seems in line with what we were getting on the old center-out data from the Shenoy group. With 100 units and longer delay periods (500-1000ms), we typically decode around 60-70% (also out of 8 targets), though in the best dataset this goes up to 100% fairly quickly after target onset.
so the ZEE march data was collected by pratik
just FYI
2/5/14, 1:53 PM
Aditya Tarigoppula
Austin Brockmeier
ok
i will get you the results and pretty plots by end of week?
*** email to austin
Hi Austin,

How is it going? How's your new job in the UK?

Here we are getting ready for DARPA's "super final demo" probably in a month or so. After that I'm starting in a new NIH project with Bill (modeling M1 in detail), so I'll stay in the cold NY a couple more years :)

I'm trying to decode target position from PMd data to feed into our spiking models, and Adi forwarded me some of the results you were getting using the kernel method... here's what you said:

"Austin Brockmeier
looks like I can get 60% using spike kernels to classifiy the first 0.5 s
Austin Brockmeier
58% is what i got on this run across 20 monte carlos divisions of the trials into 2/3 for training and 1/3 for testing
Austin â€¢ 2/5/14, 1:51 PM
is it ZEE from march
MAP2zee03252010003-01"

It would be great if you could send me that code so I can try it out. Since Stanford/UCL are not the most collaborative guys, we might as well end up using our own code :) Of course if we end up using it for the final demo we will include your name in the paper.

hope everything is going great
cheers mate! ;)
salva

*** emails from Austin and link to code
It will take me a little bit to extract all of the utility functions that go into it (a couple common packages such as SVM are also used).

I have attached some of the text and results extracted from a draft of my dissertation. They are probably what Adi is
referring too. From the visualizations, tou can see that the 60% decoding comes from mostly a left versus right sort of
division.

Yes, the matlab code would be great. I will need to plug the output of your decoder into the python/NEURON-based BMM somehow,
so might have to play with your code. Regarding the funcs, if easier you can just send me the full set of utility functions,
and I can download the common packages (eg. SVM). Thanks, I appreciate it.

Thanks for the thesis results too -- they look good, especially since I will probably end up using only either 2 or 4 targets
(since its a pretty complex task combining everything).

So far I have found the code that (1) extracts windows of spike trains from the data Adi provided recorded from Zee (2) then
I have a script that works in batch mode to compute the full distance matrix (slowest part) (3) runs 10 monte carlo cases to
get test set error

Here is a dropbox link to the code for the spike kernel based target classifier

https://www.dropbox.com/sh/vbrxj3sq4ftox1l/AADHWziKp98WRwPU7lkKm2Yfa?dl=0


The kernel I used is called the memoryless cross-intensity.. It is nothing more than smoothing the spike trains with Gaussian
window and computing the inner-product. The kernel size on the Gaussian is 1/10 Hz and I use a 1 second window following
target presentation

I didn't include the original data from Adi, but I did include the script I used to extract the windows and the extracted
windows themselves (as MATLAB datafilee)

Requires:  minFunc     and  libsvm    
               
Hope this helps and let me know of what is missing or further explanation
** Marius (UCL) - Poisson SSM, Recursive Linear Model (RLM) -- better+faster than GPFA
*** email 1
I am afraid I was not able to get any decoding at all out of the passive observation task. In this email I will tell you what I did, maybe you can tell me if I am not using the data correctly. First, some information about the decoder. 

I used the (target) decoder that I have had most success with on some of the old datasets. I have found this type of decoder to work better than various neural network based decoders and SVM based decoders, mostly because it directly takes into account the temporal dynamics of the responses, both the dynamics in response to stimuli/behaviour, and the noisy and shared dynamics on a trial by trial basis. 

The decoder models the spiking data with one of our Poisson state-space model (the RLM which I have been using in the work with Cliff as well), which has a different PSTH for each condition K, but the same dynamics model to account for the noise correlations. This model gives each trial a probability P(data(1:t) | condition) where t is any time index, and on test data we can decode by picking the condition which gives the data the highest probability.

For the manual task, the decoder goes to about 25-30% at 300ms after the target appears, 300ms being the fixed time before the centre cue disappears and the NHP is free to move. Performance keeps improving to about 40-50% at 500ms and levels off at about this value. This kind of performance seems in line with what we were getting on the old center-out data from the Shenoy group. With 100 units and longer delay periods (500-1000ms), we typically decode around 60-70% (also out of 8 targets), though in the best dataset this goes up to 100% fairly quickly after target onset. 
*** email 2
Hi Adi,

I thought I had replied to this, apparently not. Sorry about the delay. Please see my comments below!

Cheers, Marius


On 7 January 2014 17:23, Aditya Tarigoppula <aditya30887@gmail.com> wrote: Good Morning Marius,

Happy New Year to you too! I have included Salva in this conversation because he will be directly involved in integrating
SSMs with BMMs.

The initial results stated by you are not entirely unexpected. The monkey from which I am recording now is naive with respect
to the cursor. This monkey was never trained to manually perform the center out reaching task, hence he never got a chance to
incorporate the cursor into his motor map (i.e form an association between the cursor and his hand's end point). These files
were recorded as part of a different line of study which also requires us to see if the data showed any modulation with
respect to target/direction.

The preliminary results we have on the passive data are as follows: 

To begin with, We treated it as a binary classification problem of left versus right (we excluded targets 3 and 4), targets
1,5, and 8 were considered as right and targets 2,6,7 were considered left.  Austin (a PHd student in Dr. Principe's lab) ran
multiple Monte Carlo divisions of the dataset into training and test sets of size 2/3 samples per class and 1/3 samples per
class, respectively. He computed the average across 100 Monte Carlo divisions and also computed the chance rate. This work
was done on LFPs of coupe of files and he is working on improving these results.

On dataset 12/4 (which I can send over to you if you desire) file 1, both M1 and Pmd show above chance classification 58% for
PmD and 57% for motor.

Motor cortex shows 56% and 60% on files 12/4 file 2 and 12/9 file 3 respectively. All the rest of the classification rates
are below chance.  If you can send these over I can tell you the performance of the RLM decoder on them, to make sure we are
looking at the same data in similar ways. The RLM decoder can also take LFPs as input, and even combine the LFP information
with the spiking information. However, maybe I'm missing some obvious information, or maybe the RLM decoder is for some
reason crap on this kind of data --- we will know it If I cannot achieve your classification performance.

Having said that, there is one thing that can be done to improve the quality of the input data. I should have included this
information in my last email, sorry about that! Eye tracking was used to make sure that the cursor moved towards the target
only when the monkey's gaze was maintained on the task plane (i.e. I am not sure if he looking at he cursor but I am sure
that he is looking at the monitor where the cursor is moving). The trial is not aborted unless the maximum trial time alloted
to the task runs out. Therefore, there is a possibility that you might have also used the neural data pertaining to the error
trials while you performed your analysis. I would suggest considering the trials that end up with states greater than 8 as
INVALID trials (its valid even for the file corresponding to the manual task). i.e. A successfully completed trial usually
has the following sequence of states 1-2-3-4-5-6/7-8-0. If a given trial has a state above 8 or another way to put it is if a
given trial does not have a state 8 then the trial was an incomplete/aborted trial.

I have removed that already. 

Do you think you have enough trials (statistically speaking) for each target? (for both manual and passive files) 

There are enough trials to observe statistically significant differences, yes. I interpret the fact that the RLM decoder is
completely insignificant on the previous data as saying there's really no information there, at least no linearly encoded
information. I think almost all of the time even simple linear decoders will have some significant classification
performance. Nonlinear / more fancy decoders can improve on this, but it's very rare in a classification task to have an
insignificant linear decoder and a significant nonlinear decoder.

I recently recorded a passive observation 2 target center out reaching task. This allows us to have atleast 40 trials for
each target. Would you like to have a look at the PMd data for this file?
 
Would you like to try your decoder on the M1 data to see if that gives a better performance. As far as the milestone is
concerned it states that BMMs can use input from the unlesioned part of M1. This allows us to treat the M1 region that we
recorded from as the unlesioned region.

Sure, I can tell you if the RLM finds anything significant on both these PmD and M1 files, but as I said above, we should
first make sure I calibrate it in comparison to your previous positive results for those M1 and PmD datasets.
 
How would you compare RLM with other models that Dr. Sahani's lab has proposed in the past such as HSLDS?

It's important to distinguish the use of the RLM as a statistical model of the spiking and as a decoder, although on the same
data, the same RLM can be trained to perform both tasks.

As a statistical model of the spiking, the RLM seems to do better than Poisson LDS, but we've never directly compared it to
Biljana's hidden switching model (if that's what you meant by HSLDS). The RLM is however in the same broad class of hidden
state space models, with the advantage of being much faster and direct to train. In particular we can optimize the model's
likelihood exactly.

As a decoder, on the old center-out data from the Shenoy lab, the RLM achieves 6.5mm average error, where our best previous
decoder (mixture of trajectories model, Byron Yu's work, cover of J Neurphys methods a couple of years ago) achieved 11mm
average error.

I agree with your view on using a longer delay period. I also agree with your view on making the monkey more invested in the
task and to basically make him aware of the task paradigm. We are moving towards it.

Cool. I think it will be scientifically very interesting to see the passive and active neural activity in well-trained
attentive monkeys.
** Adi/John - Byron's GPFA, John's GLDS, PLDS
*** email
I believe all of you have access to my Google folder called "shared with Babak (Manual files)". This folder consists of the .plx files (M1 files), .mat files which contains information extracted from the plx files using code present in the sub folder 'loading routines', the definitions and description of the variables in the mat files are stated in the 'var_defs.txt' in the loading routines folder. Feel free to call me or email me if you have any further questions with regard to these files. 

There are 8 targets in the task. The targets are located 45 deg apart from each other.The Target locations of each of the targets and the center circle can be found in the variable called "c3d_Struct.TP_TABLE.X_Global / Y_Global".

c3d_Struct.TP_TABLE.X_Global(1), c3d_Struct.TP_TABLE.Y_Global(1) = center location

Following which index 2 - 9 gives you location of the 8 targets. 
 
% for visualization purposes of the targets
for i = 1 : 8
plot(c3dstruct.TARGET_TABLE.X_GLOBAL(c3dstruct.TP_TABLE.EndTarget(i)),c3dstruct.TARGET_TABLE.Y_GLOBAL(c3dstruct.TP_TABLE.EndTarget(i)),'*'); 
pause();hold on;
end

targets are arranged as follows with respect to the center: 1 is to the right (0 deg), 2 is to the left (180), 3 is up (90), 4 is down (270), 5 is at 45 deg, and then move anticlockwise for 6 (135), 7 (225) and 8th (315) target. 

There is more information about the target radius, task paradigm etc in c3d_struct. I will guide you to it if you ever need it.

c3dstruct.CALIBRATION has the lengths of monkey arm. Look at 'Description' for more details. For each sub variable in struct "c3d_struct" there is description available. 

Also attached is the GPFA code provided to us by Dr. Yu (Byron Yu). John developed codes for GLDS and PLDS using their papers as guide. Please ask John to send you those codes too. I am not sure if I applied GPFA to these files. But I can always apply it and send you the results when I do. 

Please keep me posted on the evolving collaboration with Dr. Shenoy's group. I would like to help in any way possible to make
this successful. 

*** chat with john - bitbucket link 
the farthest i got was applying it to predicting shoulder and elbow kinematics
i think the top level scripts for testing were
runEncodignModels.m
and runEncodingModelsWithPos.m
https://bitbucket.org/nasnysom/pldsglds

i think it requires minFunc - http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html
also: http://research.microsoft.com/en-us/um/people/minka/software/lightspeed/
*** GPFA code
- well documented but many files/funcs
- example script helpful
- not target decoder -- models neural dyanmics in lower dim space (like jPCA)
- can decode target using: " This model gives each trial a probability P(data(1:t) | condition) where t is any time index,
  and on test data we can decode by picking the condition which gives the data the highest probability."
- alternatively can use lower-dimension represntation as input to BMM - STDP between SSM output units and BMM

*** Johns PLDS GLDS code
- trouble isntalling lightspeed

Compiling lightspeed 2.7 mex files...
xcodebuild: error: SDK "macosx10.7" cannot be located.
xcrun: error: unable to find utility "clang", not a developer tool or in PATH

    mex: compile of ' "flops.c"' failed.

tried: http://www.mathworks.com/matlabcentral/answers/103904-can-i-use-xcode-5-as-my-c-or-c-compiler-in-matlab-8-1-r2013a-or-matlab-8-2-r2013b

now get:
>> install_lightspeed
Compiling lightspeed 2.7 mex files...
/u/salvadord/.matlab/R2013a/mexopts.sh: line 176: unexpected EOF while looking for matching `''
/u/salvadord/.matlab/R2013a/mexopts.sh: line 205: syntax error: unexpected end of file
/u/salvadord/.matlab/R2013a/mexopts.sh: line 176: unexpected EOF while looking for matching `''
/u/salvadord/.matlab/R2013a/mexopts.sh: line 205: syntax error: unexpected end of file
/Applications/MATLAB_R2013a.app/bin/mex: line 1343: -c: command not found

    mex: compile of ' "flops.c"' failed.

Error using mex (line 206)
Unable to complete successfully.

Error in install_lightspeed (line 52)
eval(['mex' flags '-c flops.c']);
** EMG data
*** Austin: MAP2zee03252010003-01
- same as M1 data used in paper -- would be nice to use the same PMd data!!
**** notes (Brandi)
Brandi & Shaohua
======================

3/25/2010

Task: Mar06NewAngF50cart4cmR50

KinARM was recalibrated today by Mulugeta and Brandi.

Zee spent about 20 minutes on the task before recording started. [110]
File 1: about 7.5 minutes, Zee performs the task well. 75 rewards [185]
File 2: about 7.5 minutes, performs the task well. about 70 rewards [260]
File 3: about 10 minutes, performs the tasks well. about 95 rewards [361]
File 4: about 9-10 minutes, performs the tasks well. about 95 rewards [458]
File 5: about 8-9 minutes, performs the tasks well. about 80 rewards [543]
File 6: about 8-9 minutes, performs the tasks well. about 80 rewards [628]
File 7: about 9 minutes, performs the tasks well. about 90 rewards [725]

video: zee20100325.

c3d file saved today.

She got about 140 ml water today.

Brandi & Shaohua
======================


*** Gil: MAP2zee04292009002_6sec.plx
** Playing back PMd data 
*** vec.play
- also vec.record
- not sure why need patternStim -- seems to be a unit (similar to netstim / NSLOC)
*** bill's new method?
patternstim
yes in my mpi example
cellrun.py
/u/billl/nrniv/talks/mpiHHnetTut/cellrun.py
pattern = h.PatternStim() # /usr/site/nrniv/nrn/src/nrnoc/pattern.mod
pattern.play(tvec, idvec)
have to do things in correct order -- i messed that up first time around

* 15mar01 sim - 2 targets (single), 1k neurons, dummyArm  :results:
** changeset
changeset:   119:07393792f1c7
user:        Salvador Dura <salvadordura@gmail.com>
date:        Mon Mar 02 01:59:52 2015 -0500
summary:     prepared for 15mar01_evol - debug
** description
debugged 15feb21
explor movs using E5B noise

* 15mar04 sim - 2 targets (single), 1k neurons, dummyArm - testing different algorithms  :results:
** changesets
*** genetic
changeset:   129:f3d6287f5a3d
user:        Salvador Dura <salvadordura@gmail.com>
date:        Wed Mar 04 01:12:08 2015 -0500
summary:     prepared for 15mar04_sim
*** krichmarEvol
changeset:   130:d525bfa9242b
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Wed Mar 04 10:47:18 2015 -0500
summary:     prepared for 15mar04_sim_krichmarEvol

*** evolutionStrategy
changeset:   132:0d7b4034b783
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Wed Mar 04 22:27:27 2015 -0500
summary:     15mar04_sim evolutionStrategy
*** simulatedAnnealing
changeset:   133:dfce13a74937
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Thu Mar 05 11:42:16 2015 -0500
summary:     15mar04 simulatedAnnealing
*** diffEvolution
changeset:   135:ce27a7a92f0f
user:        Salvador Dura <salvadordura@gmail.com>
date:        Thu Mar 05 17:12:15 2015 -0500
summary:     15mar04 diffEvolution

** description
debugged 15mar01
explor moves via EDSC+IDSC
reduced num params being optim
testing different optim methods from inspyred
** results
*** genetic (0.1462, 0.151)
**** pop=2
0, 2, 0.300133783635, 0.146268855105, 0.22320131937, 0.22320131937, 0.0769324642647
50, 2, 0.20004290363, 0.146268855105, 0.173155879368, 0.173155879368, 0.0268870242623

80, 2, 0.20004290363, 0.146268855105, 0.173155879368, 0.173155879368, 0.0268870242623
**** pop=100
0, 100, 0.317487304691, 0.159470073482, 0.216980743021, 0.222929138415, 0.0367570158357
21, 100, 0.151096833862, 0.151096833862, 0.151096833862, 0.151096833862, 2.22044604925e-16
*** krichmarEvol (0.138,0.139=BUG)
**** pop=2
0, 2, 0.300332272954, 0.143344059494, 0.221838166224, 0.221838166224, 0.0784941067298
50, 2, 0.141606055411, 0.138317739195, 0.139961897303, 0.139961897303, 0.00164415810839
**** pop=100
0, 100, 0.320947708087, 0.162105091636, 0.213414640609, 0.222150583396, 0.034445638365
50, 100, 0.193492181148, 0.139979383936, 0.171605491955, 0.171018238002, 0.00812888089593

75, 100, 0.192731596366, 0.139979383936, 0.172123326834, 0.171688023346, 0.0079540669261
**** BUG!
If one of the target errors = 0 (sim not finished properly), then avg error is calculated assuming default err=0.15, which is
pretty low here! solution: increase default err
*** KrichmarEvol with bounded mutator (0.1545)
**** pop=100
0        100 0.30978758 0.16042155 0.20957367 0.21569889 0.03283971 (accidently deleted statistics.csv at gen=38!)
40       4100 0.29206774 0.15454827 0.19410129 0.20143793 0.02955027
46       4700 0.28441597 0.15454827 0.19902363 0.20592142 0.02894483
75       7600 0.29819550 0.15454827 0.20035971 0.21092550 0.03551816
*** evolutionStrategy (0.120, 0.149)
**** pop=2
0, 2, 0.29309852766, 0.14321209304, 0.21815531035, 0.21815531035, 0.0749432173103
50, 2, 0.120482750696, 0.120428230984, 0.12045549084, 0.12045549084, 2.72598558803e-05

97, 2, 0.120482750696, 0.120428230984, 0.12045549084, 0.12045549084, 2.72598558803e-05
**** pop=100
0, 100, 0.318741062513, 0.165325490508, 0.215863412366, 0.221879967525, 0.0326180996345
50, 100, 0.161099087304, 0.149440203003, 0.159304010727, 0.1585991366, 0.00234451905547

86, 100, 0.158996363073, 0.149440203003, 0.157320921931, 0.156541186796, 0.00215095191058
*** simulated annealing (pop=1)
Note: population size is fixed to 1
0, 1, 0.147006731409, 0.147006731409, 0.147006731409, 0.147006731409, 0.0
4, 1, 0.152100649955, 0.152100649955, 0.152100649955, 0.152100649955, 0.0
50 = 0.24 (bug and not being logged)
*** differential evolution (0.148, 0.149)
**** pop=2
Note: population size was 2
0, 2, 0.302776430427, 0.148740419821, 0.225758425124, 0.225758425124, 0.0770180053034
50, 2, 0.361121064674, 0.215502442399, 0.288311753537, 0.288311753537, 0.0728093111373
Changed num_selected to 100
0, 2, 0.302776430427, 0.148740419821, 0.225758425124, 0.225758425124, 0.0770180053034
35, 2, 0.378108681455, 0.190169327724, 0.28413900459, 0.28413900459, 0.0939696768656
**** pop=100
0, 100, 0.315004100446, 0.152035007756, 0.2148703177, 0.220430208369, 0.0354757774148
30, 100, 0.187584364802, 0.149097688801, 0.170677041856, 0.170903622147, 0.00694698013731
50, 100, 0.19157735916, 0.157253114409, 0.172914206251, 0.172316661561, 0.00703620612567
*** Estimation of Distribution (0.144, 0.157)
**** pop=2
0, 2, 0.291259623486, 0.144390895694, 0.21782525959, 0.21782525959, 0.0734343638961
35, 2, 0.16158109519, 0.144390895694, 0.152985995442, 0.152985995442, 0.00859509974791
**** pop=100 
0, 100, 0.323931911202, 0.164463760049, 0.213415702307, 0.222954408408, 0.0385806163163
50, 100, 0.21248057311, 0.157877607753, 0.191605110845, 0.192939702229, 0.00969933129319

75, 100, 0.206858913006, 0.157649267327, 0.192276207519, 0.191750368958, 0.0101480708231
*** particle swarm optimization (0.125,0.146)
**** pop=2
0, 2, 0.310104614953, 0.149800351062, 0.229952483008, 0.229952483008, 0.0801521319456
48, 2, 0.265234153844, 0.125961663225, 0.195597908535, 0.195597908535, 0.0696362453097
200, 2, 0.298458511621, 0.143756170165, 0.221107340893, 0.221107340893, 0.0773511707283
**** pop=100
0, 100, 0.328178005284, 0.173645174581, 0.21452193834, 0.225958888439, 0.0354591936435
50, 100, 0.258803421996, 0.159739990011, 0.175131262589, 0.176260967971, 0.0117608002761
107, 100, 0.207770227251, 0.146541079278, 0.173381643313, 0.17386918623, 0.0109426519273
141, 100, 0.193864849154, 0.151181402993, 0.172267220823, 0.172158496788, 0.0100510580599

**** BUG - fixed
cand and target were wrong way around in parallel_eval (polling from file)

*** ant colony optimization (requires components)
requires initialization of 'components' - not clear what this means for our problem

* 15mar14 Learning to reach 2 targets
** Check best results for single target
- need to be able to reach 2 different targets by training individually, before can move to learning 2 simult targets
- found bug that changed output (ER5 inputs, despite PMd not spiking! -- actually spikes every 10 sec due to NSLOC sync):
        if s.cellnames[gid] == 'ER5': # PMd->ER5 conn (full conn)
            PMdId = (gid % s.server.numPMd) + s.ncells - s.server.numPMd #CHECK THIS!
            allconnprobs[PMdId] = s.connprobs[s.PMd,s.ER5] # to make this connected to ER5
            allrands[PMdId] = 0 # to make this connect to ER5
            distances[PMdId] = 300 # to make delay 5 in conndata[3] 
- looked into target 0 and target 1 -- seems more random than real learning of sensorimotor mapping - very sensitive to
  parameter variations
- Try 1-second single trials 
- Maybe strengthen weights
- One problem seems to be oscillatory nature of cortex and how to transform to spinal cord rate code 
-- Motoneuron paper shows how neuron keeps spiking after short current input 
-- testing izhi cell params using izhikevich.m -- difficult to tune
-- trying EDSC = LTS cell -- higher firing rate (faster movs), but not sure if sustained without input -- still see
-- oscillations
- check RL - weights EB5->EDSC+IDSC being increased even when arm is always moving away? why? (IDSC more increase -- probably
  due to more firing)
-- maybe due to error = accumulative over several timesteps, so when reset seems like error improved
-- maybe due to simple STDP
-- NOTE: initialArmMov not preventing arm from moving after 1st trial; also not preventing inclusion in error calculation
  (this last one makes sense) -- fixed
- Now weights being decreased but very homogeneously becasue noise is activating all muscles similarly -- try explor movs
- Trying noise to EDSC+IDSC population - better, although its curious that despite similar reward+punish, the weights show
  drastic increase
-- if enforce sh ext (away from target so punish), EB5->IDSC weights decrease more for elbow flexor neurons, which haven't almost
spiked! - check why!
-- weight EB5->IDSC was too high, so high firing rate during short reward periods counterbalanced punishment
- found bug -- explor movs weren't using explor mov rate

- rerun sim with changes (15mar16) -- now using results from gen_84_cand_1 - 0.099 (right) and 0.11 (left):
    s.targetid=0
    s.trainTime=120000.0
    s.plastConnsType=1.0
    s.RLfactor=5
    s.eligwin=85.63069916781706
    s.backgroundrate=51.6480997331155
    s.backgroundrateExplor=596.3726207812871
    s.cmdmaxrate=10
- changeset:
changeset:   168:b7f777eb11a7
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Tue Mar 17 01:21:41 2015 -0400
summary:     testing learning 2 targets simult using best results for single target - 14mar06_evol gen_84_cand_1

- Changed s.plastConnsType=3.0 to 1.0 and got similar error: 0.102

- Testing new set of params with slightly lower error:
gen=138
cand=93, target=0: error = 0.087229
cand=93, target=1: error = 0.115298
avg error: 0.101263 
 
  Setting targetid=0
  Setting trainTime=120000.0
  Setting plastConnsType=3.0
  Setting RLfactor=5
  Setting eligwin=63.36861356899377
  Setting backgroundrate=93.45211273720386
  Setting backgroundrateExplor=624.4772397851636
  Setting cmdmaxrate=13.107045956431962

*** checking 15mar04_evol_evolutionStrategy_2/gen_29_cand_76 (error: 0.149)
  Setting outfilestem=../data/15mar04_evol_evolutionStrategy_2/gen_29_cand_76
  Setting targetid=0
  Setting trainTime=120000.0
  Setting plastConnsType=3.0
  Setting RLfactor=3.252843536382365
  Setting eligwin=112.83953280360103
  Setting backgroundrate=76.78980652066235
  Setting backgroundrateExplor=1567.712912493038
  Setting cmdmaxrate=25.70078356927193
** connectivity
*** Split PMd inputs into 2 subpopulations, each of which only get activated for one target (actually project to half of target pop cells!)
- simplest method
- missing the point here! whats the conn to target pop? each PMd subpop should project only to subset of ER5
- eg. PMds 0-47 -> ER5 0-47 ; PMds 48-95 -> ER5 48-95
- code:
        elif s.PMdinput == 'targetSplit': # PMds 0-47 -> ER5 0-47 ; PMds 48-95 -> ER5 48-95
            if s.cellnames[gid] == 'ER5':
                prePMd = gid - s.popGidStart[s.ER5] + s.popGidStart[s.PMd]
                allconnprobs[prePMd] = 1
*** Different input spiking patterns for each target

** training
*** exploratory movements with time intervals to alternate each target
- extreme case is half the time to one target, and rest change to other target
- Change intervals can vary in duration - eg. 1 sec, 10 sec, 30 sec etc
*** single reaching trials to each target 
-  required when using PMd data
- implemented by restarting arm from center every trial and reseting arm variables (not sure what I'll do with virtual arm!)
** plasticity
added 2 relevant plastic connections: 
elif s.plastConnsType == 4:
        s.plastConns = [[s.ASC,s.ER2], [s.EB5,s.EDSC], [s.EB5,s.IDSC], [s.PMd,s.ER5], [s.ER5,s.EB5]] 
** Manual parameter tuning
- first trial PMd input - ok
- with 120 sec, some periods no input - ok
- maxPMdrate variables - ok
- reduce minNoise so no sync spikes every 10 sec! or increase noisefraction - ok 
- increase noise fraction so spikes not synced - ok
- check PMD->ER5 connections -- seems similar acitivty for both targets -ok 
- during testing have PMd input corresponding to target - ok
- increase ER5 input -- now input from 2 PMds
*** only learning target=0 (0.10), but target=1 (0.22) 
-- try reverse training (60 trials target 1, then 60 trials target 0) - no effect (0.10,0.22)
-- try training just target 1 to check its making a difference - no! (0.11, 0.22) - checking why (only diff is sync input
 from PMd->ER5)  
--- if get backgroundrate back to 50 (from 40), and plasttype=1, get (0.15, 017) -- this indicates original solution very unstable
--- if set backrate to 51, and plasttype=3, and connweights[PMd,ER5,AMPA]=0.0, maxPMdRate=0, get (0.153, 0.159)
--- plasttype=1 -> (0.149, 0.143)
--- cmdmaxrate=20 (from 10) -> (0.14, 0.16)
-- try increasing range of target ER5 (currently just 96 in total) -- excite all?
-- try projecting to layer E5B
*** trying new set of params from gen138_cand93 (0.08, 0.11) - - manually tuning
- target1= 0.156381221575 -- big difference from 0.11 !! 
- target0= 0.10 -- different from 0.08
-- set PMdinput = none and PMd conn and weight as original --> PMd connectivity and rates should go back to original
-- ok!! got original values:  targets=(0.087228673029, 0.115297868634)
-- 30 sec with and w/o PMd (target1) = (0.143, 0.146)
-- 60 sec with and w/o PMd (target1) = (0.152, 0.154)
-- 90 sec with and w/o PMd (target1) = (0.14314928838,0.152573779435)
-- 120 sec with and w/o PMd (target1) = (0.115297868634,0.18198232555):
--- without PMd  Spikes: 9624 (9.05 Hz),  Connections: 16635 (0 STDP; 15.63 per cell)
--- with PMd   Spikes: 9248 (8.69 Hz)  Connections: 17288 (0 STDP; 16.25 per cell)
--- Interestingly PMd input lead to less spikes (maybe PMd->ER5->EB5->IDSC) - which shows how even small variation can lead
 to big behavioral changes (actually opposite directions!)
- difference is always due to weight changes - compare fig below: right (with PMd, good result), left (without PMd, bad
  result), bottom (EB5->EDSC weight changeS), top (EB5->IDSC weight changes)
[[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150318_171829.png][fig]] - cweight comparison
- Conclusion:
1) EB5->EDSC don't get enough weight change (spikes within elig trace and stdp win) to overcome the initial bias (shflex
   increases but not enough) -- maybe due to proprioception not modulating acvity enough to distinguish position of hand
   relative to target; so contradictory info reaches synapses, ie reward and punish for same movement direction=same muscles
   (towards target=reward, and after surpass target, away from target=punishment)
2) EB5->IDSC increases weights of wrong muscle group (ext vs flex) -- maybe due to too high firing rates of IDSC, difficult
   to assign responsability of reward/punish to subset of cells
- Possible solutions:
-- Set s.trainTime=20000.0 so can test quickly
-- increse s.cmdmaxrate=50 (from 20) -- so arm doesnt go past target (smaller movements) - not enough
-- decrease explor movs amplitude -- so arm doesnt go past target: - explor movs still past target + ext weight>flex (0.148)
    s.backgroundweightExplor = 2.0 
    s.backgroundrateExplor = 500
-- s.connweights[s.IDSC,s.EDSC,s.GABAA]=0.5 - explor movs still past target + ext weight>flex; more spikes becasue less Inh (0.174)
--  s.cmdmaxrate=80 #13.107045956431962 - explor movs dont reach target! good ... but extW>flexW; (0.162 - small mov)
    s.backgroundweightExplor = 1.5 
    s.backgroundrateExplor = 200
-- try s.plastConnsType=1.0 - same
problem is that explor movs dont activate muscles differentially -- all muscles activated very similarly, so difficult to
 reinforce specific one
--     s.backgroundrate=50 (lowered from 97) -- still no distinct activation of submotor pops and extW>flexW (0.158 - small mov)
    s.backgroundweightExplor = 1.5  
    s.backgroundrateExplor = 400 (increased from 200)
-- s.backgroundweight = 1.0*array([1,0.1]) (lowered from 2.0) - similar
-- s.scaleconnweight = 4*array([[1, 1], [1, 0.1]]) # Connection weights for EE, EI, IE, II synapses, respectively - similar
--    s.eligwin=50 (reduced) - similar extW>flexW
    s.cmdmaxrate=100 (increased)
    s.backgroundrate=30 (reduced)
    s.backgroundweight = 1.0*array([1,0.1]) # Weight for background input for E cells and I cells
    s.backgroundweightExplor = 2 (increased)
    s.backgroundrateExplor = 400
    s.connweights[s.IDSC,s.EDSC,s.GABAA]=0.5 
    s.scaleconnweight = 4*array([[2, 1], [2, 0.1]]) (back to original)
--     s.scaleconnweight = 1*array([[2, 1], [2, 0.1]]) (reduced) -- aha! this worked: no Inh spikes in cortical layers; but
 flexW>extW; EDSC+IDSC clearly dominated by explor movs -- (0.14 - small mov due to low noise) -- would require more
 backgroudn noise during testing
-- s.scaleconnweight = 1.5*array([[2, 1], [2, 0.1]]) (half way) = 0.146942480777 - still no I spikes
-- s.scaleconnweight = 1.0*array([[2, 2], [2, 0.5]]) (increase ->I W ratio) -0.134387178083 still no I spikes 
--s.scaleconnweight = 1.5*array([[2, 2], [2, 0.5]]) = 0.148  -- I spikes in L5 but not L2 or L6
-- s.scaleconnweight = 1.5*array([[2, 2], [2, 0.2]]) - similar
-- s.scaleconnweight = 1.5*array([[2, 1.5], [2, 0.1]])  
--     s.scaleconnweight = 1.8*array([[2, 2], [2, 0.1]])  - yey! 0.138910395922, I spikes + flexW>extW
-- test phase: s.backgroundrate=100 - small movement! BUG - noise not changing
-- test phase: s.backgroundrate=400 - 0.138 (oscillations back during testing - no diff of EDSC output - all get equally
 excited, despite flex increased W )
-- test phase: s.backgroundrate=250 - 0.133 (nice straight line)
-- train 40 sec instead 20: test phase: s.backgroundrate=300 0.137194853422 (motor commands look good)
-- try s.scaleconnweight = 1.9*array([[2, 2], [2, 0.1]])  because not enough I spikes -- increase L5 I spikes but not L2
-- tested target 0 and worked ok (straight line to target), but need higher amplitude of movement
-- if reduce cmdmaxrate to 25 - great result (target1= 0.103418745252, target0= 0.114322980568) -- both straight trajectory 
-- if 15 -> target0= 0.0946032275338

*** got improved result (see above) - further manual tuning
- THINGS TO TRY NEXT:
-- something ot increase L2 inh spikes - not priority since likely to change 
-- longer training times to increase weight difference -> mov amplitude - can tune via evol
-- increasing EB5 inputs during testing without increasing overall background rate - eg. PMd inputs? only background EB5
 netstims?
-- something to avoid oscillations in EDSC?

-s.scaleconnweight = 1.9*array([[2, 2], [2, 0.1]]) - target0 = 0.0853974505576 , target1=  0.110104162377  still missing I in
layer 2)
- s.scaleconnweight = 1.9*array([[2, 1], [2, 0.1]]) - target0=  0.109734844284
- s.scaleconnweight = 2.0*array([[2, 2], [2, 0.1]]) - targets= 0.99, 0.94 - with a loops but ok
changeset:   176:9819e4540031
tag:         tip
user:        Salvador Dura <salvadordura@gmail.com>
date:        Thu Mar 19 13:07:33 2015 -0400
summary:     manually tuned good params for training to 2 individual targets

- maxPrate = 100 targets=(0.12, 0.10) 
- PMdinput = 'targetSplit' - only a few PMd spikes (maxPMdRate = 0.1) but still target1=0.13923396797, due to more noise, less clear diff of mus
  act during explor -- maybe requies more time?
- maxPMdRate = 50 - 0.138885258858 (similar)
- s.backgroundweight = 1.5*array([1,0.3]) - targets= 0.109571057654, 0.0923334714042
- s.backgroundweight = 1.0*array([1,0.3]) -  0.097377773176
- TODO currently requires cmdmaxrate different during training (100) and testing (15) - justified as gain control mech? good if
  can find alternative solution using rates/weights

*** testing 2 target simult with new params
-     s.trainTime=40000.0
    numTrials = ceil(s.trainTime/1000)
    s.trialTargets = [0 if i < numTrials/2 else 1 for i in range(int(numTrials+1))] # set target for each trial
    s.targetid=s.trialTargets[0]
    s.targetPMdInputs = [[i for i in range(s.popGidStart[s.PMd], int(s.popGidEnd[s.PMd]/2)+1)], [i for i in range(int(s.popGidEnd[s.PMd]/2)+1, s.popGidEnd[s.PMd]+1)]]

    # gen_138_cand_93
    s.plastConnsType=1.0
    s.RLfactor=5
    s.eligwin=50
    s.cmdmaxrate=100 
    s.backgroundrate=30
    s.backgroundweight = 1.0*array([1,0.1]) # Weight for background input for E cells and I cells
    s.backgroundweightExplor = 2
    s.backgroundrateExplor = 400
    s.connweights[s.IDSC,s.EDSC,s.GABAA]=0.5 
    s.scaleconnweight = 2.0*array([[2, 2], [2, 0.1]]) # Connection weights for EE, EI, IE, II synapses, respectively
    s.backgroundweight = 1.5*array([1,0.3])
    s.maxPMdRate = 50

    # test
    s.backgroundrate=300
    s.cmdmaxrate=15
    addBackground()
    s.usestdp = 0 # Whether or not to use STDP
    s.useRL = 0 # Where or not to use RL
    s.explorMovs = 0 # disable exploratory movements
    s.duration = s.testTime # testing time
    s.armMinimalSave = 0 # save only arm related data

- train seq = 0,1 ; test seq = 0,1 --> 0.115844295748, 0.188942301951 (results always shown as target0,target1)
- train seq = 0,1 ; test seq = 1,0 --> 0.117981198836,  0.184840979175
- train seq = 1,0 ; test seq = 0,1 --> 0.227255204128,0.0878047609185
- trainseq=0,1; traintime=120 -->0.112026315795,0.190797769314
- trainseq=0,1; traintime=120 ,s.maxPMdRate = 100 --> 0.13295838616,0.157784587263
- s.maxPMdRate = 25 --> 0.125935859475,0.184367942309
- s.maxPMdRate=100; PMd->ER5 now convers all 192 ER5 cells: --> 0.134127466404,0.160127195212
            if gid < s.popGidStart[s.ER5] + s.popnumbers[s.ER5]/2:
                prePMd = [(x - s.popGidStart[s.ER5])%(s.popnumbers[s.PMd]/2) + s.popGidStart[s.PMd] for x in range(gid, gid+2)] # input from 2 PMds  
            else:
                prePMd = [(x - s.popGidStart[s.ER5])%(s.popnumbers[s.PMd]/2) + s.popGidStart[s.PMd] + s.popnumbers[s.PMd]/2 for x in range(gid, gid+2)] # input from 2 PMds  
            if array(prePMd).all() < s.popGidEnd[s.PMd]: 
- no visible difference in ER5 during testing! - difference not so visible due to increased background noise during testing
- s.connweights[s.PMd,s.ER5,s.AMPA]=2.0 (from 1.0) --> 0.125592312444, 0.179047145048
- check still working with 1 target at a time! (train to target0 only) 0.124168093, 0.177591768439 - ~ok
- s.connweights[s.PMd,s.ER5,s.AMPA]=3.0 -> 0.128091173195,0.168522263229
- s.connweights[s.PMd,s.ER5,s.AMPA]=4.0 -> 0.112962290221,0.168414842114
- s.connweights[s.PMd,s.ER5,s.AMPA]=5.0 -> 0.109558881166,0.184846433745
- s.connweights[s.PMd,s.ER5,s.AMPA]=6.0 -> 0.115029110223,0.196152691726
-prePMd = [(x - s.popGidStart[s.ER5])%(s.popnumbers[s.PMd]/2) + s.popGidStart[s.PMd] for x in range(gid, gid+1)] (instead of
gid+2) - (0.135846528302,0.161628997419)
- s.maxPMdRate = 200 - 0.117764933802, 0.162524442976
- s.maxPMdRate = 400 
- add plasticity to ER5->ERB ! -0.106916823513, 0.192076829175 
- test alternating trials - 0.129370591289,0.161842442536 (good, almost 2 diff directions) 
- alternating without plasticity -  0.13223740198, 0.171596354519 (worse, both same direction, so keep plasticity
- s.maxPMdRate = back to 200 - 0.151714363559, 0.138038425351 (changed direction, now both to left)
- alternating+plasticity+maxPMdrate=200 -> 0.108899870345,0.195044311131
- alternating+plasticity+maxPMdrate=400 -> 0.129370591289,0.161842442536
- alternating+plasticity+maxPMdrate=600 -> 0.135180421073, 0.17617637466

** Things to try
- smaller subsets of ER5
- plastic connection types 1,2,3 + ER5->EB5
- Targeting EB5 directly (last option)
* 15mar16 sim -  2 targets (single), 1k neurons, dummyArm - after manually tuning params :results:
** changesets
** description
manually tuned and debugged network eg. trial-based training -- see 15mar14
explor moves still via EDSC+IDSC
also reduced range of params
** results
*** statistics.csv
ma% cat ../data/15mar16_evol_evolutionStrategy/statistics.csv 
0, 100, 0.176963875722, 0.116718537778, 0.14538691846, 0.146622693233, 0.0128583220942
1, 100, 0.143578569431, 0.116718537778, 0.136109088388, 0.135454521953, 0.00565621877033
2, 100, 0.139134658938, 0.116718537778, 0.134485577954, 0.133256158501, 0.00455750067942
3, 100, 0.136036500964, 0.116718537778, 0.132684009918, 0.131558523715, 0.00404076713918
4, 100, 0.134274981323, 0.108672751281, 0.13017345457, 0.129104557465, 0.00485405349786
5, 100, 0.132949163468, 0.108672751281, 0.129997369457, 0.128342049066, 0.0046838426237
6, 100, 0.13169182863, 0.108672751281, 0.129061226186, 0.127489558345, 0.00457934821354
7, 100, 0.131076215876, 0.108672751281, 0.127908694355, 0.12671826366, 0.00440425237416
8, 100, 0.130346787854, 0.108672751281, 0.127399814037, 0.12615971791, 0.00421416043083
9, 100, 0.130036781773, 0.108672751281, 0.126871642041, 0.125711055459, 0.00403306786678
10, 100, 0.129072643501, 0.108672751281, 0.126214681751, 0.125181437834, 0.00374461291318
11, 100, 0.128361414056, 0.108672751281, 0.125581670302, 0.12461187501, 0.00357259116235
12, 100, 0.127788269072, 0.108672751281, 0.125100905144, 0.124280181777, 0.00342221826601
13, 100, 0.127669643594, 0.108672751281, 0.12497639154, 0.124191018487, 0.00337331244597
14, 100, 0.127347369304, 0.108672751281, 0.124829590035, 0.123859846712, 0.00336756337617
15, 100, 0.127027839677, 0.108672751281, 0.1245319348, 0.123680589879, 0.00329483465446
16, 100, 0.126774960676, 0.108672751281, 0.124406896539, 0.123520668647, 0.00324953499721
17, 100, 0.12636682141, 0.108672751281, 0.124201808529, 0.123230006141, 0.00316315698393
18, 100, 0.126053539603, 0.108672751281, 0.124009067785, 0.122953044022, 0.00308204724199
19, 100, 0.125792707272, 0.108672751281, 0.123754796009, 0.122803911359, 0.00303007047271
20, 100, 0.125325153253, 0.108672751281, 0.123546376583, 0.122265095538, 0.00334180891989
21, 100, 0.125059085287, 0.108672751281, 0.123048231432, 0.121862960863, 0.0033390657351
22, 100, 0.1249866552, 0.108672751281, 0.12279197775, 0.121748969537, 0.00330310353844
23, 100, 0.12483042925, 0.108672751281, 0.122509578959, 0.12156618168, 0.00322480425647
24, 100, 0.124515672461, 0.108672751281, 0.122239487715, 0.121381509667, 0.00317199885099
25, 100, 0.124462584019, 0.108672751281, 0.122088568645, 0.121206228897, 0.00314026390627
26, 100, 0.124215841504, 0.108672751281, 0.121869899216, 0.121038872762, 0.00305616193097
27, 100, 0.124185470998, 0.108672751281, 0.121869899216, 0.121024108314, 0.00304432164408
28, 100, 0.123750579736, 0.108672751281, 0.121512928506, 0.120647421552, 0.00295331037898
29, 100, 0.123664170178, 0.108672751281, 0.12142524888, 0.120597366323, 0.002921841969
30, 100, 0.123574377046, 0.108672751281, 0.12142524888, 0.120572988784, 0.0029022889093
31, 100, 0.123258110527, 0.108672751281, 0.121068029081, 0.120146765671, 0.00297370016909
32, 100, 0.123051155635, 0.108672751281, 0.120990219759, 0.120019773519, 0.00292048410344
33, 100, 0.122869777015, 0.108672751281, 0.120870000709, 0.119886994591, 0.00288989213732
34, 100, 0.122795030425, 0.108672751281, 0.120614957877, 0.119820906391, 0.00285945430656
35, 100, 0.122714178485, 0.108672751281, 0.120510440006, 0.119718952784, 0.00286310544206
36, 100, 0.122576358291, 0.108672751281, 0.120354126866, 0.119599215604, 0.0028235709229
37, 100, 0.12256845223, 0.108672751281, 0.120327879465, 0.119564951514, 0.00280798207719
38, 100, 0.122344565333, 0.108672751281, 0.120225863785, 0.11934124665, 0.00284176835735
39, 100, 0.12205391927, 0.108672751281, 0.119935906835, 0.118999317781, 0.00298748381566
40, 100, 0.121934293249, 0.108672751281, 0.11977142406, 0.118862983395, 0.00300246844317
41, 100, 0.121913442647, 0.108672751281, 0.119698022009, 0.11883134577, 0.00298656522542
42, 100, 0.121913037538, 0.108672751281, 0.119698022009, 0.118830615577, 0.00298582042052
43, 100, 0.121913037538, 0.108672751281, 0.119698022009, 0.118830615577, 0.00298582042052
44, 100, 0.121833546956, 0.108672751281, 0.119363073621, 0.118706997509, 0.00294532637112
45, 100, 0.121750232514, 0.108672751281, 0.119250140854, 0.118573593224, 0.00297198605549
46, 100, 0.12167441496, 0.108672751281, 0.119250140854, 0.118552290795, 0.00295673576801
47, 100, 0.121615628812, 0.108672751281, 0.119123380106, 0.118492284202, 0.00294074877717
48, 100, 0.121595269055, 0.108672751281, 0.11908529159, 0.118460553865, 0.00292394740558
49, 100, 0.121505966664, 0.108672751281, 0.11908529159, 0.118411884974, 0.00288671701611
50, 100, 0.121450456166, 0.107769584694, 0.119064201253, 0.118274521155, 0.0030579598509
51, 100, 0.121382458743, 0.107769584694, 0.119047977375, 0.118157154968, 0.00312807172997
52, 100, 0.121344531096, 0.107769584694, 0.118981769621, 0.118125893126, 0.00311125824188
53, 100, 0.121344531096, 0.107769584694, 0.118981769621, 0.118125893126, 0.00311125824188
54, 100, 0.121256065748, 0.107769584694, 0.118898466436, 0.118016895949, 0.00318660336623
55, 100, 0.121233755027, 0.107769584694, 0.118898466436, 0.118010109855, 0.00318041466603
56, 100, 0.121164025983, 0.107769584694, 0.118822624387, 0.117923512161, 0.00315323421445
57, 100, 0.121111924707, 0.107769584694, 0.118822624387, 0.11790419937, 0.00313760672896
58, 100, 0.120980889512, 0.107769584694, 0.118651256864, 0.117648617507, 0.0031510279193
59, 100, 0.120980889512, 0.107769584694, 0.118651256864, 0.117648617507, 0.0031510279193
60, 100, 0.120905551676, 0.107769584694, 0.118586157134, 0.11757557502, 0.00312180598242
61, 100, 0.120905551676, 0.107769584694, 0.118586157134, 0.11757557502, 0.00312180598242
62, 100, 0.120831569486, 0.107769584694, 0.118523840936, 0.11752957466, 0.00309081277906
63, 100, 0.120815163298, 0.107769584694, 0.118467914868, 0.11748634411, 0.00307451600987
64, 100, 0.12057745634, 0.107769584694, 0.118416742289, 0.117402700388, 0.00302331826735
65, 100, 0.120455971637, 0.107769584694, 0.118221333992, 0.117258653568, 0.00299676144091
66, 100, 0.120379711508, 0.107769584694, 0.118221333992, 0.117252199572, 0.00299055712703
67, 100, 0.120341450512, 0.107769584694, 0.118127142377, 0.117171903739, 0.00296588791428
68, 100, 0.120251308634, 0.107769584694, 0.118003210888, 0.117098659189, 0.00292614508422
69, 100, 0.12017699238, 0.107769584694, 0.117969676563, 0.116999240772, 0.00290518744528
70, 100, 0.12017699238, 0.107769584694, 0.117969676563, 0.116999240772, 0.00290518744528
71, 100, 0.120018935477, 0.107769584694, 0.117969676563, 0.116965875669, 0.00287719603268
72, 100, 0.119897929151, 0.107769584694, 0.117942160892, 0.116933121737, 0.00285560847551
73, 100, 0.119897929151, 0.107769584694, 0.117942160892, 0.116933121737, 0.00285560847551
74, 100, 0.119897929151, 0.107769584694, 0.117942160892, 0.116933121737, 0.00285560847551
75, 100, 0.119874450185, 0.107769584694, 0.117805506286, 0.116836861478, 0.00285470387114
76, 100, 0.119794935445, 0.107769584694, 0.117503889411, 0.11674759826, 0.00284285813908
77, 100, 0.119794935445, 0.107769584694, 0.117503889411, 0.11674759826, 0.00284285813908
78, 100, 0.119791378281, 0.107769584694, 0.117503889411, 0.116732570088, 0.00283065528337
79, 100, 0.119778699764, 0.107769584694, 0.117419131674, 0.116701240056, 0.00281391537466
80, 100, 0.119619989568, 0.107769584694, 0.117328743721, 0.116593276311, 0.00275590768208
81, 100, 0.119619989568, 0.107769584694, 0.117328743721, 0.116593276311, 0.00275590768208
82, 100, 0.119545781079, 0.107769584694, 0.11723343635, 0.116489310049, 0.0027393321351
83, 100, 0.119545781079, 0.107769584694, 0.11723343635, 0.116489310049, 0.0027393321351
84, 100, 0.119436420549, 0.105578687974, 0.117208828271, 0.116283160407, 0.00292205283141
85, 100, 0.119316210526, 0.105578687974, 0.117154652924, 0.116191263392, 0.00288998747436
86, 100, 0.119149949222, 0.105578687974, 0.117059859923, 0.116024213083, 0.00295013200077
87, 100, 0.119096810989, 0.105578687974, 0.11700065062, 0.115998206206, 0.00293388086997
88, 100, 0.119073772191, 0.105578687974, 0.116933993336, 0.115929179638, 0.00294135800127
89, 100, 0.118922214807, 0.105578687974, 0.116723108402, 0.115757326874, 0.00292734262841
90, 100, 0.118874718066, 0.105578687974, 0.116684475229, 0.115666468542, 0.00290041875041
91, 100, 0.118874718066, 0.105578687974, 0.116684475229, 0.115666468542, 0.00290041875041
92, 100, 0.118770530708, 0.105578687974, 0.116684475229, 0.115651273059, 0.00288752255257
93, 100, 0.11873983224, 0.105578687974, 0.116670416954, 0.115583362797, 0.00286067310702
94, 100, 0.11873983224, 0.105578687974, 0.116670416954, 0.115583362797, 0.00286067310702
95, 100, 0.11873983224, 0.105578687974, 0.116670416954, 0.115583362797, 0.00286067310702
96, 100, 0.118684646799, 0.105578687974, 0.116587694022, 0.115539012161, 0.00282879645003
97, 100, 0.118684646799, 0.105578687974, 0.116587694022, 0.115539012161, 0.00282879645003
98, 100, 0.11864898842, 0.105578687974, 0.116528888027, 0.115501629252, 0.00281162837451
99, 100, 0.11855444734, 0.105578687974, 0.116405716116, 0.11544648462, 0.00277653156614
100, 100, 0.11840659073, 0.105578687974, 0.11637135747, 0.115303247856, 0.00275581392444
101, 100, 0.118327013282, 0.105578687974, 0.11637135747, 0.115301116299, 0.00275349419704
102, 100, 0.118327013282, 0.105578687974, 0.11637135747, 0.115301116299, 0.00275349419704
103, 100, 0.118327013282, 0.105578687974, 0.11637135747, 0.115301116299, 0.00275349419704
104, 100, 0.118327013282, 0.105578687974, 0.11637135747, 0.115301116299, 0.00275349419704
105, 100, 0.118327013282, 0.105578687974, 0.11637135747, 0.115301116299, 0.00275349419704
106, 100, 0.118327013282, 0.105578687974, 0.11637135747, 0.115301116299, 0.00275349419704
107, 100, 0.118292118235, 0.105578687974, 0.116326206765, 0.115253917814, 0.00272173756632
108, 100, 0.118292118235, 0.105578687974, 0.116326206765, 0.115253917814, 0.00272173756632
109, 100, 0.118193435055, 0.105578687974, 0.11627345621, 0.115219058184, 0.00270109313634
110, 100, 0.118186393388, 0.105578687974, 0.116263267338, 0.115200150017, 0.00268678589779
111, 100, 0.118117139613, 0.105578687974, 0.11606150583, 0.115087723788, 0.00267907439948
112, 100, 0.118085317138, 0.105578687974, 0.11606150583, 0.115086652615, 0.00267788408601
113, 100, 0.118021608135, 0.105578687974, 0.11606150583, 0.115083531725, 0.00267454959898
114, 100, 0.118021608135, 0.105578687974, 0.11606150583, 0.115083531725, 0.00267454959898
115, 100, 0.118010022369, 0.105578687974, 0.115983306267, 0.115014451911, 0.00266674379447
116, 100, 0.117984813641, 0.105578687974, 0.115954469505, 0.114941112978, 0.0026841423009
117, 100, 0.117984813641, 0.105578687974, 0.115954469505, 0.114941112978, 0.0026841423009
118, 100, 0.117974227268, 0.105578687974, 0.115954469505, 0.114940113694, 0.00268302733974
119, 100, 0.117965125858, 0.105578687974, 0.115954469505, 0.114931210819, 0.0026744079524
120, 100, 0.117954584547, 0.105578687974, 0.115954469505, 0.114919454947, 0.00266360791258
121, 100, 0.117919195925, 0.105578687974, 0.115855614029, 0.114835238213, 0.00269867739398
122, 100, 0.117867767531, 0.105578687974, 0.115766888959, 0.114690125034, 0.00280617056256
123, 100, 0.117867767531, 0.105578687974, 0.115766888959, 0.114690125034, 0.00280617056256
124, 100, 0.117792569225, 0.105578687974, 0.115766888959, 0.114682691453, 0.0027987177875
125, 100, 0.117743245042, 0.105578687974, 0.115766888959, 0.114673180838, 0.00278973516401
126, 100, 0.117545359423, 0.105074234395, 0.11560237414, 0.114468253185, 0.00295365073721
127, 100, 0.117416783424, 0.105074234395, 0.11560237414, 0.114443418001, 0.00293334247575
128, 100, 0.117362886734, 0.105074234395, 0.11560237414, 0.114428407114, 0.0029210120849
129, 100, 0.11735516972, 0.105074234395, 0.11560237414, 0.114419664275, 0.00291288331781
130, 100, 0.117300829955, 0.105074234395, 0.115502560071, 0.114373277739, 0.00289489218276
131, 100, 0.117300829955, 0.105074234395, 0.115502560071, 0.114373277739, 0.00289489218276
132, 100, 0.117300829955, 0.105074234395, 0.115502560071, 0.114373277739, 0.00289489218276
133, 100, 0.117240181271, 0.105074234395, 0.1154262887, 0.114347074345, 0.00288009588545
134, 100, 0.117240181271, 0.105074234395, 0.1154262887, 0.114347074345, 0.00288009588545
135, 100, 0.117240181271, 0.105074234395, 0.1154262887, 0.114347074345, 0.00288009588545
136, 100, 0.117240181271, 0.105074234395, 0.1154262887, 0.114347074345, 0.00288009588545
137, 100, 0.11722669143, 0.105074234395, 0.115321445947, 0.114327116354, 0.00286686325521
138, 100, 0.11719272565, 0.101263270831, 0.115260395707, 0.114161443785, 0.00312772639033
139, 100, 0.11719272565, 0.101263270831, 0.115260395707, 0.114161443785, 0.00312772639033
140, 100, 0.11719272565, 0.101263270831, 0.115260395707, 0.114161443785, 0.00312772639033
141, 100, 0.11719272565, 0.101263270831, 0.115260395707, 0.114161443785, 0.00312772639033
142, 100, 0.11719272565, 0.101263270831, 0.115260395707, 0.114161443785, 0.00312772639033
143, 100, 0.11719272565, 0.101263270831, 0.115260395707, 0.114161443785, 0.00312772639033
144, 100, 0.117124409413, 0.101263270831, 0.115260395707, 0.114146475773, 0.00311500246813
145, 100, 0.117117376341, 0.101263270831, 0.115260395707, 0.114131043639, 0.00310401442685
146, 100, 0.117117376341, 0.101263270831, 0.115260395707, 0.114131043639, 0.00310401442685
147, 100, 0.117083939784, 0.101263270831, 0.115260395707, 0.114110971063, 0.0030884295493
148, 100, 0.117083939784, 0.101263270831, 0.115260395707, 0.114110971063, 0.0030884295493
149, 100, 0.117083939784, 0.101263270831, 0.115260395707, 0.114110971063, 0.0030884295493
150, 100, 0.117057048843, 0.101263270831, 0.115221822633, 0.114038725327, 0.00309273527715
151, 100, 0.117057048843, 0.101263270831, 0.115221822633, 0.114038725327, 0.00309273527715
152, 100, 0.116989448358, 0.101263270831, 0.115221822633, 0.114023887242, 0.00308175857577
153, 100, 0.116989448358, 0.101263270831, 0.115221822633, 0.114023887242, 0.00308175857577
154, 100, 0.116923734276, 0.101263270831, 0.115053502667, 0.11393877713, 0.00306086725219
155, 100, 0.116923734276, 0.101263270831, 0.115053502667, 0.11393877713, 0.00306086725219
156, 100, 0.116923734276, 0.101263270831, 0.115053502667, 0.11393877713, 0.00306086725219
157, 100, 0.116923734276, 0.101263270831, 0.115053502667, 0.11393877713, 0.00306086725219
158, 100, 0.116855812039, 0.101263270831, 0.114972414339, 0.113918057884, 0.00304757537099
159, 100, 0.116855812039, 0.101263270831, 0.114972414339, 0.113918057884, 0.00304757537099
160, 100, 0.116855812039, 0.101263270831, 0.114972414339, 0.113918057884, 0.00304757537099
161, 100, 0.116855812039, 0.101263270831, 0.114972414339, 0.113918057884, 0.00304757537099
162, 100, 0.116855812039, 0.101263270831, 0.114972414339, 0.113918057884, 0.00304757537099
163, 100, 0.11684150781, 0.101263270831, 0.114958749046, 0.113898225185, 0.00303481938712
164, 100, 0.11684150781, 0.101263270831, 0.114958749046, 0.113898225185, 0.00303481938712
165, 100, 0.11684150781, 0.101263270831, 0.114958749046, 0.113898225185, 0.00303481938712
166, 100, 0.11684150781, 0.101263270831, 0.114958749046, 0.113898225185, 0.00303481938712
167, 100, 0.116835705494, 0.101263270831, 0.114912315101, 0.113875240526, 0.003021113897
168, 100, 0.116809380469, 0.101263270831, 0.114912315101, 0.113861841848, 0.003010908414
169, 100, 0.116809380469, 0.101263270831, 0.114912315101, 0.113861841848, 0.003010908414
170, 100, 0.116789745001, 0.101263270831, 0.114912315101, 0.11385398806, 0.00300384330177
171, 100, 0.116789745001, 0.101263270831, 0.114912315101, 0.11385398806, 0.00300384330177
172, 100, 0.116789745001, 0.101263270831, 0.114912315101, 0.11385398806, 0.00300384330177
173, 100, 0.116789745001, 0.101263270831, 0.114912315101, 0.11385398806, 0.00300384330177
174, 100, 0.116789745001, 0.101263270831, 0.114912315101, 0.11385398806, 0.00300384330177
175, 100, 0.116727679026, 0.101263270831, 0.114875408241, 0.113821889399, 0.00298344930782
176, 100, 0.116727679026, 0.101263270831, 0.114875408241, 0.113821889399, 0.00298344930782
177, 100, 0.116727679026, 0.101263270831, 0.114875408241, 0.113821889399, 0.00298344930782
178, 100, 0.116727679026, 0.101263270831, 0.114875408241, 0.113821889399, 0.00298344930782
179, 100, 0.116727679026, 0.101263270831, 0.114875408241, 0.113821889399, 0.00298344930782
180, 100, 0.116718537778, 0.101263270831, 0.114862175913, 0.113781210125, 0.002971259746
181, 100, 0.116718537778, 0.101263270831, 0.114862175913, 0.113781210125, 0.002971259746
182, 100, 0.116686491622, 0.101263270831, 0.114862175913, 0.113776792593, 0.00296721501477
183, 100, 0.116682458836, 0.101263270831, 0.114766150143, 0.113728982443, 0.00295845126413
184, 100, 0.116682458836, 0.101263270831, 0.114766150143, 0.113728982443, 0.00295845126413
185, 100, 0.116658375071, 0.101263270831, 0.114623329125, 0.11369849295, 0.00294352937742
186, 100, 0.116658375071, 0.101263270831, 0.114623329125, 0.11369849295, 0.00294352937742
187, 100, 0.116626126489, 0.101263270831, 0.114623329125, 0.113683052868, 0.00293198989896
188, 100, 0.116626126489, 0.101263270831, 0.114623329125, 0.113683052868, 0.00293198989896
189, 100, 0.116626126489, 0.101263270831, 0.114623329125, 0.113683052868, 0.00293198989896
190, 100, 0.116621133237, 0.101263270831, 0.114623329125, 0.113672589413, 0.00292332246498
191, 100, 0.116621133237, 0.101263270831, 0.114623329125, 0.113672589413, 0.00292332246498
192, 100, 0.116621133237, 0.101263270831, 0.114623329125, 0.113672589413, 0.00292332246498
193, 100, 0.116614555108, 0.101263270831, 0.114554604826, 0.113648677954, 0.00290885033895
194, 100, 0.116549261554, 0.101263270831, 0.114486171687, 0.113574660663, 0.00287152295882
195, 100, 0.116538844408, 0.101263270831, 0.114486171687, 0.113570529189, 0.0028675346497
196, 100, 0.116538844408, 0.101263270831, 0.114486171687, 0.113570529189, 0.0028675346497
197, 100, 0.116538844408, 0.101263270831, 0.114486171687, 0.113570529189, 0.0028675346497
198, 100, 0.1165085145, 0.101263270831, 0.114486171687, 0.113556305811, 0.0028562815291
199, 100, 0.1165085145, 0.101263270831, 0.114486171687, 0.113556305811, 0.0028562815291
200, 100, 0.1165085145, 0.101263270831, 0.114486171687, 0.113556305811, 0.0028562815291
201, 100, 0.116400707649, 0.101263270831, 0.114417059103, 0.113505459761, 0.00282779057494
202, 100, 0.116367836184, 0.101263270831, 0.114417059103, 0.113488655167, 0.00281330455379
203, 100, 0.116367836184, 0.101263270831, 0.114417059103, 0.113488655167, 0.00281330455379
204, 100, 0.116315091027, 0.101263270831, 0.114261949072, 0.11332526524, 0.00284300424376
205, 100, 0.11630261833, 0.101263270831, 0.114077834673, 0.113216058403, 0.00293433833846
206, 100, 0.116298242986, 0.101263270831, 0.114077834673, 0.113206356781, 0.00292570850116
207, 100, 0.116298242986, 0.101263270831, 0.114077834673, 0.113206356781, 0.00292570850116
208, 100, 0.116298242986, 0.101263270831, 0.114077834673, 0.113206356781, 0.00292570850116
209, 100, 0.116276784561, 0.101263270831, 0.11397589637, 0.113183635459, 0.00291039109714
210, 100, 0.116223916346, 0.101263270831, 0.11397589637, 0.113175997611, 0.00290298876526
211, 100, 0.116223916346, 0.101263270831, 0.11397589637, 0.113175997611, 0.00290298876526
212, 100, 0.116185077929, 0.101263270831, 0.11397589637, 0.113168236253, 0.00289585832217
213, 100, 0.116136114115, 0.101263270831, 0.113921796312, 0.113107343822, 0.00289580225523
214, 100, 0.116136114115, 0.101263270831, 0.113921796312, 0.113107343822, 0.00289580225523
215, 100, 0.116136114115, 0.101263270831, 0.113921796312, 0.113107343822, 0.00289580225523
216, 100, 0.116129253749, 0.101263270831, 0.113921796312, 0.113096750763, 0.00288662638634
217, 100, 0.116129253749, 0.101263270831, 0.113921796312, 0.113096750763, 0.00288662638634
218, 100, 0.116129253749, 0.101263270831, 0.113921796312, 0.113096750763, 0.00288662638634
219, 100, 0.116129253749, 0.101263270831, 0.113921796312, 0.113096750763, 0.00288662638634
220, 100, 0.116129253749, 0.101263270831, 0.113921796312, 0.113096750763, 0.00288662638634
221, 100, 0.116120392916, 0.101263270831, 0.113921796312, 0.113087762881, 0.00287855827719
222, 100, 0.11600570311, 0.101263270831, 0.113703910556, 0.112880066952, 0.00303509192234
223, 100, 0.115972854625, 0.101263270831, 0.113703910556, 0.11285425679, 0.0030139277457
224, 100, 0.115972854625, 0.101263270831, 0.113703910556, 0.11285425679, 0.0030139277457
225, 100, 0.115972854625, 0.101263270831, 0.113703910556, 0.11285425679, 0.0030139277457
226, 100, 0.115936084386, 0.101263270831, 0.113703910556, 0.112842436315, 0.00300397511286
227, 100, 0.115835750465, 0.101263270831, 0.113642688356, 0.112795051095, 0.00299215150659
228, 100, 0.115804650312, 0.101263270831, 0.113627183726, 0.112761957868, 0.002976598484
229, 100, 0.115775143673, 0.101263270831, 0.113588913868, 0.112730736043, 0.00296085243573
230, 100, 0.115758634245, 0.101263270831, 0.113541483063, 0.112703166538, 0.00294517048854
231, 100, 0.115758634245, 0.101263270831, 0.113541483063, 0.112703166538, 0.00294517048854
232, 100, 0.115687893094, 0.101263270831, 0.113460904986, 0.112657917756, 0.00292029054127
233, 100, 0.115687893094, 0.101263270831, 0.113460904986, 0.112657917756, 0.00292029054127
234, 100, 0.115687893094, 0.101263270831, 0.113460904986, 0.112657917756, 0.00292029054127
235, 100, 0.115687893094, 0.101263270831, 0.113460904986, 0.112657917756, 0.00292029054127
236, 100, 0.115687893094, 0.101263270831, 0.113460904986, 0.112657917756, 0.00292029054127
237, 100, 0.115687893094, 0.101263270831, 0.113460904986, 0.112657917756, 0.00292029054127
238, 100, 0.115685722907, 0.101263270831, 0.113388654689, 0.112589063487, 0.0029291967321
239, 100, 0.115647779731, 0.101263270831, 0.113369331073, 0.112548684978, 0.00291402273331
240, 100, 0.115581196097, 0.101263270831, 0.113369331073, 0.112526331494, 0.00289869720875
241, 100, 0.115579780943, 0.101263270831, 0.113348569589, 0.112449509855, 0.00291844838206
242, 100, 0.115579780943, 0.101263270831, 0.113348569589, 0.112449509855, 0.00291844838206
243, 100, 0.1155732403, 0.101263270831, 0.113305493609, 0.112426267159, 0.00290263865282
244, 100, 0.1155732403, 0.101263270831, 0.113305493609, 0.112426267159, 0.00290263865282
245, 100, 0.115519025373, 0.101263270831, 0.113305493609, 0.112420790371, 0.00289720728707
246, 100, 0.115495837688, 0.101263270831, 0.113263262912, 0.11238953988, 0.00288042532618
247, 100, 0.115447780516, 0.101263270831, 0.113202044384, 0.112306358667, 0.0028775389851
248, 100, 0.11536648263, 0.101263270831, 0.113202044384, 0.112299251613, 0.002870640802
249, 100, 0.11533245611, 0.101263270831, 0.113099896705, 0.11226359844, 0.00285441925859
250, 100, 0.115290476739, 0.101263270831, 0.113034704585, 0.112222629678, 0.0028394401229
251, 100, 0.115276409264, 0.101263270831, 0.112979828689, 0.112192343703, 0.00282265859088
252, 100, 0.115276409264, 0.101263270831, 0.112979828689, 0.112192343703, 0.00282265859088
253, 100, 0.115276409264, 0.101263270831, 0.112979828689, 0.112192343703, 0.00282265859088
254, 100, 0.115276409264, 0.101263270831, 0.112979828689, 0.112192343703, 0.00282265859088
255, 100, 0.115276409264, 0.101263270831, 0.112979828689, 0.112192343703, 0.00282265859088
256, 100, 0.115276409264, 0.101263270831, 0.112979828689, 0.112192343703, 0.00282265859088
257, 100, 0.115230465518, 0.101263270831, 0.112910364599, 0.112133331897, 0.00280669922488
258, 100, 0.115230465518, 0.101263270831, 0.112910364599, 0.112133331897, 0.00280669922488
259, 100, 0.115230465518, 0.101263270831, 0.112910364599, 0.112133331897, 0.00280669922488
260, 100, 0.115230465518, 0.101263270831, 0.112910364599, 0.112133331897, 0.00280669922488
261, 100, 0.115199263117, 0.101263270831, 0.112910364599, 0.112114610849, 0.00279218456617
262, 100, 0.115199263117, 0.101263270831, 0.112910364599, 0.112114610849, 0.00279218456617
263, 100, 0.115199263117, 0.101263270831, 0.112910364599, 0.112114610849, 0.00279218456617
264, 100, 0.115199263117, 0.101263270831, 0.112910364599, 0.112114610849, 0.00279218456617
265, 100, 0.115199263117, 0.101263270831, 0.112910364599, 0.112114610849, 0.00279218456617
266, 100, 0.115199263117, 0.101263270831, 0.112910364599, 0.112114610849, 0.00279218456617
267, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
268, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
269, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
270, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
271, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
272, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
273, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
274, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
275, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
276, 100, 0.11513331892, 0.101263270831, 0.112806245156, 0.11204242818, 0.00280478184855
277, 100, 0.115116506651, 0.101263270831, 0.112806245156, 0.112026549338, 0.00279170258287
278, 100, 0.115116506651, 0.101263270831, 0.112806245156, 0.112026549338, 0.00279170258287
279, 100, 0.115114366874, 0.101263270831, 0.112707846558, 0.111964553864, 0.00279123216783
280, 100, 0.115076808233, 0.101263270831, 0.112671109703, 0.111929253253, 0.00277343893883
281, 100, 0.114973686414, 0.101263270831, 0.112671109703, 0.111903997216, 0.00275270450867
282, 100, 0.114973686414, 0.101263270831, 0.112671109703, 0.111903997216, 0.00275270450867
283, 100, 0.114971142264, 0.101263270831, 0.112671109703, 0.111897931849, 0.00274659545695
284, 100, 0.114971142264, 0.101263270831, 0.112671109703, 0.111897931849, 0.00274659545695
285, 100, 0.114971142264, 0.101263270831, 0.112671109703, 0.111897931849, 0.00274659545695
286, 100, 0.114971142264, 0.101263270831, 0.112671109703, 0.111897931849, 0.00274659545695
287, 100, 0.114946355828, 0.101263270831, 0.112671109703, 0.111890555584, 0.00273931296065
288, 100, 0.114878274374, 0.100431209674, 0.11259308969, 0.111745404122, 0.00295000906528
289, 100, 0.114872542108, 0.100431209674, 0.11259308969, 0.111733644658, 0.00293982345547
290, 100, 0.114851809719, 0.100431209674, 0.11259308969, 0.111716360494, 0.00292636822747
291, 100, 0.114851809719, 0.100431209674, 0.11259308969, 0.111716360494, 0.00292636822747
292, 100, 0.114851809719, 0.100431209674, 0.11259308969, 0.111716360494, 0.00292636822747
293, 100, 0.114795009973, 0.100431209674, 0.112460202058, 0.111670628566, 0.00290122258961
294, 100, 0.114795009973, 0.100431209674, 0.112460202058, 0.111670628566, 0.00290122258961
295, 100, 0.114737075144, 0.100431209674, 0.112337716425, 0.111540384254, 0.00296185124263
296, 100, 0.114708015837, 0.100431209674, 0.112279008584, 0.111506828056, 0.00294440164086
297, 100, 0.114708015837, 0.100431209674, 0.112279008584, 0.111506828056, 0.00294440164086
298, 100, 0.114680490566, 0.100431209674, 0.112266760938, 0.111444838216, 0.00294160671569
299, 100, 0.114680490566, 0.100431209674, 0.112266760938, 0.111444838216, 0.00294160671569
300, 100, 0.114680490566, 0.100431209674, 0.112266760938, 0.111444838216, 0.00294160671569
301, 100, 0.114572441896, 0.100431209674, 0.112228016747, 0.111412677031, 0.00292358086639
302, 100, 0.114572441896, 0.100431209674, 0.112228016747, 0.111412677031, 0.00292358086639
303, 100, 0.114566167683, 0.100431209674, 0.112173658831, 0.111386641251, 0.00290687108748
304, 100, 0.114566167683, 0.100431209674, 0.112173658831, 0.111386641251, 0.00290687108748
305, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
306, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
307, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
308, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
309, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
310, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
311, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
312, 100, 0.114558819046, 0.100431209674, 0.112173658831, 0.11136616111, 0.00288912266142
313, 100, 0.114543041968, 0.100431209674, 0.112173658831, 0.111345330685, 0.00287349572188
314, 100, 0.114543041968, 0.100431209674, 0.112173658831, 0.111345330685, 0.00287349572188
315, 100, 0.114543041968, 0.100431209674, 0.112173658831, 0.111345330685, 0.00287349572188
316, 100, 0.114543041968, 0.100431209674, 0.112173658831, 0.111345330685, 0.00287349572188
317, 100, 0.114543041968, 0.100431209674, 0.112173658831, 0.111345330685, 0.00287349572188
318, 100, 0.114518857292, 0.100431209674, 0.112104523636, 0.111263994072, 0.00289685078637
319, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
320, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
321, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
322, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
323, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
324, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
325, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
326, 100, 0.114494661272, 0.100431209674, 0.112104523636, 0.111248923824, 0.00288376930621
327, 100, 0.114453486081, 0.100431209674, 0.112104523636, 0.111248048735, 0.00288279735821
328, 100, 0.114453486081, 0.100431209674, 0.112104523636, 0.111248048735, 0.00288279735821
329, 100, 0.11440715242, 0.100431209674, 0.112104523636, 0.111231157758, 0.00286888129382
* 15mar19 checking izhi traces
** V >80 mV for >50% of cells (1 sec sim)
[[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150319_185150.png][fig]] - example of 4 cell traces (V,u,I,gNDMA)
[[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150319_190500.png][fig]] - summed conductances - g=gAMPA+gNMDA+gGABAA+gGABAB+gOpsin
** chat with bill
oh ok
so the v looks reasonable
that's all we care about
great
maybe check all of them via a min and max and make sure nobody going nuts
(this was a problem in the intf sim that i took over from ash which is why i didn't pursue it further)
Salvador Dura
udpated labels: /u/salvadord/Downloads/izhi_traces.png
@max min - ok
William Lytton
generallywhen you graph v make all between eg -75,50 mV so can compare
but this fine -- i can see what's going on here
Salvador Dura
min =   -76.3631 
but max = 163.0450 which too high right?
will check trace
William Lytton
yes 163 too high -- 50 would be highest ~erev of Na
Salvador Dura
>50% of cells have peak spike voltages >80mv (these includes pyramidal, FS and LTS) 
is this a problem?
William Lytton
any idea why happening? -- are these spikes or are they synaptic inputs?
let's look at some of the worse offenders to see what is going on
@prob certainly more of a bug than a feature
the I shows a spike-like shape which not sure if normal
William Lytton
I'm not quite understanding how this works since generally the spike is just a paste on i thought so the V itself never goes up

** equivalence of both izhi models
*** Izhi 2003 paper and izh.mod implementation (original)
-From Izhikevich, 2003 cell paper
v' = 0.04*v*v + 5*v + 140 - u + I
u' = a*(b*v - u)

- izh.mod (original)
V = V + delta * (0.04*V*V + f*V + g - u + I - gsyn*(V-erev));
V' = 0.04*V*V + f*V + g - u + I - gsyn*(V-erev);

u = u + delta * (a*(b*vv-u));
u' = a*(b*vv-u); 

*** Izhikevich 2008 network paper and izhi.mod (cliffs)
- From Izhikevich, 2008 network paper
Cv  = k(vâˆ’vr)(vâˆ’vt)âˆ’u+I 
u Ì‡ = a{b(vâˆ’vr)âˆ’u}

- izhi.mod (cliff) 
V = V + delta*(V + k*(V-vr)*(V-vt) - u - I)/C;  % note I = -Isyn
u = u + delta*a*(b*(V-vr)-u); % Calculate recovery variable

- in izhi.mod there is an extra V!? ... should be:
V  = V + delta*(k(Vâˆ’vr)(Vâˆ’vt)âˆ’u+I) / C

*** Equivalence between Izhi,2003 and Izhi,2008 
- if vr=-60; vt=-40; C=100; k=7  --> from Izhi book:
V' = (7*V^2)/100 + 7*V + I/100 - u/100 + 168
V' = 0.07*V*V + 7V + 168 - u/100 + I/100
 
- if k = 3; vr = -60;  vt = -50; C=100; k=3 --> from Cliff:
V' = (3*V^2)/100 + (33*V)/10 + I/100 - u/100 + 90
V' = 0.03*V*V + 0.33V + 90 - u/100 + I/100

***  Izhikevich 2007 Book code
C=100; vr=-60; vt=-40; k=0.7;
a=0.03; b=-2; c=-50; d=100;
vpeak=35;
% parameters used for RS
% neocortical pyramidal neurons
% spike cutoff
T=1000; tau=1;
n=round(T/tau);
v=vr*ones(1,n);  u=0*v;
I=[zeros(1,0.1*n),70*ones(1,0.9*n)];% pulse of input DC current
for i=1:n-1                         % forward Euler method
    v(i+1)=v(i)+tau*(k*(v(i)-vr)*(v(i)-vt)-u(i)+I(i))/C;
    u(i+1)=u(i)+tau*a*(b*(v(i)-vr)-u(i));
    if v(i+1)>=vpeak
        v(i)=vpeak;
        v(i+1)=c;
        u(i+1)=u(i+1)+d;
    end;
end;
plot(tau*(1:n), v);
% a spike is fired!
% padding the spike amplitude
% membrane voltage reset
% recovery variable update
% plot the result
*** chat with cliff - maybe not equivalent
ok, well hopefuly doesnt make too much of  difference ðŸ˜ƒ
also do u happen to know how to derive the old izhi eqn (2003 paper) from the generalized formulation (2008 network paper)?
v' = 0.04*v*v + 5*v + 140 - u + I
Cv  = k(vâˆ’vr)(vâˆ’vt)âˆ’u+I 
bill and I are going nuts to find equivalence
17 mins
Salvador Dura
Cliff Kerr
i'm not so sure they are equivalent
he didn't actually explain it very well in his 2008 paper where it was derived from, and i haven't seen them used very much elsewhere, the 2003 formulatino is more common
Cliff â€¢ 7 mins
oh really? in the network paper he cites the 2003 paper and doesnt say there are any changes
7 mins
Salvador Dura
Cliff Kerr
the reason i used 2008 is because it has the additional ionic currents
ok, well hopefuly doesnt make too much of  difference ðŸ˜ƒ
also do u happen to know how to derive the old izhi eqn (2003 paper) from the generalized formulation (2008 network paper)?
v' = 0.04*v*v + 5*v + 140 - u + I
Cv  = k(vâˆ’vr)(vâˆ’vt)âˆ’u+I 
bill and I are going nuts to find equivalence
17 mins
Salvador Dura
Cliff Kerr
i'm not so sure they are equivalent
he didn't actually explain it very well in his 2008 paper where it was derived from, and i haven't seen them used very much elsewhere, the 2003 formulatino is more common
Cliff â€¢ 7 mins
oh really? in the network paper he cites the 2003 paper and doesnt say there are any changes
7 mins
Salvador Dura
Cliff Kerr
the reason i used 2008 is because it has the additional ionic currents
ok, well hopefuly doesnt make too much of  difference ðŸ˜ƒ
also do u happen to know how to derive the old izhi eqn (2003 paper) from the generalized formulation (2008 network paper)?
v' = 0.04*v*v + 5*v + 140 - u + I
Cv  = k(vâˆ’vr)(vâˆ’vt)âˆ’u+I 
bill and I are going nuts to find equivalence
17 mins
Salvador Dura
Cliff Kerr
i'm not so sure they are equivalent
he didn't actually explain it very well in his 2008 paper where it was derived from, and i haven't seen them used very much elsewhere, the 2003 formulatino is more common
Cliff â€¢ 7 mins
oh really? in the network paper he cites the 2003 paper and doesnt say there are any changes
7 mins
Salvador Dura
Cliff Kerr
the reason i used 2008 is because it has the additional ionic currents

** Fixed extra V in cliff eqn
*** chat with cliff
seems there is a diff between your izhi implementation and the one shown in the 2008 paper and the 2007 book
yours: V = V + delta*(V + k*(V-vr)*(V-vt) - u - I)/C
2008 paper + 2007 book: V = V + delta*(k(Vâˆ’vr)(Vâˆ’vt)âˆ’u+I) / C
so seems like you have an extra V inside the 1st bracket -- any reason for this?
22 mins
Salvador Dura
Cliff Kerr
oh!!! that's interesting! um, i can't think of any reason, V = V + delta*V doesn't make sense
Cliff â€¢ 19 mins
*** comparing diff in sim
**** with bugged izhi model 
Analyzing...
  Run time: 61.2 s (120-s sim; 1 scale; 1064 cells; 4 workers)
  Spikes: 957111 (7.50 Hz)
  Connections: 16803 (1612 STDP; 15.79 per cell)
  Mean connection distance: 1133.21 um
  Mean connection delay: 15.21 ms
  Plotting raster despite using too many cores (4)
  Done; time = 14.9 s
Plotting weight changes...

initial E weights:  [488.0, 360.0, 528.0, 568.0]
final E weigths:  [436.181264686824, 279.4015090314946, 370.09234788800677, 416.3713521759526]
absolute E difference:  [ -51.81873531  -80.59849097 -157.90765211 -151.62864782]
relative E difference:  [-0.10618593 -0.2238847  -0.29906752 -0.26695184]

initial I weights:  [122.0, 90.0, 132.0, 142.0]
final I weigths:  [114.96807147728033, 83.80123276314237, 129.60962416335866, 170.39194997815954]
absolute I difference:  [ -7.03192852  -6.19876724  -2.39037584  28.39194998]
relative I difference:  [-0.05763876 -0.06887519 -0.01810891  0.19994331]
Creating background inputs...
  Number created on host 0: 226

Setting up virtual arm...

Running...
  Done; run time = 0.5 s; real-time ratio: 2.09.

Gathering spikes...
  Done; gather time = 0.2 s.

Closing dummy virtual arm ...

Analyzing...
  Run time: 0.5 s (1-s sim; 1 scale; 1064 cells; 4 workers)
  Spikes: 18007 (16.92 Hz)
  Connections: 16803 (0 STDP; 15.79 per cell)
  Mean connection distance: 1133.21 um
  Mean connection delay: 15.21 ms
  Plotting raster despite using too many cores (4)
  Done; time = 0.3 s
Plotting weight changes...
Target error for target  1  is: 0.129370591289

Setting up virtual arm...

Running...
  Done; run time = 0.5 s; real-time ratio: 2.21.

Gathering spikes...
  Done; gather time = 0.1 s.

Closing dummy virtual arm ...

Analyzing...
  Run time: 0.5 s (1-s sim; 1 scale; 1064 cells; 4 workers)
  Spikes: 18060 (16.97 Hz)
  Connections: 16803 (0 STDP; 15.79 per cell)
  Mean connection distance: 1133.21 um
  Mean connection delay: 15.21 ms
  Plotting raster despite using too many cores (4)
  Done; time = 0.3 s
Plotting weight changes...
Target error for target  1  is: 0.161842442536

Done; total time = 101.1 s
**** with corrected izhi model
Analyzing...
  Run time: 62.9 s (120-s sim; 1 scale; 1064 cells; 4 workers)
  Spikes: 1026664 (8.04 Hz)
  Connections: 16803 (1612 STDP; 15.79 per cell)
  Mean connection distance: 1133.21 um
  Mean connection delay: 15.21 ms
  Plotting raster despite using too many cores (4)
  Done; time = 17.4 s
Plotting weight changes...

initial E weights:  [488.0, 360.0, 528.0, 568.0]
final E weigths:  [375.0829811586976, 261.5880149319242, 371.3416474511743, 399.6792800923642]
absolute E difference:  [-112.91701884  -98.41198507 -156.65835255 -168.32071991]
relative E difference:  [-0.23138733 -0.27336663 -0.29670143 -0.2963393 ]

initial I weights:  [122.0, 90.0, 132.0, 142.0]
final I weigths:  [99.7652838244063, 68.77564870238115, 113.89333282369523, 130.7753127447615]
absolute I difference:  [-22.23471618 -21.2243513  -18.10666718 -11.22468726]
relative I difference:  [-0.18225177 -0.23582613 -0.13717172 -0.07904709]
Creating background inputs...
  Number created on host 0: 226

Setting up virtual arm...

Running...
  Done; run time = 0.5 s; real-time ratio: 1.94.

Gathering spikes...
  Done; gather time = 0.2 s.

Closing dummy virtual arm ...

Analyzing...
  Run time: 0.5 s (1-s sim; 1 scale; 1064 cells; 4 workers)
  Spikes: 17762 (16.69 Hz)
  Connections: 16803 (0 STDP; 15.79 per cell)
  Mean connection distance: 1133.21 um
  Mean connection delay: 15.21 ms
  Plotting raster despite using too many cores (4)
  Done; time = 0.3 s
Plotting weight changes...
Target error for target  1  is: 0.126373968529

Setting up virtual arm...

Running...
  Done; run time = 0.5 s; real-time ratio: 1.94.

Gathering spikes...
  Done; gather time = 0.2 s.

Closing dummy virtual arm ...

Analyzing...
  Run time: 0.5 s (1-s sim; 1 scale; 1064 cells; 4 workers)
  Spikes: 17928 (16.85 Hz)
  Connections: 16803 (0 STDP; 15.79 per cell)
  Mean connection distance: 1133.21 um
  Mean connection delay: 15.21 ms
  Plotting raster despite using too many cores (4)
  Done; time = 0.3 s
Plotting weight changes...
Target error for target  1  is: 0.166514907959

Done; total time = 106.9 s.
* 15mar20 running sims on 'do'
** error running nrniv from nodes
~/.tcshrc
source /act/Modules/3.2.6/init/tcsh
module load neuron-gnu
setenv SITE /usr/site
setenv PATH /home/salvadord/local/scripts:/home/share/anaconda/bin:/home/share/neuron-gnu/nrn/x86_64/bin:/home/share/neuron-gnu/iv/x86_64/bin:/home/share/python/epd-7.0-2-rh5-x86_64/bin:/home/share/bin:/usr/local/python2.7/bin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/usr/X11R6/bin:$PATH
~
** error running nrnivmodl
~/.tcshrc
setenv SITE /home/billl/site
set path = ($SITE/bin $path)
setenv PAGER cat
source $SITE/config/csh_defaults
setenv MAIL /var/spool/mail/billl
# source /usr/site/nrniv/simctrl/nrnenv-epd
# unlimit coredumpsize

#source $SITE/config/csh_aliases
unsetenv XINITRC
setenv RTF2LATEX2E_DIR /usr/local/rtf2latex2e
setenv HOC_LIBRARY_PATH "$HOC_LIBRARY_PATH /u/billl/nrniv/hoc"
setenv MODL_INCLUDE $MODL_INCLUDE":/u/billl/nrniv/migliore/ca3_2002"
setenv LIBS -ldl
# setenv CVS_RSH nrncvs
setenv CVS_RSH ssh

** error importing neuron
downstate-cluster% nrniv -python -mpi
numprocs=1
NEURON -- VERSION 7.4 (1239:f59924c59c5a) 2015-03-20
Duke, Yale, and the BlueBrain Project -- Copyright 1984-2015
See http://www.neuron.yale.edu/neuron/credits

loading membrane mechanisms from x86_64/.libs/libnrnmech.so
Additional mechanisms from files
 izhi.mod nsloc.mod stdp.mod
>>> from neuron import h
Traceback (most recent call last):
  File "stdin", line 1, in <module>
  File "/home/share/python-gnu/local/lib/python2.7/site-packages/neuron/__init__.py", line 81, in <module>
    import neuron.hoc
ImportError: /home/share/python-gnu/local/lib/python2.7/site-packages/neuron/hoc.so: undefined symbol: nrn_wrap_mpi_init

- tried:
 downstate-cluster% setenv PATH /home/share/python-gnu/local/lib/python2.7/site-packages/neuron/:$PATH
* 15mar23 Generating raster, traces, LFP for report
- 10k cells
- run for 2 seconds but use only sec 1-2 because 0-1 looks strange (too much sync)
- psd for each E pop and total - not sure if matches phys enough
- for V trace had to set h.dt=0.05 to get max V right
** code
def runReportFig():
    verystart=time() # store initial time

    s.duration = 2000
    s.plotraster = 1
    s.useArm = 'None'
    s.usestdp = 0
    s.useRL = 0
    s.explorMovs = 0
    s.savelfps = 1
    s.plotpsd = 1
    s.saveraw = 1

    s.savemat = 1

    createNetwork() 
    addStimulation()
    addBackground()
    setupSim()
    runSim()
    finalizeSim()
    saveData()
    plotData()

    ## Wrapping up
    s.pc.runworker() # MPI: Start simulations running on each host
    s.pc.done() # MPI: Close MPI
    totaltime = time()-verystart # See how long it took in total
    print('\nDone; total time = %0.1f s.' % totaltime)
    if (s.plotraster==False and s.plotconn==False and s.plotweightchanges==False): h.quit() # Quit extra processes, or everything if plotting wasn't requested (since assume non-interactive)


popratios =  [0,   0,    0,     0,   150,    25,    25,   168,    72,    40,    40,   192,    32,    32] # Cell population numbers 

* 15mar24 Progress report 
** corrections
- B.6, 3: whether there may be need for further -> needed ?
- B.2, Aim 3 (network): add cell locations were determined based on yfrac ranges for each layer derived from experimental
  data
- C.5.b: File uploaded: aa.pdf ??
** interesting!
- All model neurons included the same set of ion channels: 
-- INa and IKdr for action potential generation; 
-- IKa for rapid repolarization following an action potential; 
-- IKm and IKd for afterhyperpolarization; 
-- Ih for resonance, sag, excitability, and contribution to RMP; 
-- calcium channels (L, N, T-type) for bursting, excitability modulation, and contribution to backpropagating action
 potentials
-- calcium-activated potassium channels for regulating excitability after depolarization-induced calcium influx.

- Fitness functions include
-- differences in spike timing, 
-- spike frequency, 
-- interspike interval (ISI) voltage trajectory, (K and Ca channels)
-- spike shape, (Na/K channels)
-- resting membrane potential (RMP), (depends on: reversal potentials (K, HCN), axial resistivity, leak conductance, and
 other passive parameters)
-- Ih (HCN) dependent sag, and
-- voltage onset/offset under current injection.

* TODO port msarm to M1 model and prepare final demo
** DONE add the input Proprioceptive population, which is actually really a set of netstims with location (NSLOCs), and connect them to layer 2 
** DONE reorganize definition of population/cell parameters
*** DONE set z location based on new yfrac property (0 to 1) for each population
*** DONE population and receptor names within cellpopdata module imported as p (ER2 -> p.ER2)
** DONE Add RL: weight changes at synapses, eligibility traces, stdp-like rule, keep track of target location and arm position (receive via udp every 10ms) to calculate error periodically,
*** DONE Make plexon input be optional 
*** DONE replace arminterface with arminterface_pipe.py - set an option so can use dummy virtual arm for testing!
*** DONE implement msarm.hoc (arm apparatus such as target location, arm position, error etc) in new python-based M1 model
*** DONE fix mpi bug - different motor commands for different nhosts (check if spikes diff as well!!!)
*** DONE RL and eligibility traces using George's PYNDL code
*** DONE plot weightchanges 
** DONE add interface to musculoskel arm (currently dummyArm) - 15 Feb 
*** DONE Assign SPI (spinal cord) subpopulations to different muscles (or maybe just random, which would make more realistic), convert from firing rates to muscle excitation (currently, just sum+threshold), and send udp packet with muscle excitations to arm every 10ms.
***  DONE Assign muscle lengths to the proprioceptive neurons and make them fire accordingly; requires updating muscle lengths every 10ms (via received udp messages)

*** DONE make DSC an izhi population and add connectivity to E5B
*** DONE check if can have plasticity between NSLOC and izhi -yes
** TODO replace pc.post() with pc.py_alltoall() or other ?? - maybe not required
** DONE Modularize using shared parameters+variables (shared.py)
** DONE Add the training and testing wrappers  -  17 Feb
** TODO Train and test to reach single target
*** DONE decide where plasticity will happen and restrict learning to that subset of pops - 19 Feb
-should modify the connectivity and weights tuned to M1!?  
-maybe avoid by adding plastic connections ONLY between
proprioception to L2/3? and SPI to spinal cord interneurons; use spinal cord to project to muscles - see 15jan11
- demonstrate that it is enought to learn a target
*** DONE Set izhikevich neuron type for spinal cord 
- pyramidal for now, maybe tune rate
*** DONE set yfrac appropriately
*** DONE Add proper exploratory movements
*** DONE Add antagonistic muscle inhibition

*** DONE Check why target not plot in test phase!!!
- targetid wasn't being setup
*** DONE Reproducibility - use same random seeds
*** DONE add inhibitory spinal cord pop (IDSC)
*** DONE make exploratory movements by increasing E5B noise
*** TODO setup evol alg - find params for 1k cells, 1 target, dummy arm 
** TODO Test that can learn multiple targets with artificial PMd input
** TODO Encode target using PMd activity as input - 25 Feb 
*** Find good PMd data where target can be decoded
*** Convert into vector of spikes (bill) that can be easily played back (trial/trial basis)
*** Optionally use SSM, and have plasticity between SSM neurons (can argue equivalent to PMd activity !) 
- PMd poisson from neural field model/SSM?)  
- use SSM from Marius, and convert to Poisson(?) a la cliff - Train with SSM
noise for each of the targets 
** TODO Use evol alg to find optimum set of params (first with dummyArm, the musculoskel arm) - 5 Mar
*** TODO Perform short comparison of different types of algos in inspyred:
- Krichmar custom - done
- genetic - done
- evolution strategy
- simulated annealing
- differential evolution
- Estimation of Distribution 
- particle swarm optimization
- ant colony optimization 

** TODO Test real-time with 10k cells, PMd input and virtual arm using BMI in HPC - 10 Mar
*** TODO check firing rates and LFP are not too far away from plausible
compare with firing rate per layer description in gmgs natneu 2015 paper 
** TODO Prepare final demo video - 20 Mar
*** TODO Fix visualization of virtual arm in hpc!!
*** TODO Make something like this:
[[file+sys:/u/salvadord/Documents/ISB/Models_linux/m1ms/gif/20150211_013249.png][fig]]
- NOTE: PMd input can be replaced with low-dim SSM representation (of PMd) and apply similar plasticity concept â€“ maybe
  can compare both methods -- if direct plasticity between bio+model neurons 
** TODO If have time (doubt it!) simulate perturbation and repair - 30 Mar
https://bbs.archlinux.org/viewtopic.php?pid=684936#p684936
